# Time Line
###Jul 24: Intro to NLP and Deep Learning
---
##### [Lecture 1](https://youtu.be/kZteabVD8sU?list=PLcGUo322oqu9n4i0X3cRJgKyVy7OkDdoi)
##### [Slides](http://cs224d.stanford.edu/lectures/CS224d-Lecture1.pdf)
##### [Notes](http://cs224d.stanford.edu/lecture_notes/notes1.pdf)

##### Recommended:
* [Python Tutorial Review](http://cs231n.github.io/python-numpy-tutorial/)
* [Linear Algebra Review](http://cs229.stanford.edu/section/cs229-linalg.pdf)
* [Probability Review](http://cs229.stanford.edu/section/cs229-prob.pdf)
* [Convex Optimization Review](http://cs229.stanford.edu/section/cs229-cvxopt.pdf)
* [More Optimization (SGD) Review](http://cs231n.github.io/optimization-1/)
* [From Frequency to Meaning: vector Space Models of Semantics](http://www.jair.org/media/2934/live-2934-4846-jair.pdf)

### Jul 31: Simple Word Vector representations(word2vec, GloVe)
---
##### [Lecture 2](https://youtu.be/xhHOL3TNyJs?list=PLcGUo322oqu9n4i0X3cRJgKyVy7OkDdoi)
##### [Slides](http://cs224d.stanford.edu/lectures/CS224d-Lecture2.pdf)

##### Recommended:
* [Distributed Representations of Words and Phrases and thier Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
* [Efficient Estimation of Word Representations in Vector Space](http://arxiv.org/pdf/1301.3781.pdf)


### Aug 7: Advanced word vector representations(language models, softmax, single layer networks)
---


##### [Lecture 3](https://youtu.be/UOGMsFw9V_w?list=PLcGUo322oqu9n4i0X3cRJgKyVy7OkDdoi)
##### [Slides](http://cs224d.stanford.edu/lectures/CS224d-Lecture3.pdf)
##### [Notes](http://cs224d.stanford.edu/lecture_notes/notes2.pdf)
##### Recommended:
* [GloVe: Global Vectors for Word Representation](http://nlp.stanford.edu/pubs/glove.pdf)
* [Improving Word Representations via Global Context and Multiple Word Prototypes](http://www.aclweb.org/anthology/P12-1092)

##### [Problem Set 1](http://cs224d.stanford.edu/assignment1/index.html) 
* We will talk about solutions in 4 weeks

### Aug 14: Neural Networks and backpropagation for named entity recognition
---
##### [Lecture 4](https://youtu.be/bjDbNbSbwY4?list=PLcGUo322oqu9n4i0X3cRJgKyVy7OkDdoi)
##### [Slides](http://cs224d.stanford.edu/lectures/CS224d-Lecture4.pdf)
##### [Notes](http://cs224d.stanford.edu/lecture_notes/notes3.pdf)
##### Recommended:
* [UFLDL Tutorial](http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm)
* [Learning Representations by Backpropogating Errors](http://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf)

### Aug 21: Neural Networks and Back-Prop (in full gory detail)
---
##### [Lecture 5](https://youtu.be/k50GPWfjG7I?list=PLcGUo322oqu9n4i0X3cRJgKyVy7OkDdoi)
##### [Slides](http://cs224d.stanford.edu/lectures/CS224d-Lecture5.pdf)
##### Recommended:
* [Natural Language Processing (almost) from Scratch](http://arxiv.org/pdf/1103.0398v1.pdf)
* [A Neural Network for Factoid Question Answering over Paragraphs](https://cs.umd.edu/~miyyer/pubs/2014_qb_rnn.pdf)
* [Grounded Compositional Semantics for Finding and Describing Images with Sentences](http://nlp.stanford.edu/~socherr/SocherKarpathyLeManningNg_TACL2013.pdf)
* [Deep Visual-Semantic Alignments for Generating Image Descriptions](http://cs.stanford.edu/people/karpathy/deepimagesent/devisagen.pdf)
* [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)

###Aug 28: Practical tips(gradient checks, overfitting, regularization, activation functions)
---
##### [Lecture 6](https://youtu.be/l0k-30FNua8?list=PLcGUo322oqu9n4i0X3cRJgKyVy7OkDdoi)
##### [Slides](http://cs224d.stanford.edu/lectures/CS224d-Lecture6.pdf)
##### Recommended:
* [Practical recommendations for gradient-based training of deep architectures](http://arxiv.org/abs/1206.5533)
* [UFLDL page on gradient checking](http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization)


### Sep 4: Introduction to Tensorflow
---
##### [Lecture 7](https://youtu.be/L8Y2_Cq2X5s?list=PLcGUo322oqu9n4i0X3cRJgKyVy7OkDdoi)
##### [Slides](http://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf)

##### Recommended:
* [TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed System](http://download.tensorflow.org/paper/whitepaper2015.pdf)
* [AWS Tutorial](http://cs224d.stanford.edu/supplementary/aws-tutorial-2.pdf)
* [AWS Tutorial Supplementary](http://cs224d.stanford.edu/lectures/CS224D-Lecture7-2.pdf)
* [AWS Tutorial Video](https://youtu.be/zdnMXKHP-m4)

##### [Problem Set 2](http://cs224d.stanford.edu/assignment2/index.html)
* We will talk about solutions in 4 weeks


##### Problem Set 1 Solutions
* [Written](http://cs224d.stanford.edu/assignment1/assignment1_soln)
* [Code](http://cs224d.stanford.edu/assignment1/assignment1_sol.zip)

### Sep 11: Recurrent neural networks for language modeling and other tasks
---
##### [Lecture 8](https://youtu.be/nwcJuGuG-0s?list=PLcGUo322oqu9n4i0X3cRJgKyVy7OkDdoi)
##### [Slides](http://cs224d.stanford.edu/lectures/CS224d-Lecture8.pdf)
##### [Notes](http://cs224d.stanford.edu/lecture_notes/notes4.pdf)
##### Recommended:
* [Recurrent neural network based language model](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)
* [Extensions of recurrent neural network language model](http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf)
* [Opinion Mining with Deep Recurrent Neural Networks](http://www.cs.cornell.edu/~oirsoy/drnt.htm)
* [minimal net example(karpathy)](http://cs231n.github.io/neural-networks-case-study/)
* [vanishing grad example](http://cs224d.stanford.edu/notebooks/vanishing_grad_example.html)
* [vanishing grad notebook](http://cs224d.stanford.edu/notebooks/vanishing_grad_example.ipynb)

### Sep 18: GRUs and LSTMs for machien translation
---
##### [Lecture 9](https://youtu.be/OFCuW8VA7A4?list=PLmImxx8Char8dxWB9LRqdpCTmewaml96q)
* lecture 9 from 2016 is missing but lecture 8 from 2015 is a close equivalent

##### [Slides](http://cs224d.stanford.edu/lectures/CS224d-Lecture9.pdf)
##### Recommended:
* [Long Short-Term memory](http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf)
* [Gated Feedback Recurrent Neural Networks](http://arxiv.org/pdf/1502.02367v3.pdf)
* [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/pdf/1412.3555v1.pdf)


### Sep 25: Recursive neural networks for parsing
---
##### [Lecture 10](https://youtu.be/83J3PMValS4?list=PLmImxx8Char9Ig0ZHSyTqGsdhb9weEGam)
##### [Slides](http://cs224d.stanford.edu/lectures/CS224d-Lecture10.pdf)
##### [Notes](http://cs224d.stanford.edu/lecture_notes/LectureNotes5.pdf)
##### Recommended:
* [Parsing with Compositional Vector Grammars](http://nlp.stanford.edu/pubs/SocherBauerManningNg_ACL2013.pdf)
* [Subgradient Methods for Structure Prediction](http://repository.cmu.edu/cgi/viewcontent.cgi?article=1054&context=robotics)
* [Parsing Natural Scences and Natural Language with Recursive Neural Networks](http://www-nlp.stanford.edu/pubs/SocherLinNgManning_ICML2011.pdf)

### Oct 2: Recursive neual networks for different tasks ( e.g. sentiment analysis)
---

##### [Lecture 11](https://youtu.be/elgkSIWm1vs?list=PLmImxx8Char9Ig0ZHSyTqGsdhb9weEGam)
##### [Slides](http://cs224d.stanford.edu/lectures/CS224d-Lecture11.pdf)
##### Recommended:
* [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)
* [Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection](http://papers.nips.cc/paper/4204-dynamic-pooling-and-unfolding-recursive-autoencoders-for-paraphrase-detection.pdf)
* [Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](http://arxiv.org/pdf/1503.00075v2.pdf)

##### [Problem Set 3](http://cs224d.stanford.edu/assignment3/index.html)
* We will talk about solutions in 4 weeks

##### Problem Set 2 Solutions
* [Written](http://cs224d.stanford.edu/assignment2/assignment2_sol.pdf)
* [Code](http://cs224d.stanford.edu/assignment2/assignment2_dev.zip)

## Future Work

This later stage is still underdelevepment and much will depend on where the study group as a collective wants to head.
From here there are several guest speekers to choose from from the 2015 class and 2016 class.
We may also want to form groups and work on team projects. 

## Other Resources
[Reddit](https://www.reddit.com/r/CS224d)



http://cs224d.stanford.edu/

https://github.com/fx2323/NLP_Deep_Learning

https://github.com/ahmed-touati/DeepNLP