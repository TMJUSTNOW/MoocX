<center> <h2>Pactical Predictive Analytics - Week 1</h2> </center>
<center> <h3>---------------------------------------</h3> </center>

####*Video 1.1.1 - Appetite Whetting: Bad Science*

**A. Notes**

- now moving into Analytics
    - statistical estimation and prediction
    - you can implement using informatics (e.g. matrix multiplication with joins)
- 80% of "analytics" is just sums and averages

**B. The Truth Wears Off**

- "statistical results have gotten weaker over time"
- we'll explore some statistics/estimation test cases

####*Video 1.1.2 - Hypothesis Testing*

**A. Notes**

- inference is "methods for drawing conclusions about a population from sample data"
- two key methods:
    - hypothesis tests (significance tests)
    - confidence intervals

**B. Hypothesis Testing**

- compare an experimental and control group
- "null hypothesis" = no difference
- "alternative hypothesis" - statistical significance different between the groups
- in a "big data" regime, we have no control over data collection
- analysis techniques are secondary to analysis techniques
- "blinded" trial:
    - participants don't know what group they're in
- "randombized" trial:
    - draw a sample through some method and randomly assign to control and experimental
- Type I error = "H0 is true but you reject it"
- Type II error = "H0 is false and you fail to reject it" (there is an effect and you fail to detect it
    - Type II error = "statistical power"
    
####*Video 1.1.3 - Significance Tests and P-Values*

**A. Notes**

- we can calculate the odds that the diference is due to change
- p-value:
    - "in repeated experiments at this sample size, how often wouldy ou see a result at least this extreme assuming the null hypothesis?"
- no good reason that 0.05 is used in practice
    - definitely the source of controversy
    
####*Video 1.2.1 - Example: Difference of Means*

**A. Classical Method: Derive the Sampling Distribution**

- assumptions:
    - number of survival days follows a normal distribution (bell curve)
    - variances of the two sets of patients are the same
- the test stat has some sampling distribution
    - this has a "estimated standard error"
    - standard deviation of the sampling distribution
    
####*Video 1.2.2 - Deriving the Sampling Distribution*

**A. Notes**

- variance of (t_bar - s_bar) = (variance of t_bar) + (variance of s_bar)
    - "always add the variances"
- "standard deviation of the sampling distribution" is:
    - square root of the sum of the individual variances over their sample sizes
- t distribution gets closer to normal distribution as sample size increases
- this is a painful process...there must be a better way

####*Video 1.2.3 - What are we doing?*

**A. Notes**

- we're asking questions of the form "what would happen if we ran this experiment 1000 times?"
- we can't actually run it a bunch of times, so we make assumptions to derive theg eneral distirbution of the quantity in which we are interested
- *what if we run a simulated experiment?*
    - we can't draw new data from the original population, but we have a representative sample of the population
    - just sample from that
- get the difference in means by just doing a ton of trials
- the classical work is designed to reason about what would happen if you could run trials!

####*Video 1.2.4 - Comparing Classical and Resampling Methods*

**A. Notes**

- p-value is the probability that the outcome observed would happen by chance with repeated experiments
- we simulate those experiments directly!
- classical:
    - resampling is more robust and easier to itnerpret
    - makes no assumptions about the underlying distribution
    - handles skewed distributions naturally
        - with resampling methods, you'll never have to worry about doing things like bounding the distribution
        
**B. More**

- Classical theory:
    - based on idea that averaging produces normal distributions, and most stats are averages of one sort
    - "asymptotically normal"
- Monte Carlo
    - pretend we know the pop, simulate samples from it
    - repeat over and over to construct sampling distribution
- MC not just easier, but actually closer to what you want!
- deriving the sampling distirbution is hard, simulating the sampling distribution is easy

####*Video 1.2.5 - Bootstrap*

**A. Notes**

- classical confidence interval = (diff mean - marginal, diff mean + marginal)
- resampling:
    - literally run it over and over and over and just compute the 5th and 95th percentiles

**B. The Bootstrap**

- given a dataset of size N
- draw N samples \textbf{with replacement} to create a new dataset (may get duplicates)
- repeat ~1000 times
- you now have ~1000 sample datasets
    - all drawn from same pop
    - interepret these as repeated experiments, which is exactly what classical methods call for
- key assumption is independence, but not much else
- motivation: we don't know the distribution from which the data was drawn, but the data we have already collected estimates its own distribution - we draw random samples from this distribution
- if you don't do "with replacement", you've violated "independnt" assumption
- regression with bootstrapping:
    - generate 1000 sampels and 1000 linear regressions
    - a 90% confidence interval for the slope is just the 5th percentile and 95th percentile from these
  
####*Video 1.2.6 - Resampling Caveats*

**A. Caveats about the Bootstrap**

- bootstrapping may underestimate the confidence interval for small samples
- can't be used to estiamte the min or max of a population
- outliers can cause issues
- resampling can be done incorrectly, failing to preserve the original sampling structure
- data are dependent, but resmpling data done as though they were independent
- some really weird statistics, like the maximum, depend on very small features of the data
- time series bootstrap:
    - clearly dependent (think stocks)
    - maybe take chunks of connected periods and then bootstrapped samples of the chunks
- Nate Silver: "the numbers have no way of speaking for themselves"

####*Video 1.3.1 - Outliers and Rank Transformation*

**A. Notes**

- replace each value with its ordinal position when sorted
- rank transform:
    - "do the treatment patients tend to be among those who live longer?"
    - NOT:
    - "is the mean number of days survived higher among treated patients?"

####*Video 1.3.2 - Chi-Squared Test*

**A. Notes**

- used for categorical data
    - "what is the relationship between health and wealth?"
- Chi-squared is only valid if every category has 10 or more values in it

####*Video 1.3.3 - Bad Science Revisited: Publication Bias*

**A. Notes**

- only positive results get published
- "decline effect" completely explained by publication bias

####*Video 1.3.4 - Effect Size*

**A. Notes**

- effect size = ([mean of experimental group]-[mean of control])/(standard deviation)
- not just significant..."how" significant
- used in meta-analysis
- other definitions of effect size: odds-ratio, correlation coefficient
- effect size should always be reported with the p value
- "the standard deviation" = "pooled stdev.
    - take stdev of control group
- standardized mean differenct effect size:
    - small = 0.2
    - medium = 0.5
    - large = 0.8 (bits of diff in mean that you're accounting for)
- what does a 95% conf. interval of the effect size mean?
    - if we repeated the experiment 100 times, the interval would include htis effect size 95/100 times
    - if this includes 0.0, that's equivlent to saying the result is not statistically significant

####*Video 1.3.5 - Meta-Analysis*

**A. Notes**

- looking back at many studies and comparing results
- this is even more importance in data science:
    - working with data you didn't collect
    - "big data" may have become big by combining data from different sources
    - when is this ok? test for homogeneity

**B. How It's Done**

- idea: average across multiple studies, but give more weight to more precise studies (more power)
    - weight by sample size
    - inverse-variance weight (weight = 1/standard error
- fixed-effect model: it assumes that each individual study is measuring the same true effect

**C. Heteroskedasticity**

- changing variance as a function of "something"
    - think "funnel plot"
    - general condition of non-constant variance
- can increase error estimates, leading to type 2 errors:
    - overlooking a real effect because the variance isn't as wide as you think
    - loss of statistical power
 
####*Video 1.3.6 - Fraud and Benford's Law*

**A. Reason 2 for decline: Mistakes and Fraud**

- # of retractions as a measure of mistakes?
- Benford's law:
    - distribution of first digit of data
    - remarkably shows up in many many places
    
####*Video 1.3.7 - Intuition for Benford's Law*

**A. Notes**

- first and second digits of published statistical estimates were approximately Benford distirbuted
- asked subjects to manufacture regression coefficient, and found that the first digits were hard to detect as anomolous
- fradulent regression coefficients were NOT Benford distributed

####*Video 1.3.8 - Benford's Law Explained Visually*

**A. Notes**

- "under this model, values with 1 in the front are in the hat already by the time you get to the 2s"
- there are closed-form expressions for Benford's Law
- limitations:
    - data must span several orders of magnitude
    - no min/max cutoffs
- can be used to detect fraud

####*Video 1.4.1 - Multiple Hypothesis Testing: Bonferroni and Sidak Corrections*

**A. Notes**

- if you perform experiments over and over, you're bound to find something
- this is a bit different than pulication bias: ame sample, different hypotheses
- maybe adjust the significance level down to something stricter
- dude:
    - P(detecting an effect when there is none) = a = 0.05
    - P(detecting an effect when it exists) = 1-a
    - P(detecting an effect when it exists on every experiment) = (1-a)^k
    - P(detecting an effect when there is none on at least one experiment) = 1 - (1-a)^k

**B. Controlling Familywise Error**

- Bonferroni correction --> divide p val by number of hypotheses
- Sidak correction (asserts independence) --> a = 1 - (1-a_c)^k
- probabilities multiplied together = assuming independence

####*Video 1.4.2 - Multiple Hypothesis Testing: False Discovery Rate*

**A. Notes**

- Sidak more conservative than Bonferroni
- both considered to be "too conservative"

**B. False Discovery Rate**

- Q = FDR = (false discovery)/(all discoveries)

**C. FDR (2)**

- Bonferroni correction and other FWER corrections tend to wipe out evidence of the most interesting effects; they suffer from low power
- FDR control offers a way to increase power while maintaining some principled bound on error
- it is based on the assessment that:
    - 4 false discoveries out of 10 rejected null hypotheses
    - is more serious than:
    - 20 false discoveries out of 100 rejected null hypotheses
    
####*Video 1.4.3 - Benjamini-HOchberg Procedure*

**A. Notes**

- compute p-value of m hypotheses
- order them in increasing order of p-value (most likely hypotheses are first)
- test against (i/m)*alpha, with i = rank order and m = number of hypotheses
    - find the highest i for which this holds, reject all lower than that
    - this procedure helps to control the false discovery rate
   
####*Video 1.4.4 - Big Data and Spurious Correlations*

**A. Notes**

- view among statisticians that data scientists don't understand stats
    - don't just blindly apply algorithms!
- "classical statistics was fashioned for small problems, a few hundred data points at most, a few parameters"
- something is changing
- procedures to understand heteroskedasticity, other biases are taught in stats programs but not in ML classes

####*Video 1.4.5 - Spurious COrrelations: Stock Price Example*

**A. Notes**

- Vincent Granville - with big dataset, you find things...but are those things valuable?
- big data = more opportunities for spurious findings

####*Video 1.4.6 - How is Big data different?*

**A. Notes**

- Big P vs. Big N:
    - P = number of variables (columns)
    - N = number of records
- essentially 0 marginal cost to increase the number of records
- while bigg N decreases variance, it amplifies bias
- beware multiple hypothesis tests
- think about representativeness
- Taleb's "Black Swan" events:
    - the turkey's model of human behavior
    - things that are inherently unpredictable
    - not really normal...extreme values not exponentially less common

####*Video 1.5.1 - Frequentist Approach*

**A. Notes**

- frequentist:
    - probability of seeing a certain outcome, given the null hypothesis
- Bayesian:
    - probability of a given outcome, given the data we already have
    
**B. Differences**

- frequentist:
    - data are a repeatable random sample (freq.)
    - underlying parameters remain constant during this repeatable process
    - parameters are fixed
- Bayesian:
    - data are observed from the realized sample
    - parameters are unknown and described probabilistically

**C. Bayes' Theorem**

- P(A|B) = [P(B|A)P(A)]/P(B)
- key benefit: ability to incorporate prior knowledge
- key weakness: the need to incorporate prior knowledge
- you can use this rule to confirm or deny any set of data

####*Video 1.5.2 - Motivation for Bayesian Approaches*

**A. Notes**

- Bayes' theroem only shows how to alter one's belief
- some criticism that the 0.05 standard for "Fisher's P-value" is a problem
- maybe the pendulum is swinging back towards Bayes
- conditional and prior probabilities can be hard to model
    - computationally intractable
    - more popular now because of computing power
- a combo of frequentist and Bayesian approaches will dominate in the 21st century

####*Video 1.5.3 - Bayes' Theorem*

**A. Notes**

- breaking down Bayes:
    - \textbf{the prior} = P(H) = "the probability of the hypothesis being true before collecting the data
    - \textbf{marginal} = P(D) = "what is the probability of collecting this data under all possible hypotheses?"
    - \textbf{posterior} = P(H|D) = "the probability of our hypothesis being true given the data collected"
    - \textbf{Likelihood} = probability of collecting this data when our hypothesis is true
    
####*Video 1.5.4 - Applying Bayes' Theorem*

**A. Notes**

- P(cancer|positive test) = P(positive test | cancer)P(cancer) \ P(positive_test)
- P(positive test) = P(positive test | cancer)*P(cancer) + P(positive test|no cancer)*P(no cancer)

####*Video 1.5.5 - Naive Bayes: Spam Filter*

**A. Notes**

- determine P(spam|words)
- the "naive" bit says "assume independence
    - mathematical simplification
    - P(viagra|spam)*P(rich|spam)*...P(friend|spam)
- 