<center> <h2>Statistical Inference - Week 4</h2> </center>
<center> <h3>---------------------------------------</h3> </center>

####*Video 4.1 - Power*

**A. Power**

- Power is the probability of rejecting the null hypothesis when it is false
- More power is a good thing
- A type II error is failing to reject the null hypothesis when it's false; the probability of this is usually called \beta
- If you fail to reject even with a ton of data, that would be a more meaninful result
- power is most important if, even after getting a ton of data, you still find that the null hypothesis is true
- A type II erro is a bad thing
    - It is the probability of failing to reject the null hypothesis when it's false
- You care a lot about alpha and beta
- Power = 1- \beta
- Power assumes the alternative is true

**B. Notes**

- Power is a function that depends on the mean of the null hypothesis
- Power should get bigger as our alternative hypothesis gets bigger
    - "it's easier to prove if 1000 is different from 10 than if 10.01 is different from 10"
    
**C. An Example**

- Power to detect differences should get larger as null gets bigger
- Power also gets larger with a larger sample size
- "We're more likely to detect a difference if that difference is very big"
- Higher alpha = higher power
- More variability ("noise") = lower power
- Consider going through the code for this manipulate experiment
- Power relies on variability, sample size, alternative mean, alpha level

**D. Inputs to Power Formula**

- The unknowns: alternative mean, sigma, sample size, beta
- Power = 1- \beta
- You could also pick a power that you want and solve for the sample size you'd need
- Mostly you're concerned with either *n* or \beta

**D. Notes**

- Power goes up as \alpha gets larger
- Power of a one sided test is greater than the power of the associated two-sided test
- Power goes up as *n* goes up
- Power goes up as *n* goes up
- Power doesn't necessarily depend on all these parameters, but actually on a more concise function
- The difference between the null and alternative means is most important
- In the context of calculating power, the effect size is the difference between the null and alternative means divided by the standard deviation

**E. T-test Power**

- We never actually calculate power like this
- You can just do power.t.test() in R
- Omit one argument and it will solve for it

```{r eval=FALSE}
power.t.test(n=16, delta=100, sd=200, type="one.sample", alt="one.sided")$power
```
- Power only depends on the effect size
- You could also give it a power you want and figure out the sample size you'd need

```{r}
power.t.test(power=0.8, delta=2/4, sd=1, type="one.sample", alt="one.sided")$n
```

-"You need a sample of size 27 to have a power of 80% to detect an effect size as large as 0.5"
- Omitting delta would give you the minimum detectable delta for a given power and sample size
- Almost always use power.t.test as first attack on a power determination
- It's easy to think you have better power than you actually have

####*Video 4.2 - Multiple Comparisons*

**A. Key Ideas**

- Hypothesis testing/significance analysis is commonly overused
- Correcting for multiple testing avoids false positives or discoveries
- Two key components
    - error measure
    - correction
- Basically, just doing a bunch of tests and only reporting the best p-value is not good
- We are now in "the era of scientific mass production" of data
    - http://www-stat.standford.edu/~ckirby/brad/papers/2010SIexcerpt.pdf
- Multiple testing corrections are neceessary now that we can conduct tons of experiments

**B. Types of Errors**

- Hypothesis tests and p-values are not exactly the same concept
- Type I error = "false positive"
- Type II error = "false negative"

**C. Error Rates**

- False positive rate - The rate at which false results are called significant
- Family wise error rate (FWER) - the probability of at least one false positive
- False discovery rate (FDR) - the rate at which claims of significance are false
- Check out the false discovery rate on wikipedia

**D. COntrolling the False Positive Rate**

- IF the p-values are correctly calculated calling all P < sigma significant will control the false positive rate at level \alpha on average
- If you do 10,000 hypothesis tests (maybe in a sigmal processing setting), the expected number of false positive is alpha % of false positives
- You could get 500 significant false positives - not good
- How do we avoid so many false positives?

**E. Controlling family-wise error rate (FWER)**

- The Bonferroni correction (see wikipedia)
- basics:
    - Suppose you do *m* tests
    - You want to control FWER at level \alpha so the probability of false positives is less than \alpha
    - Calculate normal p-values
    - set the new \alpha equal to \alpha / *m*
    - Call all p-values less than the new \alpha significant
    - This could be too conservative

**F. Controlling false discovery rate (FDR)**

- This is the most popular correction when performingl ots of tests.
- Basic idea:
    - Suppose you do *m* tests
    - You want to control FDR at some level \alpha
    - Calculate the p-values normally, order them from smaller to largest
    - Call any P(i) <= \alpha x i/m significant
    - This should control the false discovery rate
- Easy to calculate; less conservative than Bonferroni correction
- Allows for more false positives. May behave strangely under dependence between tests
- The traditional "no correction" rate does control the false positive rate

**G. Adjusted P-Values**

- One approach is to adjust the threshold \alpha
- A different approach is to calculate "adjusted p-values"
- They are not p-values anymore, in the classic sense
- you can use p.adjust() and some method in the method="" argument

**H. Notes and resources**

- Multiple testing is an entire subfield
- A basic Bonferroni/BH correction is usually enough
- If there is strong dependence between tests there may be problem
    - consider method = "BY"
    
####*Video 4.3 - Resampling/ Resampled Interest*

**A. The Bootstrap**

- The bootstrap is a tremendously useful tool for constructing confidence intervals and calculating standard errors for difficult statistics
- For example, how would one derive a confidence interval for the median?
- THe bootstrap procedure follows from the so-called bootstrap principle
- comes from "pulling yourself up by your bootstraps"
- Liberates analysts from a lot of mathematical analysis

**B. An example: Sample of 50 Die Rolls"

- We know a lot about the population average, so we don't have to do a bunch of tests (necessarily)
- So maybe you don't know if it's a fair die
- Bootstrapping: just keep sampling from the *empirical* distribution
- You only observe one true average
- You're basically replicated the simulation you would have done if you knew the true one
- that's the basic idea of bootstrapping
- The bootstrap uses the empirical distribution that puts probability 1/n on each data point

**C. Father-Son Example**

- Bootstrapping uses the empirical distribution to construct confidence intervals in the face of weird distributions or just where the real distribution is unknown

**D. The bootstrap Principle**

- Suppose that I have a statistic that estimates some population parameter, but I don't know its sampling distribution
- The bootstrap principle suggests using the distribution defined by the data to approximate its sampling distribution

**E. The bootstrap in practice**

- In practice, the bootstrap principle is always acrried out using simulation
- We will cover only a few aspects of bootstrap resampling
- The general proceudre follows by first simulating complete data sets from the observed data with replacement
    - This is approximately drawing from the sampling distribution of that statistic

**F. Nonparametric Boostrap Algorithm Example**

- Bootstrap procedure for calculating confidence interval for the median from a data set of *n* observations
    i. Sample n observations with replacement from the observed data resulting in one simulated complete data set
    ii. Take the median of the simulated data set
    iii. Repeat these two steps *B* times, resulting in *B* simulated medians
    Iv. These medians are approximately drawn from the sampling distribution of the median of *n observations; therefore we can:
        - Darw a histogram of them
        - Calculate their standard deviation to estimate the standard error of the median
        - Take the 2.5 and 97.5 percentiles as a confidence interval for the median
        
**G. A Few Notes on the bootstrap**

- The bootstrap is non-parametric
- Better percentile bootstrap confidence intervals correct for bias (try "BCA" interval in the boostrap package)
- There are lots of variations on bootstrap procedures. "Introduction to the Bootstrap" by Efron is a great resource

**H. Group Comparisons (Permutation Tests)**

- Consider the null hypothesis that the distribution of observations from each group is the same
- Group labels are irrelevant
- Permute the group labels (just change which group each observation is in)
- Recalculate the statistics
- Calculate the percentage of simulations where the simulated statistic was more extreme (toward the alternative) than the observed

**I. Variations on Permutation Testing**

- rank sum test
- hypergeometric probability (Fisher's exact test)
- Raw data = ordinary permutation test
- *randomization tests* are exactly permutation tests, with a different motivation
- For matched data, one can randomize the signs
    - For ranks, this results in the signed rank test
- Permutation strategies work for regression as well
    - Permuting a regressor of interest
- Permutation tests work very well in multivariate settings
    - You can calculate maximum stats that control family-wise error rates
- Permutation tests **create a null distirbution for a hypothesis test by permuting a predictor variable.**

**J. Permutation Test**

- If you use sample() with no arguments, it just permutes the vector
- Looking for: "The percentage of permuted test statistics that are larger or more e xtreme than the observed statistic". 
- If we can't find many reconfigurations that led to a more extreme value of the statistic, we can reject the null hypothesis strongly






