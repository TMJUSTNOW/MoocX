<center> <h2>Statistical Inference - Week 2</h2> </center>
<center> <h3>---------------------------------------</h3> </center>

####Video 1.2 - Probability

**A. What is probability**

- probability is a population quantity
- it is conceptual --> intrinsic in the event; not from the data
- probability operates on the outocme of an experiemnt
- P(A U B) is probability of the "union"...like an "or"

**B. Rules of probability**

- Probability that nothing occurs is 0; somethign occurs is 1
- Probability of somethign is 1-prob of something else
- If A implies B, probability of A occurring is less than probability B occuring
- For any two events, the probability that at least one occurs is sum of probs + their intersection
- You can only add probabilities for mutually exclusive events

**C. Going Further**

- Densities and mass function for random variables are more useful
- the famous "Bell Curve" is some characteristic of the *population*, not the sample

**D. Random variables**

- Discrete or continuous
- Continuous random variables can take any value on the real line or some subset of that line
- Examples:
    - Flip of a coin
    - Outcome of a die role
- Specific instances of treating variables *as if* random:
    - BMI of a subject four years after baseline
    - hypertension status of randomly drawn subject
    - Number of People who click on an ad
    - Intelligence quotients for a sample of children
    
**E. PMF**
    
- A probability mass function gives probability of a discrete rnd variable taking a given value
- Rules of a valid pmf
    - Always greater than 0
    - Sum of possible values has to add up to one
- Bernoulli Distribution

    p(x)=(1/2)^2^(1/2)^1-x^
    
**F. PDF**

- A probability density function gives cumulative probabilities for variables
- This is, again, a statement about the population (not the data itself)
- Probability of any single value being observed is 0

**G. An example with density**

    
```{r fig.height=4,fig.width=6, fig.cap="Plot of Sample PDF", fig.align="center", echo=FALSE, }
#Create the Right-Triangle Density
x <- c(-0.5,0,1,1,1.5)
y <- c(0,0,2,0,0)
plot(x, y, lwd =3, frame=FALSE, type="l")
```

- This is a special case of a known density called a beta distribution

**H. CDF and Survival Function**

- cumulative distribution function --> returns the P(X <= x)
- survival function --> 1-CDF = P(X > x)
- CDF = "left tail", survival = "right tail"

**I. Quantiles**

- 95th percentile a number >= 95% of observed values in a distribution
- the a^th^ quantile of a distribution function alpha is the x for which CDF(x)=a
- The 95th percentile is the value such that the probability of drawing a random variable less than it is 95%

**J. Quantiles in R**

- We never work with the densities. R has an easy trick to do quantiles
- Functions starting with q_ gives us the relevant quantile

**K. Summary**

- We're referring to population quantiles. The median in this context is the *population* median
- A probability model connects the data to the population using assumptions
- The population value is an *estimand*, the sample median would be the *estimator*



####Video 1.3 - Expectations

**A. Expected Values**

- Expected values are useful for characterizing a distribution
- Characterizations can be of the center (mean) or "spread" (variance, standard deviation)

**B. The Population Mean**

- Discrete Variable --> mean = sum of p(x)*x for all x's
- The sample mean is estimated by the population mean
- Sample mean --> "center of mass" --> empirical mean, where all outcomes are assigned the same weight

**C. Using 'manipulate'**

- You can use "manipulate" to add a data slider and change graphs around
- Expected value of the *sample mean* is just the population mean

**D. Facts about Expected Values**

- Expected values are properties of distributions
- The average of random variables is itself a random variable
- Mean of distribution of averages is the same as center of distribution of variable itself

**E. Simulation Experiement**

- The more data you collect, the better your approximation is
- The distribution of averages is tighter than the distribution of the underlying variable (but same mean)

**F. Summarizing What We Know**

- The pop mean is the center of mass of the population
- The sample mean is unbiased
- The more data that goes into the sample mean, the more concentrated its density/mass function is around the population mean






        