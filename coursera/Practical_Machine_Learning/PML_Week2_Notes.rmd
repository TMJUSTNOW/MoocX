<center> <h2>Predictive Machine Learning - Week 2</h2> </center>
<center> <h3>---------------------------------------</h3> </center>

####*Video 2.1 Caret package*

**A. Caret functionality**

- some preprocessing
- data splitting
    - createDataPartition
    - createResample
    - createTimeSlices
- training/testing functions
    - train
    - predict
- model comparison
    - confusionMatrix
    
**B. Machine Learning Algorithms in R**

- regression
- linear discriminant analysis
- random forests
- boosting
- support vector machines

**C. Why Caret?**

- different algorithms have different classes
    - predicting requires different args in the ```predict()``` function
- caret as a unifying framework for doing ML in R

**D. Some Example Code**

```{r sec1test, eval=FALSE}
library(caret); library(kernlab); 
data(spam)
inTrain <- createDataPartition(y=spam$type, p = 0.75, list=FALSE) ##split out training and test. Use 75% to train model

training <- spam[inTrain,] # subset training data
testing <- spam[-inTrain,] # subset testing data

dim(training) ## print number of obs in each

set.seed(32343) #set initial conditions (for randomization)
modelFit <- train(type ~., data=training, method = "glm") #train the model to predict "type" based on a glm model
modelFit
```

- the output shows that this used some bootstrapping
- lets take a look at the final model:

```{r sec2model, eval=FALSE}
modelFit$finalModel ## print final coefficients assigned to all predictors
```

- we can also see some predicted values:

```{r sec3pred, eval=FALSE}
predictions <- predict(modelFit, newdata=testing)
predictions
```

**E. Calculating the Confusion Matrix**

```{r sec3pred, eval=FALSE}
confusionMatrix(predictions, testing$type)
```

**F. Some Additional Info**

- see links to tutorials in the last slide

####*Video 2.2 Data slicing*

**A. SPAM Exampe: data splitting**

```{r sec4split, eval=FALSE}
inTrain <- createDatapartition(y=spam$type, p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,] #samples not in the inTrain index set
dim(training)
```

**B. Doing some Cross-Validation (k-fold method)**

```{r sec4kfold, eval=FALSE}
set.seed(32323)
folds <- createFolds(y=spam$type, k =10, list=TRUE, returnTrain=TRUE) ##split the samples you want to split on. It will return indices as a list
    ## if you set returnTrain = false, you'd get the testing set instead
sapply(folds, length) ## print the lengths of each fold
```

**C. Resampling**

```{r sec4resample, eval=FALSE}
set.seed(32323)
folds <- createResample(y=spam$type, times = 10, list=TRUE) ## create 10 folds of equal size, but with resampling with replacement
sapply(folds, length) #print the sample lengths
```

**D. Time Slices**

```{r sec4timeslice, eval=FALSE}
set.seed(32323)
tme <- 1:1000
folds <- createTimeSlices(y=tme, initialWindow=20, horizon=10) ## rolling samples for forecasting
names(folds)
```

####*Video 2.3 Training options*

**A. Training Control Options**

- you can do some weighting of observations
- you can set different metrics to maximize/minimize
- using trControl you can pass lots of other customization

**B. Metric options**

- continuous outcomes
    - RMSE
    - RSquared
- Categorical outcomes
    - accuracy (fraction correct)
    - kappa (a measure of concordance)
    
**C. trainControl**

- control how many times to resample, how to resample
- you can set defaults for initialWindow and horizon
- have it return the actual predictions (save predicted values)
- set other summary options
- prediction bounds
- set seeds for resampling
    - especially important if you are parallelizing across many cores
    
**D. trainControl resampling**

- methods:
    - boot = bootstrapping
    - boot632 = bootstraping with adjustment
    - cv = cross validation
    - repeated cv = repeated cross validation
    - LOOCV = leave one out cross validation
- number:
    - for boot/cross validation
    - number of subsamples to take
- repeats:
    - number of times to repeat subsampling
    - if this is big it can slow things down a lot
    
**E. Setting the Seed**

- there are random draws in these procedures
- setting a seed ensures that same random numbers get generated each time
    - the computer is generating psuedo-random numbers
- set the "overall seed" to get repeatable results
- for parallelization, you can set the seed for each resample using the seed=arg in trainControl
    - this is not critical if you are doing everything on one machine
- this is critical for reproducibility
- see link on "model training and tuning"

####*Video 2.4 Plotting predictions*

**A. Plotting by Factor, With regression smoothers**

```{r sec6plots eval=FALSE}
qq <- qplot(age, wage, color=education, data=training)
qq + geom_smooth(method='lm', formula=y~x)
```

**B. Cutting the Dataset into Quantile Groups**

```{r sec5cut, eval=FALSE}
library(Hmisc)
cutWage <- cut2(training$wage, g=3) ## cut it into 3 groups (based on quantile groups)
table(cutWage)
```

**C. ggplot2 side-by side Plots**

```{r sec5plots, eval=FALSE}
p1 <- qplot() ## some qplot
p2 <- qplot(cutWage, age, data=training, fill=cutWage, geom = c("boxplot","jitter")) ## boxplot gives boxplot, jitter gives the points
grid.arrange(p1, p2, ncol=2) ## plot the two plots side by side

```

**D. Tables by factors**

- use ```prop.table(tablename,1)``` to get row proportions of a table

**E. Density Plots**

- used for continuous predictors

```{r sec5density, eval=FALSE}
qplot(wage, color=education, data=training, geom="density")
```

-density plots with multiple distributions are easier to see than side-by-side boxplots

**F. Notes and further reading**

- only use training data for exploration
- look for:
    - imbalance in outcomes/predictors
    - outliers
    - groups of points not explained by a predictor
    - skewed variables
- other stuff:
    - caret visualizations link
    - ggplot tutorial link
    - EDA course in this specialization
    
####*Video 2.5 Basic preprocessing*

**A. Why preprocess?**

- tough to deal with really skewed data in model-based approaches
    - ML will indulge those big outliers
- you could try standardizing
    - demean, divide by standard deviation
    - this gives you mean 0, standard deviation 1
- when you standardize the testing set....make sure that you use the training mean and standard deviation
    - assumption is that the test data come from same distribution
    
**B. Preprocess function in caret**

```{r sec6preprocess, eval=FALSE}
preObj <- preProcess(training[,-58], method=c("center","scale")) ## scale everything excep the variable of interest (in the 58th column)
testCapAves <- predict(preObj, test[,-58])$capitalAve ## use the preprocessed values to predict the test set
```

**C. Standardizing - preProcess argument**

- you can pass the preprocessing stuff directly to the ```train()``` function in the caret package

```{r sec6prep, eval=FALSE}
set.seed(32343)
modelFit <- train(type ~., data=training, preProcess=c("center","scale"), method="glm")
```

**D. Box-Cox Transforms**

- take continuous data and try to make them look normal
- use Maximum Likelihoods

```{r sec6boxcox, eval=FALSE}
preObj <- preProcess(training[,-58], method=c("BoxCox")) ## use Box-Cox transforms on the data
trainCapAveS <- predict(preObj, training[,-58])$capitalAve ## use that preProcess object to predict training
```

- Box-Cox won't handle 0 data

**E. Standardizing - Imputing data**

- let's see an example of k-nearest-neighbor (KNN) imputation

```{r sec6imputation, eval=FALSE}
set.seed(13343)

## make some values NA
training$capAve <- training$capitalAve
selectNA <- rbinom(dim(training)[1], size=1,prob=0.05)==1
training$capAve[selectNA] <- NA

## Impute and standardize
preObj <- preProcess(training[,-58], method="knnImpute")
capAve <- predict(preObj, training[,-58])$capAve

## Standardize true values
capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth-mean(capAveTruth))/sd(capAveTruth)
```

**F. Notes and Further Reading**

- training and test set must be processed in the same way
- test transformations will likely be imperfect
    - especially if the test/training sets were collected at different times or in different ways
- all these transformations are based on continuous variables
    - be very careful with factor variables!

####*Video 2.6 Covariate creation*

**A. What are Covariates?**

- feature/predictor

**B. Two levels of covariate creation**

1. From raw data to covariate
2. Transforming tidy covariates
    - turn into quantitative or qualitative characteristics
    - compress the raw data

**C. What are features?**

- things that describe the concept of interest
    - counts
    - dummy variables
    - continuous-scale measurements
    - relative levels, changes

**D. Level 1, Raw data -> covariates**

- depeneds heavily on application
- the balancing act is summarization vs. information loss
- the more knowledge of the system you have the better job you will do
    - like in the Einav piece on economists and big data
- when in doubt, err on the side of more features
- can be automated, but be careful
    - the more black-boxy you get, the more susceptible you are to rapid (and inexplicable) changeds in predictions

**E. Level 2, Tidy covariates -> new covariates**

- more necessary for some methods (regressions, svms) than for others (classification trees)
- should be done only on  the training set 
- the best approach is through exploratory analysis (plotting/tables)
- new covariates should be added to data frames
    - make sure all your potential predictors are included in the data fed to the ML algorithm
    - "to be used in downstream prediction"

**F. Common covariates to add, dummy variables**

- you can turn factor variables into dummy variables using the ```dummyVars()``` function

```{r sec7dummies, eval=FALSE}
dummies <- dummyVars(wage ~ jobclass, data=training)
```

**G. Removing zero covariates**

- useful for removing things with very little variability (and therefore will be poor predictors)

```{r sec7novar, eval=FALSE}
nvs <- nearZeroVar(training, saveMetrics=TRUE) ## get a TRUE/FALSE for if the variable has pretty low variation
```

- this gives you a measure of frequency ratio and percent uniques

**H. Spline basis**

- sometimes you want to fit curvy lines through the data
- use the ```splines``` function for this
- the ```bs() variable creates a polynomial 

```{r sec7exponents, eval=FALSE}
library(splines)
bsBasis <- bs(training$age, df=3) ## df arg tells it polynomial degree to go to

## estimate a linear model with those polynomials
lm1 <- lm(wage ~ bsBasis, data = training)
plot(training$age, training$wage, pch=19, cex=0.5) ## plots a scatterplot
points(training$age, predict(lm1, newdata=training), col="red", pch=19, cex=0.5) ## plot the polynomial fitted line through the data

## applying this to test set, plug in the prediction values from training
predict(bsBasis, age=testing$age)
```

**I. Notes and futher reading**

- Level 1 feature creation (raw data -> covariates)
    - science is key
    - erro on overcreation of features
    - in some applications (images, voices) automated feature creation is possible/necessary
    - see link in the notes on deep learning
- Level 2 feature creation (covariates -> new covariates)
    - the function preProcess in caret will handle some preprocessing
    - create new covariates if you think they will improve fit
    - use exploratory analysis on the trianing set for creating them
    - be careful about overfitting!
- link to preprocessing with caret
- if you want to fit spline models, use the *gam* method in the caret package wich allows smoothing of multiple variables
- more on feature creatin/data tidying in the Getting & Cleaning Data course from the Data Science

####*Video 2.7 Preprocessing with principal components analysis*

**A. Why Use PCA?**

- a bunch of quantitative variables that are closely related
- let's see some code to get started

```{r sec8pca, eval=FALSE}
library(caret); library(kernlab);
data(spam)
inTrain <- createDataPartition(y=spam$type, p=0.75, list=FALSE) ##partition 75% of the data for training
training <- spam[inTrain,] ## put training data in a training dataframe
testing <- spam[-inTrain,] ## testing data in a testing dataframe

M <- abs(cor(training[,-58])) ## get columnwise correlations between all variables except the one of interest
diag(M) <- 0 ## set self-correlations to 0 (we don't care about them)
which(M > 0.8, arr.ind=T) ## find which variables are most closely related
```

**B. Basic PCA Idea**

- we don't need every predictor
- a weighted combination of predictors might be better
- we should pick this combination to capture the "most information" possible
- benefits:
    - reduced number of predictors
    - reduced noise (due to averaging)
- try to find a combination which explains the variability

**C. Related Problems**

- let's say you have multivariate variables
    - find a new set of multivariate variables that are uncorrelated and explain as much variance as possible
    - if you put all the variables together in one matrix, find the best matrix created with fewer variables (lower rank) that explains the original data
- you have a statistical and data compression goal

**D. Related Solutions - PCA/SVD**

- SVD
    - if X is a matrix with each variable in a column and each observation in a row then the SVD is a "matrix composition"
    - where the columns of U are orthogonal, the columns of V are orthogonal and D is a diagonal matrix
    - SVD = "singular value decomposition"
    - the variables in V are constructed to explain the most variation in the data
- PCA
    - the principal components are equal to the right singular values if your first scale (subtract the mean, divide by the sd) the variables
- some code for PCA:

```{r sec8pcaexample, eval=FALSE}
smallSpam <- spam[,c(34,32)] ## small dataframe with just the two concepts that are highly correlated
prComp <- prcomp(smallSpam) ## run principal components analysis on the two variables that seemed highly correlated
plot(prComp$x[,1], prComp$x[,2]) # plot the first two principal components against each other
```

- you can also look at the "rotation matrix" to see how PCA is summing things up

```{r sec8pcarotate, eval=FALSE}
prComp$rotation ## print a table of the weights PCA assigned
```

- this will 

**E. PCA on an entire dataset**

```{r sec8pcatot, eval=FALSE}
typeColor <- ((spam$type=="spam")*1+1) ## set color for plotting the two variables
prComp <- prcomp(log10(spam[,-58]+1)) ## run PCA on the entire dataframe (excluding the variable we want). Log to dampen variation
plot(prComp$x[,1], prComp$x[,2], col=typeColor, xlab="PC1", ylab="PC2") #plot first PC vs. second PC
```

- note that PC1 explains most variation, PC2 explains next most, etc.

**F. Doing PCA in caret**

```{r sec8pcacaret, eval=FALSE}
preProc <- preProcess(log10(spam[,-58]+1), method="pca", pcaComp=2) ## calculate PCA in the preprocessing step, tell it how many components
spamPC <- predict(preProc, log10(spam[,-58]+1)) ## use predict to run PCA on the training data
plot(spamPC[,1],spamPC[,2],col=typeCOlor) ## plot the first two components

modelFit <- train(training$type ~., method="glm", data=trainPC) ## generate a predicted model with GLM using the PC's as RHS variables
```

**G. Extention to the Test Data**

```{r sec8pcatest, eval=FALSE}
testPC <- predict(preProc, log10(testing[,-58]+1)) ## take principal components from training, get new values for test dataset
confusionMatrix(testing$type, predict(modelFit,testPC)) ## do some prediction and get the accuracy
```

**H. Alternative (sets # of PCs)**

- this is more elegant, but equivalent to previous stuff

```{r sec8pcaalt, eval=FALSE}
modelFit <- train(training$type ~., method="glm", preProcess="pca", data=training) ## preprocess with PCA as part of the training
confusion$Matrix(testing$type, predict(modelFit, testing)) ## pass it the testing dataset and it will do the PC calculation for you
```

**I. Final Thoughts on PCs**

- most useful for linear-type models
- can't make it harder to interpret predictors
- watch out for outliers (these can wreak havoc on PCs)
    - transform first (with logs/Box Cox)
    - plot predictors to identify problems
- for more info see:
    - EDA class (PCA and SVD talked about in more detail)
    - link to book called "elements of statistical learning"
    
####*Video 2.8 Predicting with regression*

**A. Key Ideas**

- fit a simple regression model
- plug in new covariates and multiply by the coefficients
- useful when the linear model is correct
- pros:
    - easy to implement
    - easy to interpret
- cons:
    - poor performance in nonlinear settings

**B. Plotting the Regression**

- use ```lines(variablename, eq1$fitted, lwd=2)`` to add the regression line to a plot
- to predict, simply use ```predict({modelobject}, {newdata})```
    - this will take the regression coefs and multiple them by the appropriate variables

**C. Get Training set/test set errors**

```{r sec9errors, eval=FALSE}
RMSE1 <- sqrt(sum((lm1$fitted-trainFaith$eruptions)^2)) ## root mean square error
RMSE_test <- sqrt(sum((predict(lm1, newdata=testFaith)-testFaith$eruptions)^2)) ## more realistic error estimate than training RMSE
```

**D. Prediction Intervales**

```{r sec9errorint, eval=FALSE}
pred1 <- predict(lm1, newdata=testFaith, interval="prediction") ## make some predictions, ask for a prediction interval
ord <- order(testFaith$waiting) ## order the values for the test data set by waiting
plot(testFaith$waiting, testFaith$eruptions, pch=19, col="blue")
matlines(testFaith$waiting[ord], pred1[ord,],type="l",col=c(1,2,2),lty=c(1,1,1),lwd=3) ## plot the prediction lines
```

**E. Same process with caret**

```{r sec9caretreg, eval=FALSE}
modFit <- train(eruptions ~ waiting, data=trainFaith, method="lm") ## using train() in caret to do regression forecasting
summary(modFit$finalModel) ## prints exact final model used for prediction
```

**F. Notes and futher reading**

- this can be combined with other methods
- regression models with multiple covariates can be included
- readings:
    - elements of statistical learning
    - modern applied statistics with S
    - intro to statistical learning
    
####*Video 2.9 Predicting with Regression Multiple Covariates*

**A. Plot age vs wage, color by education**

- as a reminder:

```{r sec10colorplot, eval=FALSE}
qplot(age, wage, color=education, data=training) ## plot age (x) vs wage (y), coloring by education. legend automatic.
```

**B. Fit a linear Model**

```{r sec9model, eval=FALSE}
modFit <- train(wage ~ age + jobclass + education, method = "lm", data=training)
finMod <- modFit$finalModel ## assign the final model from this to its own variable
print(modFit) ## let's take a look at the results
```

**C. Diagnostics**

- plot the fitted values vs. the residuals:

```{r sec9resids, eval=FALSE}
plot(findMod,1,pch=19,cex=0.5,col="#00000010") ## plot fitted vs resids
```

- we want a horizontal line centered at 0
- plot the fitted vs. residuals with colors by some other variable (even if it wasn't in the model)

```{r sec9resids2, eval=FALSE}
qplot(finMod$fitted, finMod$residuals, color=race, data=training)
```

**D. Plot by index**

- plot by the row number from the dataset (this row is the "index")
- a trend in the residuals suggests that there's something missing from the model
    - a trend might mean you have serial correlation or something else going on
    
**E. Predicted vs. truth in test set**

```{r sec9moreplots, eval=FALSE}
pred <- poredict(modFit, testing)
qplot(wage, pred, colour=year, data=testing)
```

- remember, you cannot look at the test set and then go back to the training set
    - this will give you a biased predictor set
    - this is more just to do a post-mortem on the anaylsis

**F. If you want to use all covariates**

```{r sec9all, eval=FALSE}
modFitAll <- train(wage ~ ., data=training, method="lm") ## the "~." says "kitchen sink it"
pred <- predict(modFitAll, testing) ##get some predicted values
qplot(wage, pred, data=testing) ## plot wage vs. predictions (should see a 45 degree line, roughly)
```

**G. Some Final Notes and Resources**

- regression is useful in combination with other models
- consider using it as one of multiple candidates in a forecast-averaging setting.
- 














