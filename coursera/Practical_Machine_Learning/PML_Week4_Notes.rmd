<center> <h2>Predictive Machine Learning - Week 4</h2> </center>
<center> <h3>---------------------------------------</h3> </center>

####*Video 4.1 Regularized regression*

**A. Basic Idea**

1. Fit a regression model
2. Penalize (or shrink) large coefficients
    - "shrinkage" in econometrics terms

- Pros:
    - can help with the bias/variance tradeoff
    - can help with model selection
    - can also help with model selection (like LASSO)
- Cons:
    - may be computationally demanding on large datasets
    - doesn't perform as well as random forests or boosting
    
**B. A Motivating example**

- Some simple regression
    - assume that X~1~ and X~2~ are perfectly colinear
- what is we just include X~1~ and multiply it by beta1 + beta2 ?
- The result:
    - you will get a good estimate of Y
    - the estimate of Y will be biased
    - by we may get a more-than-compensating reduction in variance of the estimate
    
**C. Prostate cancer example**

```{r prostate, eval=FALSE}
library(ElemStatLearn); data(prostate)
```

- after some point, adding more predictors will hurt the test set error
    - you get overfitting
- with more predictors:
    - training error declines monotonically
    - test set error initially declines, then bottoms out, then even starts to rise

**D. Model selection approach: split samples**

- no method better when data/computation time permits it
- Approach:
    1. Divide data into training/test/validation
    2. Treat validation as test data, train all competing models on the train data and pick the best one on validation
    3. To appropriately assess performance on new data apply to test set
    4. You may re-split and reperform steps 1-3
- two common problems
    - limited data
    - computation complexity (lots and lots of models)
    
**E. Another Approach: Decomposing Expected Prediction Error**

- assume that Y is f(X) + e
- Expected squared error decomposed into:
    - Irreducible error + Bias^2^ + Variance
- see [here](http://www.cbcb.umd.edu/~hcorrada/PracticalML/)

**F. Another issue for high-dimensional data**

- you can very very quickly hit a X >> n problem

**G. High-Dimension Solution: Hard Thresholding**

- model Y = f(x) + e
- constrain only lambda coefficients to be nonzero
- selection problem is after choose lambda figure out which p-lambda coefficients to make nonzero (which variables to drop, basically)

**H. Regularization for Regression**

- the betas are unconstrained (no particular restrictions)
    - they can explode
    - hence susceptible to very high variance
- to control vriance, we might regularize/shrink the coefficients
- where PRSS is a penalized form of the sum of squares.
- add a penalty term in the objective function which penalizes you if betas are too big
    - this can be used to reduce complexity and to reduce variance
    - reduce variance
    - respect structure of the problem
    
**I. Ridge Regression**

- first approach to this
- another example with a penalty term that keeps the betas from getting too big
- include of lambda makes the problem non-singular even if you have very high dimension data
- as lamda increases, the coefficients get closer and closer to 0

**J. Tuning parameter lamda**

- lambda controls the size of the coefficients
- lamda controls the amount of regularization
- as lambda -> 0, we obtain the least squares solution
- as lambda -> infinity, we have all the ridge regression beta hats falling to 0
- might consider choosing the parameter using cross validation

**K. The Lasso**

- similar to ridge regression
- also tried to minimize sum of squared errors plus some penalty term for the size of the betas
- similar lagrangian form
- the idea: for [orthonormal design matrices](http://b.chalmond.free.fr/mini-projets/lasso.pdf) (not the norm!) this has a closed form solution, but not in general
- difference between this and ridge regression: the penalty is on the absolute value, not the squared value (doesn't punish large betas as heavily)
    - LASSO shrinks coefficients to exactly 0
    - this performs model selection in advance (by shrinking things right to 0)

**L. Additional Resources**

- [Link 1](http://www.biostat.jhsph.edu/~ririzarr/Teaching/649/)
- [link 2](http://www.cbcb.umd.edu/~hcorrada/PracticalML/)
- see Hector Corrada's notes on penalized regressions
- to do this in ```caret```, set method =
    - ridge
    - lasso
    - relaxo

####*Video 4.2 Combining predictors*

**A. Ensembling Methods: Key Ideas**

- combine classifiers by averaging/voting
- combining classifiers improves accuracy
- combining classifiers reduces interpretability
- boosting, baggin, and random forests are variants on this
    - but each of these is the same type of classifier
- example: the Netflix prize, Heritage Health prize used a bunch of ensembled methods

**B. Basic inutition - majority vote**

- consider 5 completely independent classifiers
- If accuracy is 70% for each:
    - you might see 83.7% majority vote accuracy
    - 101 independent classifiers would yield 99.9% majority vote accuracy

**C. Approaching for cobining classifiers**

1. Bagging, boosting, random forests
    - usually combine similar classifiers
2. Combining different classifiers
    - model stacking
    - model ensembling
    
**D. Model Stacking Example**

```{r wagedata, eval=FALSE}
library(ISLR); data(Wage); library(ggplot2); library(caret);
Wage <- subset(Wage,select=-c(logwage))

# Create a building data set and validation set
inBuild <- createDataPartition(y=Wage$wage,
                              p=0.7, list=FALSE)
validation <- Wage[-inBuild,]; buildData <- Wage[inBuild,]

inTrain <- createDataPartition(y=buildData$wage,
                              p=0.7, list=FALSE)
training <- buildData[inTrain,]; testing <- buildData[-inTrain,]

## build two different models
mod1 <- train(wage ~.,method="glm",data=training)
mod2 <- train(wage ~.,method="rf",
              data=training, 
              trControl = trainControl(method="cv"),number=3)

## plot predictions against each other
pred1 <- predict(mod1,testing); pred2 <- predict(mod2,testing)
qplot(pred1,pred2,colour=wage,data=testing)

## fit a model that combines predictors
predDF <- data.frame(pred1,pred2,wage=testing$wage) 
combModFit <- train(wage ~.,method="gam",data=predDF) ## relate the wage variable to the two predictions. Fit a regression for weighting
combPred <- predict(combModFit,predDF) ## predict from the combined dataset

## predict on validation data set
pred1V <- predict(mod1,validation); pred2V <- predict(mod2,validation) ## get predictions from both models on validation
predVDF <- data.frame(pred1=pred1V,pred2=pred2V) ## create data frame containing the predictions
combPredV <- predict(combModFit,predVDF) ## build a model using the two predictions
```

**E. Notes and further resources**

- even simple blending can be useful
- typical model for binary/multiclass data
    - build an odd number of models
    - predict with each model
    - predict the class by majority vote
- this can get dramatically more complicated
    - simple blending in caret: ```caretEnsemble``` package (use at your own risk)
    - Wikipedia [ensemble learning](http://en.wikipedia.org/wiki/Ensemble_learning)
- a cautionary note: scalability matters
    - the netflix prize winner was not chosen
    - it was too computationally intensive...couldn't get fast predictions
    
####*Video 4.3 Forecasting*

**A. What is Different? (forecasting as a specific prediction problem)**

- data are dependent over time
- specific pattern types:
    - trends - long term increase or decrease
    - seasonal patterns - patterns related to time of week, month, year
    - cycles
- subsampling into training/test is more complicated
- similar issues arise in spatial data
    - dependency between nearby observations
    - location specific effects
- typically the goal is to predict one or more observations into the future
- all standard predictions can be used (with caution!)

**B. Beware spurious correlations**

- [correlation of words](http://www.google.com/trends/correlate)
- population-based heatmaps will always have similar patterns
- beware extrapolation too

**C. Google data**

```{r goog, eval=FALSE}
library(quantmod)
from.dat <- as.Date("01/01/08", format="%m/%d/%y")
to.dat <- as.Date("12/31/13", format="%m/%d/%y")
getSymbols("GOOG", src="google", from = from.dat, to = to.dat)

## summarize monthly and store as time series
mGoog <- to.monthly(GOOG) ## change daily data to monthly summaries
googOpen <- Op(mGoog)
ts1 <- ts(googOpen,frequency=12) ## covnert to monthly time series
plot(ts1,xlab="Years+1", ylab="GOOG")

## decompose into trend, seasonal, cycle
plot(decompose(ts1),xlab="Years+1")
```

**D. Some Machine Learning with Time Series Data (GOOG continued)**

```{r googcont, eval=FALSE}
ts1Train <- window(ts1,start=1,end=5) 
ts1Test <- window(ts1,start=5,end=(7-0.01)) ## next consecutive set of points after training
ts1Train
```

**E. Forecasting methods**

- simple moving average:

```{r smooth, eval=FALSE}
plot(ts1Train)

## moving average
lines(ma(ts1Train,order=3),col="red")

## exponential smoothing
ets1 <- ets(ts1Train,model="MMM")
fcast <- forecast(ets1)
plot(fcast); lines(ts1Test,col="red")

## get forecast accuracy
accuracy(fcast, ts1Test)
```

**F. Notes and further resources**

- Rob Hyndman's "Forecasting: principles and practice"

####*Video 4.4 Unsupervised Prediction*

**A. What is unsupervised prediction?**

- you don't know the clusters/labels to predict
- to build a predictor:
    - create clusters
    - name clusters
    - build preditors for clusters
- in a new data set:
    - predict clusters
    
**B. Iris example ignoring species labels**

```{r unsup, eval=FALSE}
data(iris); library(ggplot2)
inTrain <- createDataPartition(y=iris$Species,
                              p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)

## k-means clustering (create 3 clusters, ignoring species information)
kMeans1 <- kmeans(subset(training,select=-c(Species)),centers=3)
training$clusters <- as.factor(kMeans1$cluster)
qplot(Petal.Width,Petal.Length,colour=clusters,data=training) ## assign clusters colors

## table of the clusters
table(kMeans1$cluster,training$Species)

## build predictor
modFit <- train(clusters ~.,data=subset(training,select=-c(Species)),method="rpart") ## classification tree
table(predict(modFit,training),training$Species) ## see concordance table

## apply on test dataset
testClusterPred <- predict(modFit,testing) 
table(testClusterPred ,testing$Species)
```

- pretty close (not typical)
- problems:
    - you have error in the clustering and in the prediction
    
**C. Notes and Further Resources**

- the cl_predict function in the clue package provides similar functionality to what was done here
- beware over-interpretation of clusters (it's just an exploratory technique)
- this is one approach to building recommendation engines
