<center> <h2>Communicating Data Science Results - Week 2</h2> </center>
<center> <h3>---------------------------------------</h3> </center>

####*Video 2.1.2 - Barrow Study Problems*

**A. Notes**

- participants not in control of their data or context of presentation
- think about personally identifiable information
- it's inadequate to boil a study down to "leaked vs. not leaked"
- you need to come to a project thinking about ethical consequences
- "there aren't a lot of limits "

####*Video 2.1.3 - Reifying Ethics: Codes of Conduct*

**A. Notes**

- rules vs. principles
- principles are probably the right way to go (appeal to beliefs, rather than trying to control behavior)

**B. American Statistical Association**

- professionalism
- responsibilities to various stakeholders
- "guard against the possibility that a predisposition by investigators or data providers might predetermine the analytic result"
- "expert testimony should be given the same level of rigor as peer-reviewed research"

####*Video 2.1.4 - ASA: Responsibilities to Stakeholders*

**A. Notes**

- "present a client or employer with choices"
- apply statistical sampling and analysis procedures scientifically, withouot predetermining the outcome
- "make new statistical knowledge widely available to provide benefits to society at large and beyond your own scope of applications"
- "anticipate secondary and indirect uses of the data/results"

####*Video 2.1.5 - Other Codes of Conduct*

**A. Notes**

- ensure professional reporting of the statistical design and analysis
- when using statistics with non-statisticians, communicate the proper use of statistics
- promote sharing of (nonproprietary) data methods
	- default to open!
- "recognize that the results of valid statistical studies cannot be guaranteed to conform to the expectations of those commission the study"
- constantly communicate with the end-user as you work
- breaking down the rules by topic rather than affected parties is tough

####*Video 2.1.6 - Examples of Codified Rules: HIPAA*

**A. Notes**

- codes of conduct = ethical principles as the main way to codify ethical considerations
- alternatively, we have some attempts at specific rules
	- "how can we formalize the privacy problem?"
- with rules, you can always find ways to satisfy "the letter of the law"

####*Video 2.2.2 - Examples of Privacy Leaks*

**A. Notes**

- "curse of dimensionality" --> high dimensional data is almost always sparse (many 0s)
- high dimensional data is always spare
	- as the number of dimensions increases, everything becomes futher apart
	- no two documements look exactly the same...there are a lot of words
- de-anonymizing netflix data:
	- no two users are more than 50% similar
	- with large probability, no two profiles are similar up to $epsilon$

**B. Two fundamental "ingredients" for privacy problems**

- a high-dimensional dataset (search history)
- combination with another public data source (phone book)
- high dimensionality helps you "break open the records"

####*Video 2.2.3 - Formalizing the Privacy Problem*

**A. Notes**

- Latanya Sweenery - grad student who tried to show how easy it was to de-anonymize supposedly anonymous data
- Zip code, birth data, and sex sufficient to identify 87% of Americans

**B. Two recurring features of privacy failures**

- high-dim data to distinguish individuals
- combination of multiple datasets to re-identify those individuals
- most methods address NEITHER of these issues
- privacy is not a "closed" loop....depends on your data in the context of all other publicly-available data
- first attempt:
	- "if the release of statistic S makes it possible to determine the value [of private information] more accurately than is possible without access to S, a disclosure has taken place"

####*Video 2.3.1 - Differential Privacy Defined*

**A. Notes**

- "as long as I knew that anyone wouldn't be able to discover anything private about me by looking at the published results"
- if it possible to achieve --> Pr(secret(me) | R) = Pr(secret(me))
	- not possible
	- if R shows there's a strong trend in my population, then with high probability the trend is true of me too
- example where you privacy is violated even if your specific data isn't used in a study:
	- researchers show that red meat casuses cancer
	- your insurance company knows you eat red meat
	- the company may now conclude you are at an increased risk of cancer!
	
**B. Another Attempt**

- can't promise my data won't affect the results
- can't promise that an attacker won't be able to learn new info about me from looking at the results
- what about this?
	- "If the result R were just about as likely to occur with or without your data, you might as well provide it"
	- you info won't affect the result "much"
- technical guarantees aren't enough to solve the societal outcomes we might want to avoid

**C. DIfferential Privacy**

- DP is a guarantee from the data collector to the individuals in the dataset
- the change that the noisy, publicized result will be R is aobut the same whether or not you include your data
- see CYnthia Dwork for more

**D. More on Differential Privacy**

- this "reasons about all possible datasets"

####*Video 2.3.2 - Global Sensitivity*

**A. Notes**

- "sum of the absolute value of the differences is the L1 norm"
	- L1 norm is the sum of absolute values

####*Video 2.3.3 - Laplacian Noise*

**A. Notes**

- lots of analyses can be boiled down to a few counting queries:
	- histograms
	- SVD
	- ID3 decision trees
	- k-means clusterings
	- association rules

**B. How do we add noise?**

- we have to add noise that is a funciton the sensitivity of the dataset
- the higher the sensitivity, the mroe we have to change the result to prevent an adversary from distinguishing between the two possible worlds
- simplest approach is "Laplacian noise"
- LaPlacian is a "peaked distribution" (two exponentials back to back)

####*Video 2.3.4 - Adding Laplacian Noise and Providing Differential Privacy*

**A. Notes**

- privatized R = F(D1) + Lap(change_in_F / epsilon)
- "each result is blurred into neighboring ones by adding LaPlacian noise"
	- higher tolerance or more sensitivity? Things get more spread out

####*Video 2.3.5 - Weaknesses of Differential Privacy*

**A. Notes**

1. counts work well, but the presence or absence of a single record can only change the result slightly
2. Sums can be a problem. A single outlying observation could change the result by an enormous amount, so you'd have to add a LOT of noise for this worst-case individual
3. Max is even worse, since the result is independent of the size of the dataset

**B. More on this**

- think of a "privacy budget". After a series of queries exhausts the privacy budget, that's it! Kill the user
- we might need to add so much noise that it effectively destroys the output
- "if I ask a series of differentially private queries, I can disambiguate between different worlds"
	- think bootstrapping! resampling!
- "various approaches exist to do better for special types of queries, which in some cases must be known up-front (The "non-interactive" case). But the concept of the privacy budget is fundamental to DP."
- there are limits to ALL technical guarantees about privacy
	- DP is most promising, but still leaves much to be desired