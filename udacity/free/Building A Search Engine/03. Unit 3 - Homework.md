# [Building A Search Engine] (http://www.udacity.com/overview/Course/cs101/)

====================================


## Lists

### Fill in the initial values of the elements of `p` to produce the behaviour shown below

```python
p = [blank, blank, blank]
p[0] = p[0] + p[1]
p[1] = p[0] + p[2]
p[2] = p[0] + p[1]

print p # should equal [1,2,3]
```

**Answer:** `p = [1,0,1]`

## Mutating Lists

### Which of the following leaves the value passed in as the input unchanged?

- (a)

    ```python
    def proc1(p):
        p[0] = p[1]
    ```

- (b)

    ```python
    def proc2(p):
        p = p + [1]
    ```

- (c)

    ```python
    def proc3(p):
        q = p
        p.append(3)
        q.pop()
    ```

- (d)

    ```python
    def proc4(p):
        q = []
        while p:
            q.append(p.pop())
        while q:
            p.append(q.pop())
    ```

**Answer:** (b), (c) & (d)


## Product List

### Define a procedure, `product_list`, takes as input a list of numbers, and returns a number that is the result of multiplying all those numbers together.


```python
def product_list(arr):
    prod = 1
    for i in arr:
        prod *= i
    return prod

print product_list([9]) # 9
print product_list([1,2,3,4]) # 24
```
    
## Greatest

### Define a procedure, `greatest`, that takes as input a list of positive numbers, and returns the greatest number in that list. If the input list is empty, the output should be 0.

```python
def greatest(arr):
    if len(arr) > 0:
        max = arr[0]
        for i in arr:
            if i > max:
                max = i
        return max
    return 0

print greatest([4,23,1]) # 23
print greatest([]) # 0
```

## Lists of Lists



### Define a procedure, `total_enrollment`, that takes as an input a list of elements, where each element is a list containing three elements: a university name, the total number of students enrollect, and the annual tuition.

### The outputs should be two numbers giving the total number of students enrollect at all of the universities in the list, and the total tuition (which is the sum of the number of students enrolled times the tuition for each university).


```python
def total_enrollment(arr):
    total_students = 0
    total_tuition  = 0
    for i in arr:
        total_students += i[1]
        total_tuition  += i[1]*i[2]
    return total_students, total_tuition

udacious_univs = [['Udacity',90000,0]]
usa_univs = [ ['California Institute of Technology',2175,37704],
              ['Harvard',19627,39849],
              ['Massachusetts Institute of Technology',10566,40732],
              ['Princeton',7802,37000],
              ['Rice',5879,35551],
              ['Stanford',19535,40569],
              ['Yale',11701,40500]  ]

print total_enrollment(udacious_univs) # (90000,0)
print total_enrollment(usa_univs) #(77285,3058581079L)
```

## Max Pages

### The web crawler we built at the end of Unit 2 has some serious flaws if we were going to use it in a real crawler. One problem is if we start with a good seed page, it might run for an extremely long time (even forever, since the number of URLS on the web is not actually finite). The final two questions of the homework ask you to explore two different ways to limit the pages that it can crawl.


### Modify the `crawl_web` procedure to take a second parameter, `max_pages`, that limits the number of pages to crawl. Your procedure should terminate the crawl after `max_pages` different pages have been crawled, or when there are no more pages to crawl.



### The following definition of `get_page provides` an interface to the website found at `http://www.udacity.com/cs101x/index.html`


```python
def get_page(url):
    try:
        if url == "http://www.udacity.com/cs101x/index.html":
            return  '<html> <body> This is a test page for learning to crawl! <p> It is a good idea to  <a href="http://www.udacity.com/cs101x/crawling.html">learn to crawl</a> before you try to  <a href="http://www.udacity.com/cs101x/walking.html">walk</a> or  <a href="http://www.udacity.com/cs101x/flying.html">fly</a>. </p> </body> </html> '
        elif url == "http://www.udacity.com/cs101x/crawling.html":
            return  '<html> <body> I have not learned to crawl yet, but I am quite good at  <a href="http://www.udacity.com/cs101x/kicking.html">kicking</a>. </body> </html>'
        elif url == "http://www.udacity.com/cs101x/walking.html":
            return '<html> <body> I cant get enough  <a href="http://www.udacity.com/cs101x/index.html">crawling</a>! </body> </html>'
        elif url == "http://www.udacity.com/cs101x/flying.html":
            return '<html> <body> The magic words are Squeamish Ossifrage! </body> </html>'
    except:
        return ""
    return ""

def get_next_target(page):
    start_link = page.find('<a href=')
    if start_link == -1: 
        return None, 0
    start_quote = page.find('"', start_link)
    end_quote = page.find('"', start_quote + 1)
    url = page[start_quote + 1:end_quote]
    return url, end_quote

def union(p,q):
    for e in q:
        if e not in p:
            p.append(e)


def get_all_links(page):
    links = []
    while True:
        url,endpos = get_next_target(page)
        if url:
            links.append(url)
            page = page[endpos:]
        else:
            break
    return links


def crawl_web(seed,max_pages):
    tocrawl = [seed]
    crawled = []
    limit = 0

    while tocrawl and limit < max_pages:
        page = tocrawl.pop()
        if page not in crawled:
            union(tocrawl, get_all_links(get_page(page)))
            crawled.append(page)
            limit += 1
    return crawled

print crawl_web("http://www.udacity.com/cs101x/index.html",1) 
# ['http://www.udacity.com/cs101x/index.html']
print crawl_web("http://www.udacity.com/cs101x/index.html",3) 
# ['http://www.udacity.com/cs101x/index.html', 'http://www.udacity.com/cs101x/flying.html', 'http://www.udacity.com/cs101x/walking.html']
print crawl_web("http://www.udacity.com/cs101x/index.html",500)
# ['http://www.udacity.com/cs101x/index.html', 'http://www.udacity.com/cs101x/flying.html', 'http://www.udacity.com/cs101x/walking.html', 'http://www.udacity.com/cs101x/crawling.html', 'http://www.udacity.com/cs101x/kicking.html']
```    


## Max Depth

### Modify the `crawl_web` procedure to take a second parameter, `max_depth`, that limits the minimum number of consecutive links that would need to be followed from the seed page to reach this page. For example, if `max_depth` is `0`, the only page that should be crawled is the seed page. If `max_depth` is `1`, the pages that should be crawled are the seed page and every page that links to it directly. If `max_depth` is `2`, the crawl should also include all pages that are linked to by these pages.


```python
def get_page(url):
    try:
        if url == "http://www.udacity.com/cs101x/index.html":
            return  '<html> <body> This is a test page for learning to crawl! <p> It is a good idea to  <a href="http://www.udacity.com/cs101x/crawling.html">learn to crawl</a> before you try to  <a href="http://www.udacity.com/cs101x/walking.html">walk</a> or  <a href="http://www.udacity.com/cs101x/flying.html">fly</a>. </p> </body> </html> '
        elif url == "http://www.udacity.com/cs101x/crawling.html":
            return  '<html> <body> I have not learned to crawl yet, but I am quite good at  <a href="http://www.udacity.com/cs101x/kicking.html">kicking</a>. </body> </html>'
        elif url == "http://www.udacity.com/cs101x/walking.html":
            return '<html> <body> I cant get enough  <a href="http://www.udacity.com/cs101x/index.html">crawling</a>! </body> </html>'
        elif url == "http://www.udacity.com/cs101x/flying.html":
            return '<html> <body> The magic words are Squeamish Ossifrage! </body> </html>'
    except:
        return ""
    return ""

def get_next_target(page):
    start_link = page.find('<a href=')
    if start_link == -1: 
        return None, 0
    start_quote = page.find('"', start_link)
    end_quote = page.find('"', start_quote + 1)
    url = page[start_quote + 1:end_quote]
    return url, end_quote

def union(p,q):
    for e in q:
        if e not in p:
            p.append(e)


def get_all_links(page):
    links = []
    while True:
        url,endpos = get_next_target(page)
        if url:
            links.append(url)
            page = page[endpos:]
        else:
            break
    return links


def crawl_web(seed,max_depth):
    tocrawl = [seed]
    crawled = []
    i = 1
    
    while tocrawl:
        page = tocrawl.pop()
        if page not in crawled:
            if i <= max_depth:
                union(tocrawl, get_all_links(get_page(page)))
            crawled.append(page)
        i += 1
    return crawled

print crawl_web("http://www.udacity.com/cs101x/index.html",0) 
# ['http://www.udacity.com/cs101x/index.html']
print crawl_web("http://www.udacity.com/cs101x/index.html",1) 
# ['http://www.udacity.com/cs101x/index.html', 'http://www.udacity.com/cs101x/flying.html', 'http://www.udacity.com/cs101x/walking.html', 'http://www.udacity.com/cs101x/crawling.html']
print crawl_web("http://www.udacity.com/cs101x/index.html",50) 
# ['http://www.udacity.com/cs101x/index.html', 'http://www.udacity.com/cs101x/flying.html', 'http://www.udacity.com/cs101x/walking.html', 'http://www.udacity.com/cs101x/crawling.html', 'http://www.udacity.com/cs101x/kicking.html']
```




## Sudoku

### Define a procedure, `check_sudoku`, that takes as input a square list of lists representing an `n x n` sudoku puzzle solution and returns `True` if the input is a valid sudoku square and returns `False` otherwise.

**Valid Sudoku**

1. Each column of the square contains each of the numbers from 1 to n exactly once.
2. Each row of the square contains each of the numbers from 1 to n exactly once.



```python
def check_sudoku(arr):
    b1 = b2 = True

    # Rule 2
    for i in arr:
        d = {}
        for j in set(i):
            d[j] = i.count(j)
        for k,v in d.iteritems():
            if v > 1:
                b1 = False
            
    # Rule 1
    for i in range(len(arr)):
        d = {}
        for j in range(i, len(arr)):
            arr[i][j],  arr[j][i] = arr[j][i],  arr[i][j]
        for j in arr[i]:
            d[j] = arr[i].count(i)
        for k,v in d.iteritems():
            if v > 1:
                b2 = False
    
    return b1 and b2


correct = [[5,3,4,6,7,8,9,1,2],
           [6,7,2,1,9,5,3,4,8],
           [1,9,8,3,4,2,5,6,7],
           [8,5,9,7,6,1,4,2,3],
           [4,2,6,8,5,3,7,9,1],
           [7,1,3,9,2,4,8,5,6],
           [9,6,1,5,3,7,2,8,4],
           [2,8,7,4,1,9,6,3,5],
           [3,4,5,2,8,6,1,7,9]]
           
incorrect = [[1,2,3,4],
             [2,3,1,3],
             [3,1,2,3],
             [4,4,4,4]]


print check_sudoku(correct)  # True
print check_sudoku(incorrect) # False
```