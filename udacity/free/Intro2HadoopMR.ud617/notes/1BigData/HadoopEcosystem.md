An awful lot of other software has grown up around HDFS and MapReduce. This is called Hadoop Ecosystem. 

 
 -Lots of it designed to make Hadoop easier to use.  
 -Writing Map reduced code isn't completely simple. You need to know a programming language such as Java, Python, Ruby or Perl.  
 -There are lots of non programmers who use SQL. For that reason, Other open source projects have been created to make it easier for people to query their data without knowing how to code. Two key ones are,  
       - Hive  
       - Pig 
 -The Hive interpreter turns the SQL into map reduce code, which then runs on the cluster.  
 -And an alternative is Pig, which allows you to write code to analyze your data in a fairly simple scripting language. Again the code is just turned into map reduce and run on a cluster.  
 -Hive and Pig are great, but they're still running map reduce jobs. Which can take a reasonable around of time to run. Especially over large amounts of data. 

 

Impala 

 
 -Another open source project, is called Impala. Impala was developed as a way to query your data with SQL, but which directly accesses the data in HDFS. 
 -Rather than needing map reduce. Impala is optimized for low latency queries.  
 -In other words Impala queries run very quickly, typically many times faster than Hive,  
 -while Hive is optimized for running long batch processing jobs.  

 

Sqoop, Flume, Hbase, Hue, Oozie, Mahout 

 
 -Another project used by many people is Sqoop. Sqoop takes data from a traditional relational database, such as Microsoft SQL Server. And, puts it in HDFS, as the limited files. So, it can be processed along with other data on the cluster.  
 -Then, there's Flume. Which ingests data as it's generated by external systems. And, again, puts it into the cluster.  
 -HBase is a real time database, built on top of HDFS. 
 -Hue is a graphical front end to the cluster.  
 -Oozie is a workflow management tool. 
 -Mahout is a machine learning library.  
 -In fact there are so many ecosystem projects that making them all talk to one another, and work well, can be tricky.  

 

CDH 

 
 -To make installing and maintaining a cluster like this easier, Cloudera has put together a distribution of HADOOP called CDH.  
 -CDH or the Cloudera distribution including a patchy HADOOP, takes all the key ecosystem projects, along with HADOOP itself, and packages them together so that installation is a really easy process.  
 -And the components are all tested together, so you can be sure there's no incompatibilities between them.  
 -Of course, it's free and open source, just like Hadoop itself.  
 -While you could install everything from scratch, it's far easier to use CDH. (recommended)  