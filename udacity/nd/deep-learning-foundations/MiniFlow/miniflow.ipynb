{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniflow\n",
    "MiniFlow has the makings of becoming a powerful deep learning tool. It is entirely possible to classify something like the [MNIST](http://yann.lecun.com/exdb/mnist/) database with MiniFlow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, youâ€™ll build a library called MiniFlow which will be your own version of [TensorFlow](http://tensorflow.org/) - a deep learning library. Another deep learning library is [Keras](https://keras.io/).\n",
    "\n",
    "The goal of this lab is to demystify two concepts at the heart of neural networks - **backpropagation and differentiable graphs**.\n",
    "\n",
    "**Backpropagation** is the process by which neural networks update the weights of the network over time. You can see it [here](../../nd/ml/notes/supervised-learning/artificial-neural-networks/artificial-neural-networks.ipynb).\n",
    "\n",
    "**Differentiable graphs** are graphs where the nodes are differentiable functions. They are also useful as visual aids for understanding and calculating complicated derivatives. This is the fundamental abstraction of TensorFlow - it's a framework for creating differentiable graphs.\n",
    "\n",
    "With graphs and backpropagation, you will be able to create your own nodes and properly compute the derivatives. Even more importantly, you will be able to think and reason in terms of these graphs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs\n",
    "What is a Neural Network?\n",
    "![example-neural-network](./images/example-neural-network.png)\n",
    "\n",
    "A neural network is a graph of mathematical functions such as [linear combinations](https://en.wikipedia.org/wiki/Linear_combination) and activation functions. The graph consists of **nodes**, and **edges**.\n",
    "\n",
    "Nodes in each layer (except for nodes in the input layer) perform mathematical functions using inputs from nodes in the previous layers. For example, a node could represent f(x,y)=x+y, where x and y are input values from nodes in the previous layer.\n",
    "\n",
    "Similarly, each node creates an output value which may be passed to nodes in the next layer. The output value from the output layer does not get passed to a future layer (last layer!)\n",
    "\n",
    "Layers between the input layer and the output layer are called **hidden layers**.\n",
    "\n",
    "The edges in the graph describe the connections between the nodes, along which the values flow from one layer to the next. These edges can also apply operations to the values that flow along them, such as multiplying by weights, adding biases, etc.. MiniFlow won't use a special class for edges. Instead, its nodes will perform both their own calculations and those of their input edges. This will be more clear as you go through these lessons.\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "By propagating values from the first layer (the input layer) through all the mathematical functions represented by each node, the network outputs a value. This process is called a **forward pass**.\n",
    "\n",
    "### Graphs\n",
    "\n",
    "The nodes and edges create a graph structure. Though the example above is fairly simple, it isn't hard to imagine that increasingly complex graphs can calculate . . . well . . . *almost anything*.\n",
    "\n",
    "There are generally two steps to create neural networks:\n",
    "\n",
    "1. Define the graph of nodes and edges.\n",
    "2. Propagate values through the graph.\n",
    "\n",
    "`MiniFlow` works the same way. You'll define the nodes and edges of your network with one method and then propagate values through the graph with another method. `MiniFlow` comes with some starter code to help you out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miniflow Architecture\n",
    "Let's consider how to implement this graph structure in `MiniFlow`. We'll use a Python class to represent a generic node.\n",
    "\n",
    "```\n",
    "class Node(object):\n",
    "    def __init__(self):\n",
    "        # Properties will go here!\n",
    "\n",
    "```\n",
    "\n",
    "We know that each node might receive input from multiple other nodes. We also know that each node creates a single output, which will likely be passed to other nodes. Let's add two lists: one to store references to the inbound nodes, and the other to store references to the outbound nodes.\n",
    "\n",
    "```\n",
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        # Node(s) from which this Node receives values\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # Node(s) to which this Node passes values\n",
    "        self.outbound_nodes = []\n",
    "        # For each inbound Node here, add this Node as an outbound Node to _that_ Node.\n",
    "        for n in self.inbound_nodes:\n",
    "            n.outbound_nodes.append(self)\n",
    "\n",
    "```\n",
    "\n",
    "Each node will eventually calculate a value that represents its output. Let's initialize the `value` to `None` to indicate that it exists but hasn't been set yet.\n",
    "\n",
    "```\n",
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        # Node(s) from which this Node receives values\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # Node(s) to which this Node passes values\n",
    "        self.outbound_nodes = []\n",
    "        # For each inbound Node here, add this Node as an outbound Node to _that_ Node.\n",
    "        for n in self.inbound_nodes:\n",
    "            n.outbound_nodes.append(self)\n",
    "        # A calculated value\n",
    "        self.value = None\n",
    "\n",
    "```\n",
    "\n",
    "Each node will need to be able to pass values forward and perform backpropagation (more on that later). For now, let's add a placeholder method for forward propagation. We'll deal with backpropagation later on.\n",
    "\n",
    "```\n",
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        # Node(s) from which this Node receives values\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # Node(s) to which this Node passes values\n",
    "        self.outbound_nodes = []\n",
    "        # For each inbound Node here, add this Node as an outbound Node to _that_ Node.\n",
    "        for n in self.inbound_nodes:\n",
    "            n.outbound_nodes.append(self)\n",
    "        # A calculated value\n",
    "        self.value = None\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        Compute the output value based on `inbound_nodes` and\n",
    "        store the result in self.value.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "```\n",
    "\n",
    "### Nodes that Calculate\n",
    "\n",
    "While `Node` defines the base set of properties that every node holds, only specialized [subclasses](https://docs.python.org/3/tutorial/classes.html#inheritance) of `Node` will end up in the graph. As part of this lab, you'll build the subclasses of `Node` that can perform calculations and hold values. For example, consider the `Input` subclass of `Node`.\n",
    "\n",
    "```\n",
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        # An Input node has no inbound nodes,\n",
    "        # so no need to pass anything to the Node instantiator.\n",
    "        Node.__init__(self)\n",
    "\n",
    "    # NOTE: Input node is the only node where the value\n",
    "    # may be passed as an argument to forward().\n",
    "    #\n",
    "    # All other node implementations should get the value\n",
    "    # of the previous node from self.inbound_nodes\n",
    "    #\n",
    "    # Example:\n",
    "    # val0 = self.inbound_nodes[0].value\n",
    "    def forward(self, value=None):\n",
    "        # Overwrite the value if one is passed in.\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "\n",
    "```\n",
    "\n",
    "Unlike the other subclasses of `Node`, the `Input` subclass does not actually calculate anything. The `Input` subclass just holds a `value`, such as a data feature or a model parameter (weight/bias).\n",
    "\n",
    "You can set `value` either explicitly or with the `forward()` method. This value is then fed through the rest of the neural network.\n",
    "\n",
    "### The Add Subclass\n",
    "\n",
    "`Add`, which is another subclass of `Node`, actually can perform a calculation (addition).\n",
    "\n",
    "```\n",
    "class Add(Node):\n",
    "    def __init__(self, x, y):\n",
    "        Node.__init__(self, [x, y])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        You'll be writing code here in the next quiz!\n",
    "        \"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "Notice the difference in the `__init__` method, `Add.__init__(self, [x, y])`. Unlike the `Input` class, which has no inbound nodes, the `Add` class takes 2 inbound nodes, `x` and `y`, and adds the values of those nodes.\n",
    "\n",
    "To sum up:\n",
    "```\n",
    "        Node (input - inbound nodes, output - outbound nodes)\n",
    "        /\\\n",
    "       /  \\\n",
    "      /    \\\n",
    "    Input   Add (Input class has no inbound nodes, but Add class has 2 inbound nodes)\n",
    "```\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Forward Propagation\n",
    "`MiniFlow` has two methods to help you define and then run values through your graphs: `topological_sort()` and `forward_pass()`.\n",
    "\n",
    "In order to define your network, you'll need to define the order of operations for your nodes. Given that the input to some node depends on the outputs of others, you need to flatten the graph in such a way where all the input dependencies for each node are resolved before trying to run its calculation. This is a technique called a [topological sort](https://en.wikipedia.org/wiki/Topological_sorting).\n",
    "\n",
    "The `topological_sort()` function implements topological sorting using [Kahn's Algorithm](https://en.wikipedia.org/wiki/Topological_sorting#Kahn.27s_algorithm). The details of this method are not important, the result is; `topological_sort()` returns a sorted list of nodes in which all of the calculations can run in series. `topological_sort()` takes in a `feed_dict`, which is how we initially set a value for an `Input` node. The `feed_dict` is represented by the Python dictionary data structure. Here's an example use case:\n",
    "\n",
    "```\n",
    "# Define 2 `Input` nodes.\n",
    "x, y = Input(), Input()\n",
    "\n",
    "# Define an `Add` node, the two above`Input` nodes being the input.\n",
    "add = Add(x, y)\n",
    "\n",
    "# The value of `x` and `y` will be set to 10 and 20 respectively.\n",
    "feed_dict = {x: 10, y: 20}\n",
    "\n",
    "# Sort the nodes with topological sort.\n",
    "sorted_nodes = topological_sort(feed_dict=feed_dict)\n",
    "\n",
    "```\n",
    "\n",
    "### Setup\n",
    "\n",
    "Review `nn.py` and `miniflow.py`.\n",
    "\n",
    "The neural network architecture is already there for you in nn.py. It's your job to finish `MiniFlow` to make it work.\n",
    "\n",
    "For this quiz, I want you to:\n",
    "\n",
    "1. Open `nn.py` below. **You don't need to change anything.** I just want you to see how `MiniFlow` works.\n",
    "2. Open `miniflow.py`. **Finish the forward method on the Add class. All that's required to pass this quiz is a correct implementation of forward.**\n",
    "3. Test your network by hitting \"Test Run!\" When the output looks right, hit \"Submit!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convention for running the code\n",
    "Note that nn.py contains the runner code and Miniflow has the lib. So, please refer the code. Here we will just run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 + 5 = 15 (according to miniflow)\n"
     ]
    }
   ],
   "source": [
    "from forward_propagation.nn import *\n",
    "from forward_propagation.miniflow import *\n",
    "main1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Learning and Loss\n",
    "Like MiniFlow in its current state, neural networks take inputs and produce outputs. But unlike MiniFlow in its current state, neural networks can improve the accuracy of their outputs over time (it's hard to imagine improving the accuracy of Add over time!). To explore why accuracy matters, I want you to first implement a trickier (and more useful!) node than Add: the Linear node.\n",
    "\n",
    "\n",
    "As described by Charles and Michael, a Neuron calculates the weighted sum of its inputs.\n",
    "\n",
    "Think back to Neural Networks lesson with Charles and Michael. A simple artificial neuron depends on three components:\n",
    "\n",
    "- inputs, *x* (vector)\n",
    "- weights, *w* (vector)\n",
    "- bias, *b* (scalar)\n",
    "\n",
    "The output, *o*, is just the weighted sum of the inputs plus the bias:\n",
    "\n",
    "\n",
    "o =  Î£x<sub>i</sub>w<sub>i</sub> + b\n",
    "\n",
    "\n",
    "Equation (1)\n",
    "\n",
    "Remember, by varying the weights, you can vary the amount of influence any given input has on the output. The learning aspect of neural networks takes place during a process known as backpropagation. In backpropogation, the network modifies the weights to improve the network's output accuracy. You'll be applying all of this shortly.\n",
    "\n",
    "In this next quiz, you'll try to build a linear neuron that generates an output by applying a simplified version of Equation (1). `Linear` should take an list of inbound nodes of length *n*, a list of weights of length *n*, and a bias.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Open nn.py below. Read through the neural network to see the expected output of `Linear`.\n",
    "2. Open miniflow.py below. Modify `Linear`, which is a subclass of `Node`, to generate an output with Equation (1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.7\n"
     ]
    }
   ],
   "source": [
    "from learning_and_loss.nn import *\n",
    "from learning_and_loss.miniflow import *\n",
    "main2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Linear Transform\n",
    "[Linear algebra](https://www.khanacademy.org/math/linear-algebra) nicely reflects the idea of transforming values between layers in a graph. In fact, the concept of a [transform](https://www.khanacademy.org/math/linear-algebra/matrix-transformations/linear-transformations/v/vector-transformations) does exactly what a layer should do - it converts inputs to outputs in many dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.  4.]\n",
      " [-9.  4.]]\n"
     ]
    }
   ],
   "source": [
    "from linear_transform.nn import *\n",
    "from linear_transform.miniflow import *\n",
    "main3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.23394576e-04   9.82013790e-01]\n",
      " [  1.23394576e-04   9.82013790e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sigmoid_function.nn import *\n",
    "from sigmoid_function.miniflow import *\n",
    "main4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.4166666667\n"
     ]
    }
   ],
   "source": [
    "from cost.nn import *\n",
    "from cost.miniflow import *\n",
    "main5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Backpropogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ -3.34017280e-05,  -5.01025919e-05],\n",
      "       [ -6.68040138e-05,  -1.00206021e-04]]), array([[ 0.9999833],\n",
      "       [ 1.9999833]]), array([[  5.01028709e-05],\n",
      "       [  1.00205742e-04]]), array([ -5.01028709e-05])]\n"
     ]
    }
   ],
   "source": [
    "from backpropogation.nn import *\n",
    "from backpropogation.miniflow import *\n",
    "main7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 510.164502845\n",
      "Epoch: 1, Loss: 444.530657643\n",
      "Epoch: 2, Loss: 389.70973052\n",
      "Epoch: 3, Loss: 343.438649186\n",
      "Epoch: 4, Loss: 303.975701476\n",
      "Epoch: 5, Loss: 269.986862339\n",
      "Epoch: 6, Loss: 240.461680926\n",
      "Epoch: 7, Loss: 214.655074434\n",
      "Epoch: 8, Loss: 192.040544869\n",
      "Epoch: 9, Loss: 172.259307102\n",
      "Epoch: 10, Loss: 155.049149776\n",
      "Epoch: 11, Loss: 140.15552408\n",
      "Epoch: 12, Loss: 127.296892953\n",
      "Epoch: 13, Loss: 116.200092662\n",
      "Epoch: 14, Loss: 106.627462255\n",
      "Epoch: 15, Loss: 98.3750576388\n",
      "Epoch: 16, Loss: 91.2640371618\n",
      "Epoch: 17, Loss: 85.135429729\n",
      "Epoch: 18, Loss: 79.8483096807\n",
      "Epoch: 19, Loss: 75.2790420282\n",
      "Epoch: 20, Loss: 71.3200394282\n",
      "Epoch: 21, Loss: 67.8784725329\n",
      "Epoch: 22, Loss: 64.8756025701\n",
      "Epoch: 23, Loss: 62.2461923213\n",
      "Epoch: 24, Loss: 59.9370654335\n",
      "Epoch: 25, Loss: 57.9046125671\n",
      "Epoch: 26, Loss: 56.1119126601\n",
      "Epoch: 27, Loss: 54.5264077078\n",
      "Epoch: 28, Loss: 53.1186406034\n",
      "Epoch: 29, Loss: 51.8619300541\n",
      "Epoch: 30, Loss: 50.7325154657\n",
      "Epoch: 31, Loss: 49.7097427946\n",
      "Epoch: 32, Loss: 48.7760769002\n",
      "Epoch: 33, Loss: 47.9169150519\n",
      "Epoch: 34, Loss: 47.1202667248\n",
      "Epoch: 35, Loss: 46.3763779512\n",
      "Epoch: 36, Loss: 45.6773578579\n",
      "Epoch: 37, Loss: 45.0168395256\n",
      "Epoch: 38, Loss: 44.38968817\n",
      "Epoch: 39, Loss: 43.791758228\n",
      "Epoch: 40, Loss: 43.2196952746\n",
      "Epoch: 41, Loss: 42.6707765274\n",
      "Epoch: 42, Loss: 42.1427833793\n",
      "Epoch: 43, Loss: 41.6338999317\n",
      "Epoch: 44, Loss: 41.1426323393\n",
      "Epoch: 45, Loss: 40.6677446642\n",
      "Epoch: 46, Loss: 40.2082077492\n",
      "Epoch: 47, Loss: 39.7631583266\n",
      "Epoch: 48, Loss: 39.3318661718\n",
      "Epoch: 49, Loss: 38.9137075947\n",
      "Epoch: 50, Loss: 38.5081439595\n",
      "Epoch: 51, Loss: 38.1147042306\n",
      "Epoch: 52, Loss: 37.7329707874\n",
      "Epoch: 53, Loss: 37.3625679231\n",
      "Epoch: 54, Loss: 37.003152576\n",
      "Epoch: 55, Loss: 36.6544069219\n",
      "Epoch: 56, Loss: 36.3160325223\n",
      "Epoch: 57, Loss: 35.9877457562\n",
      "Epoch: 58, Loss: 35.6692743003\n",
      "Epoch: 59, Loss: 35.3603544488\n",
      "Epoch: 60, Loss: 35.0607290932\n",
      "Epoch: 61, Loss: 34.770146214\n",
      "Epoch: 62, Loss: 34.4883577686\n",
      "Epoch: 63, Loss: 34.2151188848\n",
      "Epoch: 64, Loss: 33.9501873005\n",
      "Epoch: 65, Loss: 33.6933230051\n",
      "Epoch: 66, Loss: 33.444288053\n",
      "Epoch: 67, Loss: 33.2028465291\n",
      "Epoch: 68, Loss: 32.9687646449\n",
      "Epoch: 69, Loss: 32.7418109472\n",
      "Epoch: 70, Loss: 32.5217566177\n",
      "Epoch: 71, Loss: 32.3083758384\n",
      "Epoch: 72, Loss: 32.1014462017\n",
      "Epoch: 73, Loss: 31.900749137\n",
      "Epoch: 74, Loss: 31.706070335\n",
      "Epoch: 75, Loss: 31.5172001482\n",
      "Epoch: 76, Loss: 31.3339339546\n",
      "Epoch: 77, Loss: 31.1560724709\n",
      "Epoch: 78, Loss: 30.9834220106\n",
      "Epoch: 79, Loss: 30.815794683\n",
      "Epoch: 80, Loss: 30.6530085323\n",
      "Epoch: 81, Loss: 30.4948876207\n",
      "Epoch: 82, Loss: 30.3412620583\n",
      "Epoch: 83, Loss: 30.1919679869\n",
      "Epoch: 84, Loss: 30.0468475212\n",
      "Epoch: 85, Loss: 29.905748657\n",
      "Epoch: 86, Loss: 29.768525149\n",
      "Epoch: 87, Loss: 29.6350363671\n",
      "Epoch: 88, Loss: 29.5051471342\n",
      "Epoch: 89, Loss: 29.3787275527\n",
      "Epoch: 90, Loss: 29.2556528208\n",
      "Epoch: 91, Loss: 29.1358030453\n",
      "Epoch: 92, Loss: 29.0190630521\n",
      "Epoch: 93, Loss: 28.9053221969\n",
      "Epoch: 94, Loss: 28.7944741791\n",
      "Epoch: 95, Loss: 28.6864168597\n",
      "Epoch: 96, Loss: 28.5810520838\n",
      "Epoch: 97, Loss: 28.4782855105\n",
      "Epoch: 98, Loss: 28.3780264482\n",
      "Epoch: 99, Loss: 28.2801876972\n",
      "Epoch: 100, Loss: 28.1846854004\n",
      "Epoch: 101, Loss: 28.0914388993\n",
      "Epoch: 102, Loss: 28.0003705982\n",
      "Epoch: 103, Loss: 27.911405835\n",
      "Epoch: 104, Loss: 27.8244727577\n",
      "Epoch: 105, Loss: 27.7395022078\n",
      "Epoch: 106, Loss: 27.6564276088\n",
      "Epoch: 107, Loss: 27.575184861\n",
      "Epoch: 108, Loss: 27.4957122402\n",
      "Epoch: 109, Loss: 27.4179503018\n",
      "Epoch: 110, Loss: 27.3418417899\n",
      "Epoch: 111, Loss: 27.2673315492\n",
      "Epoch: 112, Loss: 27.1943664421\n",
      "Epoch: 113, Loss: 27.1228952682\n",
      "Epoch: 114, Loss: 27.0528686888\n",
      "Epoch: 115, Loss: 26.984239153\n",
      "Epoch: 116, Loss: 26.9169608279\n",
      "Epoch: 117, Loss: 26.8509895314\n",
      "Epoch: 118, Loss: 26.7862826679\n",
      "Epoch: 119, Loss: 26.7227991663\n",
      "Epoch: 120, Loss: 26.6604994211\n",
      "Epoch: 121, Loss: 26.5993452357\n",
      "Epoch: 122, Loss: 26.5392997676\n",
      "Epoch: 123, Loss: 26.4803274767\n",
      "Epoch: 124, Loss: 26.4223940751\n",
      "Epoch: 125, Loss: 26.3654664795\n",
      "Epoch: 126, Loss: 26.309512765\n",
      "Epoch: 127, Loss: 26.2545021216\n",
      "Epoch: 128, Loss: 26.2004048123\n",
      "Epoch: 129, Loss: 26.1471921323\n",
      "Epoch: 130, Loss: 26.0948363713\n",
      "Epoch: 131, Loss: 26.0433107762\n",
      "Epoch: 132, Loss: 25.9925895161\n",
      "Epoch: 133, Loss: 25.9426476484\n",
      "Epoch: 134, Loss: 25.8934610868\n",
      "Epoch: 135, Loss: 25.8450065703\n",
      "Epoch: 136, Loss: 25.7972616334\n",
      "Epoch: 137, Loss: 25.7502045783\n",
      "Epoch: 138, Loss: 25.7038144472\n",
      "Epoch: 139, Loss: 25.6580709964\n",
      "Epoch: 140, Loss: 25.6129546715\n",
      "Epoch: 141, Loss: 25.5684465833\n",
      "Epoch: 142, Loss: 25.5245284846\n",
      "Epoch: 143, Loss: 25.4811827482\n",
      "Epoch: 144, Loss: 25.4383923456\n",
      "Epoch: 145, Loss: 25.3961408265\n",
      "Epoch: 146, Loss: 25.3544122989\n",
      "Epoch: 147, Loss: 25.3131914102\n",
      "Epoch: 148, Loss: 25.2724633289\n",
      "Epoch: 149, Loss: 25.2322137267\n",
      "Epoch: 150, Loss: 25.1924287618\n",
      "Epoch: 151, Loss: 25.1530950618\n",
      "Epoch: 152, Loss: 25.1141997084\n",
      "Epoch: 153, Loss: 25.0757302215\n",
      "Epoch: 154, Loss: 25.0376745448\n",
      "Epoch: 155, Loss: 25.0000210309\n",
      "Epoch: 156, Loss: 24.9627584278\n",
      "Epoch: 157, Loss: 24.9258758652\n",
      "Epoch: 158, Loss: 24.8893628415\n",
      "Epoch: 159, Loss: 24.8532092113\n",
      "Epoch: 160, Loss: 24.8174051732\n",
      "Epoch: 161, Loss: 24.781941258\n",
      "Epoch: 162, Loss: 24.7468083171\n",
      "Epoch: 163, Loss: 24.7119975117\n",
      "Epoch: 164, Loss: 24.6775003022\n",
      "Epoch: 165, Loss: 24.6433084374\n",
      "Epoch: 166, Loss: 24.6094139451\n",
      "Epoch: 167, Loss: 24.575809122\n",
      "Epoch: 168, Loss: 24.5424865246\n",
      "Epoch: 169, Loss: 24.5094389601\n",
      "Epoch: 170, Loss: 24.4766594776\n",
      "Epoch: 171, Loss: 24.4441413598\n",
      "Epoch: 172, Loss: 24.4118781147\n",
      "Epoch: 173, Loss: 24.3798634677\n",
      "Epoch: 174, Loss: 24.3480913543\n",
      "Epoch: 175, Loss: 24.3165559123\n",
      "Epoch: 176, Loss: 24.285251475\n",
      "Epoch: 177, Loss: 24.2541725642\n",
      "Epoch: 178, Loss: 24.2233138836\n",
      "Epoch: 179, Loss: 24.1926703125\n",
      "Epoch: 180, Loss: 24.1622368994\n",
      "Epoch: 181, Loss: 24.1320088563\n",
      "Epoch: 182, Loss: 24.1019815527\n",
      "Epoch: 183, Loss: 24.0721505101\n",
      "Epoch: 184, Loss: 24.0425113969\n",
      "Epoch: 185, Loss: 24.0130600228\n",
      "Epoch: 186, Loss: 23.983792334\n",
      "Epoch: 187, Loss: 23.9547044087\n",
      "Epoch: 188, Loss: 23.9257924518\n",
      "Epoch: 189, Loss: 23.8970527911\n",
      "Epoch: 190, Loss: 23.8684818724\n",
      "Epoch: 191, Loss: 23.840076256\n",
      "Epoch: 192, Loss: 23.811832612\n",
      "Epoch: 193, Loss: 23.7837477172\n",
      "Epoch: 194, Loss: 23.7558184508\n",
      "Epoch: 195, Loss: 23.7280417913\n",
      "Epoch: 196, Loss: 23.7004148128\n",
      "Epoch: 197, Loss: 23.6729346821\n",
      "Epoch: 198, Loss: 23.6455986552\n",
      "Epoch: 199, Loss: 23.6184040748\n",
      "Epoch: 200, Loss: 23.5913483671\n",
      "Epoch: 201, Loss: 23.5644290393\n",
      "Epoch: 202, Loss: 23.537643677\n",
      "Epoch: 203, Loss: 23.5109899421\n",
      "Epoch: 204, Loss: 23.48446557\n",
      "Epoch: 205, Loss: 23.4580683679\n",
      "Epoch: 206, Loss: 23.4317962124\n",
      "Epoch: 207, Loss: 23.4056470481\n",
      "Epoch: 208, Loss: 23.3796188856\n",
      "Epoch: 209, Loss: 23.3537097997\n",
      "Epoch: 210, Loss: 23.3279179281\n",
      "Epoch: 211, Loss: 23.30224147\n",
      "Epoch: 212, Loss: 23.2766786847\n",
      "Epoch: 213, Loss: 23.2512278906\n",
      "Epoch: 214, Loss: 23.2258874639\n",
      "Epoch: 215, Loss: 23.2006558375\n",
      "Epoch: 216, Loss: 23.1755315004\n",
      "Epoch: 217, Loss: 23.1505129968\n",
      "Epoch: 218, Loss: 23.1255989249\n",
      "Epoch: 219, Loss: 23.1007879366\n",
      "Epoch: 220, Loss: 23.0760787366\n",
      "Epoch: 221, Loss: 23.0514700816\n",
      "Epoch: 222, Loss: 23.0269607801\n",
      "Epoch: 223, Loss: 23.0025496908\n",
      "Epoch: 224, Loss: 22.9782357229\n",
      "Epoch: 225, Loss: 22.9540178346\n",
      "Epoch: 226, Loss: 22.9298950327\n",
      "Epoch: 227, Loss: 22.9058663717\n",
      "Epoch: 228, Loss: 22.8819309524\n",
      "Epoch: 229, Loss: 22.8580879217\n",
      "Epoch: 230, Loss: 22.8343364706\n",
      "Epoch: 231, Loss: 22.8106758331\n",
      "Epoch: 232, Loss: 22.7871052854\n",
      "Epoch: 233, Loss: 22.7636241434\n",
      "Epoch: 234, Loss: 22.7402317617\n",
      "Epoch: 235, Loss: 22.7169275313\n",
      "Epoch: 236, Loss: 22.6937108779\n",
      "Epoch: 237, Loss: 22.6705812595\n",
      "Epoch: 238, Loss: 22.6475381641\n",
      "Epoch: 239, Loss: 22.6245811074\n",
      "Epoch: 240, Loss: 22.6017096301\n",
      "Epoch: 241, Loss: 22.5789232951\n",
      "Epoch: 242, Loss: 22.5562216851\n",
      "Epoch: 243, Loss: 22.5336043991\n",
      "Epoch: 244, Loss: 22.5110710502\n",
      "Epoch: 245, Loss: 22.4886212621\n",
      "Epoch: 246, Loss: 22.4662546664\n",
      "Epoch: 247, Loss: 22.4439708998\n",
      "Epoch: 248, Loss: 22.4217696008\n",
      "Epoch: 249, Loss: 22.3996504076\n",
      "Epoch: 250, Loss: 22.3776129545\n",
      "Epoch: 251, Loss: 22.3556568704\n",
      "Epoch: 252, Loss: 22.3337817754\n",
      "Epoch: 253, Loss: 22.3119872794\n",
      "Epoch: 254, Loss: 22.2902729794\n",
      "Epoch: 255, Loss: 22.2686384582\n",
      "Epoch: 256, Loss: 22.2470832823\n",
      "Epoch: 257, Loss: 22.2256070011\n",
      "Epoch: 258, Loss: 22.2042091451\n",
      "Epoch: 259, Loss: 22.1828892254\n",
      "Epoch: 260, Loss: 22.161646733\n",
      "Epoch: 261, Loss: 22.140481138\n",
      "Epoch: 262, Loss: 22.1193918898\n",
      "Epoch: 263, Loss: 22.0983784168\n",
      "Epoch: 264, Loss: 22.0774401265\n",
      "Epoch: 265, Loss: 22.0565764062\n",
      "Epoch: 266, Loss: 22.0357866229\n",
      "Epoch: 267, Loss: 22.0150701247\n",
      "Epoch: 268, Loss: 21.9944262411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 269, Loss: 21.973854284\n",
      "Epoch: 270, Loss: 21.9533535488\n",
      "Epoch: 271, Loss: 21.9329233155\n",
      "Epoch: 272, Loss: 21.9125628499\n",
      "Epoch: 273, Loss: 21.8922714046\n",
      "Epoch: 274, Loss: 21.8720482208\n",
      "Epoch: 275, Loss: 21.8518925287\n",
      "Epoch: 276, Loss: 21.8318035497\n",
      "Epoch: 277, Loss: 21.8117804972\n",
      "Epoch: 278, Loss: 21.7918225777\n",
      "Epoch: 279, Loss: 21.7719289927\n",
      "Epoch: 280, Loss: 21.7520989394\n",
      "Epoch: 281, Loss: 21.732331612\n",
      "Epoch: 282, Loss: 21.7126262029\n",
      "Epoch: 283, Loss: 21.6929819039\n",
      "Epoch: 284, Loss: 21.6733979072\n",
      "Epoch: 285, Loss: 21.6538734063\n",
      "Epoch: 286, Loss: 21.634407597\n",
      "Epoch: 287, Loss: 21.6149996787\n",
      "Epoch: 288, Loss: 21.5956488545\n",
      "Epoch: 289, Loss: 21.5763543328\n",
      "Epoch: 290, Loss: 21.5571153275\n",
      "Epoch: 291, Loss: 21.5379310588\n",
      "Epoch: 292, Loss: 21.5188007542\n",
      "Epoch: 293, Loss: 21.4997236487\n",
      "Epoch: 294, Loss: 21.4806989856\n",
      "Epoch: 295, Loss: 21.461726017\n",
      "Epoch: 296, Loss: 21.4428040041\n",
      "Epoch: 297, Loss: 21.4239322178\n",
      "Epoch: 298, Loss: 21.4051099392\n",
      "Epoch: 299, Loss: 21.3863364597\n",
      "Epoch: 300, Loss: 21.3676110817\n",
      "Epoch: 301, Loss: 21.3489331186\n",
      "Epoch: 302, Loss: 21.3303018951\n",
      "Epoch: 303, Loss: 21.3117167478\n",
      "Epoch: 304, Loss: 21.293177025\n",
      "Epoch: 305, Loss: 21.2746820872\n",
      "Epoch: 306, Loss: 21.2562313071\n",
      "Epoch: 307, Loss: 21.2378240702\n",
      "Epoch: 308, Loss: 21.2194597742\n",
      "Epoch: 309, Loss: 21.2011378301\n",
      "Epoch: 310, Loss: 21.1828576614\n",
      "Epoch: 311, Loss: 21.1646187049\n",
      "Epoch: 312, Loss: 21.1464204104\n",
      "Epoch: 313, Loss: 21.1282622413\n",
      "Epoch: 314, Loss: 21.1101436739\n",
      "Epoch: 315, Loss: 21.0920641982\n",
      "Epoch: 316, Loss: 21.0740233176\n",
      "Epoch: 317, Loss: 21.056020549\n",
      "Epoch: 318, Loss: 21.0380554232\n",
      "Epoch: 319, Loss: 21.0201274842\n",
      "Epoch: 320, Loss: 21.0022362901\n",
      "Epoch: 321, Loss: 20.9843814125\n",
      "Epoch: 322, Loss: 20.9665624367\n",
      "Epoch: 323, Loss: 20.9487789619\n",
      "Epoch: 324, Loss: 20.9310306011\n",
      "Epoch: 325, Loss: 20.9133169808\n",
      "Epoch: 326, Loss: 20.8956377416\n",
      "Epoch: 327, Loss: 20.8779925378\n",
      "Epoch: 328, Loss: 20.8603810371\n",
      "Epoch: 329, Loss: 20.8428029214\n",
      "Epoch: 330, Loss: 20.8252578859\n",
      "Epoch: 331, Loss: 20.8077456397\n",
      "Epoch: 332, Loss: 20.7902659052\n",
      "Epoch: 333, Loss: 20.7728184187\n",
      "Epoch: 334, Loss: 20.7554029297\n",
      "Epoch: 335, Loss: 20.738019201\n",
      "Epoch: 336, Loss: 20.720667009\n",
      "Epoch: 337, Loss: 20.7033461431\n",
      "Epoch: 338, Loss: 20.6860564058\n",
      "Epoch: 339, Loss: 20.6687976125\n",
      "Epoch: 340, Loss: 20.6515695918\n",
      "Epoch: 341, Loss: 20.6343721844\n",
      "Epoch: 342, Loss: 20.6172052442\n",
      "Epoch: 343, Loss: 20.600068637\n",
      "Epoch: 344, Loss: 20.5829622411\n",
      "Epoch: 345, Loss: 20.5658859466\n",
      "Epoch: 346, Loss: 20.5488396556\n",
      "Epoch: 347, Loss: 20.5318232816\n",
      "Epoch: 348, Loss: 20.5148367498\n",
      "Epoch: 349, Loss: 20.4978799962\n",
      "Epoch: 350, Loss: 20.4809529678\n",
      "Epoch: 351, Loss: 20.4640556222\n",
      "Epoch: 352, Loss: 20.4471879275\n",
      "Epoch: 353, Loss: 20.4303498615\n",
      "Epoch: 354, Loss: 20.413541412\n",
      "Epoch: 355, Loss: 20.3967625761\n",
      "Epoch: 356, Loss: 20.38001336\n",
      "Epoch: 357, Loss: 20.3632937786\n",
      "Epoch: 358, Loss: 20.3466038552\n",
      "Epoch: 359, Loss: 20.3299436212\n",
      "Epoch: 360, Loss: 20.3133131154\n",
      "Epoch: 361, Loss: 20.296712384\n",
      "Epoch: 362, Loss: 20.28014148\n",
      "Epoch: 363, Loss: 20.2636004629\n",
      "Epoch: 364, Loss: 20.2470893982\n",
      "Epoch: 365, Loss: 20.2306083569\n",
      "Epoch: 366, Loss: 20.2141574152\n",
      "Epoch: 367, Loss: 20.1977366542\n",
      "Epoch: 368, Loss: 20.181346159\n",
      "Epoch: 369, Loss: 20.1649860188\n",
      "Epoch: 370, Loss: 20.1486563261\n",
      "Epoch: 371, Loss: 20.1323571764\n",
      "Epoch: 372, Loss: 20.1160886676\n",
      "Epoch: 373, Loss: 20.0998508998\n",
      "Epoch: 374, Loss: 20.0836439747\n",
      "Epoch: 375, Loss: 20.067467995\n",
      "Epoch: 376, Loss: 20.0513230644\n",
      "Epoch: 377, Loss: 20.0352092867\n",
      "Epoch: 378, Loss: 20.0191267657\n",
      "Epoch: 379, Loss: 20.0030756046\n",
      "Epoch: 380, Loss: 19.9870559057\n",
      "Epoch: 381, Loss: 19.9710677698\n",
      "Epoch: 382, Loss: 19.9551112961\n",
      "Epoch: 383, Loss: 19.9391865816\n",
      "Epoch: 384, Loss: 19.923293721\n",
      "Epoch: 385, Loss: 19.9074328058\n",
      "Epoch: 386, Loss: 19.8916039246\n",
      "Epoch: 387, Loss: 19.8758071625\n",
      "Epoch: 388, Loss: 19.8600426005\n",
      "Epoch: 389, Loss: 19.8443103159\n",
      "Epoch: 390, Loss: 19.8286103813\n",
      "Epoch: 391, Loss: 19.8129428649\n",
      "Epoch: 392, Loss: 19.7973078298\n",
      "Epoch: 393, Loss: 19.7817053342\n",
      "Epoch: 394, Loss: 19.766135431\n",
      "Epoch: 395, Loss: 19.7505981675\n",
      "Epoch: 396, Loss: 19.7350935854\n",
      "Epoch: 397, Loss: 19.7196217208\n",
      "Epoch: 398, Loss: 19.7041826036\n",
      "Epoch: 399, Loss: 19.6887762581\n",
      "Epoch: 400, Loss: 19.673402702\n",
      "Epoch: 401, Loss: 19.6580619473\n",
      "Epoch: 402, Loss: 19.6427539996\n",
      "Epoch: 403, Loss: 19.6274788582\n",
      "Epoch: 404, Loss: 19.6122365163\n",
      "Epoch: 405, Loss: 19.5970269608\n",
      "Epoch: 406, Loss: 19.5818501723\n",
      "Epoch: 407, Loss: 19.5667061254\n",
      "Epoch: 408, Loss: 19.5515947883\n",
      "Epoch: 409, Loss: 19.5365161234\n",
      "Epoch: 410, Loss: 19.5214700869\n",
      "Epoch: 411, Loss: 19.5064566294\n",
      "Epoch: 412, Loss: 19.4914756954\n",
      "Epoch: 413, Loss: 19.4765272239\n",
      "Epoch: 414, Loss: 19.4616111485\n",
      "Epoch: 415, Loss: 19.4467273971\n",
      "Epoch: 416, Loss: 19.4318758926\n",
      "Epoch: 417, Loss: 19.4170565527\n",
      "Epoch: 418, Loss: 19.4022692904\n",
      "Epoch: 419, Loss: 19.3875140138\n",
      "Epoch: 420, Loss: 19.3727906264\n",
      "Epoch: 421, Loss: 19.3580990275\n",
      "Epoch: 422, Loss: 19.343439112\n",
      "Epoch: 423, Loss: 19.3288107711\n",
      "Epoch: 424, Loss: 19.314213892\n",
      "Epoch: 425, Loss: 19.2996483585\n",
      "Epoch: 426, Loss: 19.2851140508\n",
      "Epoch: 427, Loss: 19.2706108461\n",
      "Epoch: 428, Loss: 19.2561386185\n",
      "Epoch: 429, Loss: 19.2416972396\n",
      "Epoch: 430, Loss: 19.2272865781\n",
      "Epoch: 431, Loss: 19.2129065007\n",
      "Epoch: 432, Loss: 19.1985568715\n",
      "Epoch: 433, Loss: 19.1842375531\n",
      "Epoch: 434, Loss: 19.169948406\n",
      "Epoch: 435, Loss: 19.1556892893\n",
      "Epoch: 436, Loss: 19.1414600608\n",
      "Epoch: 437, Loss: 19.1272605768\n",
      "Epoch: 438, Loss: 19.113090693\n",
      "Epoch: 439, Loss: 19.0989502639\n",
      "Epoch: 440, Loss: 19.0848391435\n",
      "Epoch: 441, Loss: 19.0707571853\n",
      "Epoch: 442, Loss: 19.0567042426\n",
      "Epoch: 443, Loss: 19.0426801683\n",
      "Epoch: 444, Loss: 19.0286848153\n",
      "Epoch: 445, Loss: 19.0147180368\n",
      "Epoch: 446, Loss: 19.0007796861\n",
      "Epoch: 447, Loss: 18.986869617\n",
      "Epoch: 448, Loss: 18.9729876838\n",
      "Epoch: 449, Loss: 18.9591337413\n",
      "Epoch: 450, Loss: 18.9453076453\n",
      "Epoch: 451, Loss: 18.9315092523\n",
      "Epoch: 452, Loss: 18.91773842\n",
      "Epoch: 453, Loss: 18.9039950068\n",
      "Epoch: 454, Loss: 18.8902788727\n",
      "Epoch: 455, Loss: 18.8765898787\n",
      "Epoch: 456, Loss: 18.8629278871\n",
      "Epoch: 457, Loss: 18.8492927619\n",
      "Epoch: 458, Loss: 18.8356843682\n",
      "Epoch: 459, Loss: 18.8221025731\n",
      "Epoch: 460, Loss: 18.8085472449\n",
      "Epoch: 461, Loss: 18.7950182538\n",
      "Epoch: 462, Loss: 18.7815154716\n",
      "Epoch: 463, Loss: 18.768038772\n",
      "Epoch: 464, Loss: 18.7545880304\n",
      "Epoch: 465, Loss: 18.741163124\n",
      "Epoch: 466, Loss: 18.7277639321\n",
      "Epoch: 467, Loss: 18.7143903355\n",
      "Epoch: 468, Loss: 18.7010422173\n",
      "Epoch: 469, Loss: 18.6877194624\n",
      "Epoch: 470, Loss: 18.6744219575\n",
      "Epoch: 471, Loss: 18.6611495915\n",
      "Epoch: 472, Loss: 18.6479022551\n",
      "Epoch: 473, Loss: 18.6346798412\n",
      "Epoch: 474, Loss: 18.6214822444\n",
      "Epoch: 475, Loss: 18.6083093614\n",
      "Epoch: 476, Loss: 18.5951610909\n",
      "Epoch: 477, Loss: 18.5820373336\n",
      "Epoch: 478, Loss: 18.5689379919\n",
      "Epoch: 479, Loss: 18.5558629706\n",
      "Epoch: 480, Loss: 18.5428121759\n",
      "Epoch: 481, Loss: 18.5297855163\n",
      "Epoch: 482, Loss: 18.5167829021\n",
      "Epoch: 483, Loss: 18.5038042454\n",
      "Epoch: 484, Loss: 18.4908494601\n",
      "Epoch: 485, Loss: 18.477918462\n",
      "Epoch: 486, Loss: 18.4650111688\n",
      "Epoch: 487, Loss: 18.4521274998\n",
      "Epoch: 488, Loss: 18.4392673761\n",
      "Epoch: 489, Loss: 18.4264307205\n",
      "Epoch: 490, Loss: 18.4136174576\n",
      "Epoch: 491, Loss: 18.4008275134\n",
      "Epoch: 492, Loss: 18.3880608158\n",
      "Epoch: 493, Loss: 18.3753172941\n",
      "Epoch: 494, Loss: 18.3625968792\n",
      "Epoch: 495, Loss: 18.3498995035\n",
      "Epoch: 496, Loss: 18.3372251008\n",
      "Epoch: 497, Loss: 18.3245736066\n",
      "Epoch: 498, Loss: 18.3119449575\n",
      "Epoch: 499, Loss: 18.2993390917\n",
      "Epoch: 500, Loss: 18.2867559485\n",
      "Epoch: 501, Loss: 18.2741954688\n",
      "Epoch: 502, Loss: 18.2616575944\n",
      "Epoch: 503, Loss: 18.2491422687\n",
      "Epoch: 504, Loss: 18.2366494359\n",
      "Epoch: 505, Loss: 18.2241790417\n",
      "Epoch: 506, Loss: 18.2117310326\n",
      "Epoch: 507, Loss: 18.1993053563\n",
      "Epoch: 508, Loss: 18.1869019616\n",
      "Epoch: 509, Loss: 18.1745207983\n",
      "Epoch: 510, Loss: 18.1621618169\n",
      "Epoch: 511, Loss: 18.1498249692\n",
      "Epoch: 512, Loss: 18.1375102075\n",
      "Epoch: 513, Loss: 18.1252174853\n",
      "Epoch: 514, Loss: 18.1129467566\n",
      "Epoch: 515, Loss: 18.1006979765\n",
      "Epoch: 516, Loss: 18.0884711005\n",
      "Epoch: 517, Loss: 18.076266085\n",
      "Epoch: 518, Loss: 18.0640828872\n",
      "Epoch: 519, Loss: 18.0519214646\n",
      "Epoch: 520, Loss: 18.0397817755\n",
      "Epoch: 521, Loss: 18.0276637789\n",
      "Epoch: 522, Loss: 18.015567434\n",
      "Epoch: 523, Loss: 18.0034927009\n",
      "Epoch: 524, Loss: 17.9914395398\n",
      "Epoch: 525, Loss: 17.9794079116\n",
      "Epoch: 526, Loss: 17.9673977775\n",
      "Epoch: 527, Loss: 17.9554090991\n",
      "Epoch: 528, Loss: 17.9434418385\n",
      "Epoch: 529, Loss: 17.9314959578\n",
      "Epoch: 530, Loss: 17.9195714197\n",
      "Epoch: 531, Loss: 17.9076681871\n",
      "Epoch: 532, Loss: 17.895786223\n",
      "Epoch: 533, Loss: 17.8839254907\n",
      "Epoch: 534, Loss: 17.8720859538\n",
      "Epoch: 535, Loss: 17.8602675759\n",
      "Epoch: 536, Loss: 17.8484703208\n",
      "Epoch: 537, Loss: 17.8366941524\n",
      "Epoch: 538, Loss: 17.8249390347\n",
      "Epoch: 539, Loss: 17.8132049319\n",
      "Epoch: 540, Loss: 17.8014918079\n",
      "Epoch: 541, Loss: 17.789799627\n",
      "Epoch: 542, Loss: 17.7781283533\n",
      "Epoch: 543, Loss: 17.7664779508\n",
      "Epoch: 544, Loss: 17.7548483837\n",
      "Epoch: 545, Loss: 17.7432396159\n",
      "Epoch: 546, Loss: 17.7316516114\n",
      "Epoch: 547, Loss: 17.7200843341\n",
      "Epoch: 548, Loss: 17.7085377475\n",
      "Epoch: 549, Loss: 17.6970118153\n",
      "Epoch: 550, Loss: 17.6855065009\n",
      "Epoch: 551, Loss: 17.6740217676\n",
      "Epoch: 552, Loss: 17.6625575784\n",
      "Epoch: 553, Loss: 17.6511138961\n",
      "Epoch: 554, Loss: 17.6396906834\n",
      "Epoch: 555, Loss: 17.6282879028\n",
      "Epoch: 556, Loss: 17.6169055164\n",
      "Epoch: 557, Loss: 17.6055434861\n",
      "Epoch: 558, Loss: 17.5942017736\n",
      "Epoch: 559, Loss: 17.5828803402\n",
      "Epoch: 560, Loss: 17.571579147\n",
      "Epoch: 561, Loss: 17.5602981547\n",
      "Epoch: 562, Loss: 17.5490373239\n",
      "Epoch: 563, Loss: 17.5377966146\n",
      "Epoch: 564, Loss: 17.5265759865\n",
      "Epoch: 565, Loss: 17.5153753992\n",
      "Epoch: 566, Loss: 17.5041948118\n",
      "Epoch: 567, Loss: 17.4930341829\n",
      "Epoch: 568, Loss: 17.4818934709\n",
      "Epoch: 569, Loss: 17.4707726338\n",
      "Epoch: 570, Loss: 17.4596716292\n",
      "Epoch: 571, Loss: 17.4485904143\n",
      "Epoch: 572, Loss: 17.4375289459\n",
      "Epoch: 573, Loss: 17.4264871803\n",
      "Epoch: 574, Loss: 17.4154650737\n",
      "Epoch: 575, Loss: 17.4044625816\n",
      "Epoch: 576, Loss: 17.3934796592\n",
      "Epoch: 577, Loss: 17.3825162613\n",
      "Epoch: 578, Loss: 17.3715723421\n",
      "Epoch: 579, Loss: 17.3606478557\n",
      "Epoch: 580, Loss: 17.3497427554\n",
      "Epoch: 581, Loss: 17.3388569944\n",
      "Epoch: 582, Loss: 17.3279905254\n",
      "Epoch: 583, Loss: 17.3171433004\n",
      "Epoch: 584, Loss: 17.3063152714\n",
      "Epoch: 585, Loss: 17.2955063896\n",
      "Epoch: 586, Loss: 17.284716606\n",
      "Epoch: 587, Loss: 17.2739458711\n",
      "Epoch: 588, Loss: 17.2631941349\n",
      "Epoch: 589, Loss: 17.2524613471\n",
      "Epoch: 590, Loss: 17.2417474569\n",
      "Epoch: 591, Loss: 17.2310524131\n",
      "Epoch: 592, Loss: 17.2203761641\n",
      "Epoch: 593, Loss: 17.2097186578\n",
      "Epoch: 594, Loss: 17.1990798418\n",
      "Epoch: 595, Loss: 17.1884596632\n",
      "Epoch: 596, Loss: 17.1778580688\n",
      "Epoch: 597, Loss: 17.1672750049\n",
      "Epoch: 598, Loss: 17.1567104174\n",
      "Epoch: 599, Loss: 17.146164252\n",
      "Epoch: 600, Loss: 17.1356364536\n",
      "Epoch: 601, Loss: 17.1251269673\n",
      "Epoch: 602, Loss: 17.1146357372\n",
      "Epoch: 603, Loss: 17.1041627076\n",
      "Epoch: 604, Loss: 17.0937078221\n",
      "Epoch: 605, Loss: 17.0832710241\n",
      "Epoch: 606, Loss: 17.0728522565\n",
      "Epoch: 607, Loss: 17.0624514619\n",
      "Epoch: 608, Loss: 17.0520685828\n",
      "Epoch: 609, Loss: 17.0417035611\n",
      "Epoch: 610, Loss: 17.0313563385\n",
      "Epoch: 611, Loss: 17.0210268564\n",
      "Epoch: 612, Loss: 17.0107150558\n",
      "Epoch: 613, Loss: 17.0004208777\n",
      "Epoch: 614, Loss: 16.9901442624\n",
      "Epoch: 615, Loss: 16.9798851503\n",
      "Epoch: 616, Loss: 16.9696434813\n",
      "Epoch: 617, Loss: 16.9594191953\n",
      "Epoch: 618, Loss: 16.9492122317\n",
      "Epoch: 619, Loss: 16.9390225298\n",
      "Epoch: 620, Loss: 16.9288500286\n",
      "Epoch: 621, Loss: 16.9186946671\n",
      "Epoch: 622, Loss: 16.9085563838\n",
      "Epoch: 623, Loss: 16.8984351174\n",
      "Epoch: 624, Loss: 16.888330806\n",
      "Epoch: 625, Loss: 16.8782433878\n",
      "Epoch: 626, Loss: 16.8681728008\n",
      "Epoch: 627, Loss: 16.8581189829\n",
      "Epoch: 628, Loss: 16.8480818717\n",
      "Epoch: 629, Loss: 16.8380614049\n",
      "Epoch: 630, Loss: 16.82805752\n",
      "Epoch: 631, Loss: 16.8180701544\n",
      "Epoch: 632, Loss: 16.8080992454\n",
      "Epoch: 633, Loss: 16.7981447302\n",
      "Epoch: 634, Loss: 16.7882065461\n",
      "Epoch: 635, Loss: 16.7782846302\n",
      "Epoch: 636, Loss: 16.7683789197\n",
      "Epoch: 637, Loss: 16.7584893515\n",
      "Epoch: 638, Loss: 16.7486158629\n",
      "Epoch: 639, Loss: 16.7387583909\n",
      "Epoch: 640, Loss: 16.7289168725\n",
      "Epoch: 641, Loss: 16.719091245\n",
      "Epoch: 642, Loss: 16.7092814455\n",
      "Epoch: 643, Loss: 16.6994874112\n",
      "Epoch: 644, Loss: 16.6897090794\n",
      "Epoch: 645, Loss: 16.6799463873\n",
      "Epoch: 646, Loss: 16.6701992726\n",
      "Epoch: 647, Loss: 16.6604676727\n",
      "Epoch: 648, Loss: 16.6507515251\n",
      "Epoch: 649, Loss: 16.6410507678\n",
      "Epoch: 650, Loss: 16.6313653385\n",
      "Epoch: 651, Loss: 16.6216951754\n",
      "Epoch: 652, Loss: 16.6120402165\n",
      "Epoch: 653, Loss: 16.6024004002\n",
      "Epoch: 654, Loss: 16.592775665\n",
      "Epoch: 655, Loss: 16.5831659497\n",
      "Epoch: 656, Loss: 16.573571193\n",
      "Epoch: 657, Loss: 16.5639913342\n",
      "Epoch: 658, Loss: 16.5544263125\n",
      "Epoch: 659, Loss: 16.5448760674\n",
      "Epoch: 660, Loss: 16.5353405387\n",
      "Epoch: 661, Loss: 16.5258196664\n",
      "Epoch: 662, Loss: 16.5163133908\n",
      "Epoch: 663, Loss: 16.5068216523\n",
      "Epoch: 664, Loss: 16.4973443917\n",
      "Epoch: 665, Loss: 16.4878815502\n",
      "Epoch: 666, Loss: 16.478433069\n",
      "Epoch: 667, Loss: 16.4689988897\n",
      "Epoch: 668, Loss: 16.4595789543\n",
      "Epoch: 669, Loss: 16.4501732051\n",
      "Epoch: 670, Loss: 16.4407815846\n",
      "Epoch: 671, Loss: 16.4314040355\n",
      "Epoch: 672, Loss: 16.4220405013\n",
      "Epoch: 673, Loss: 16.4126909253\n",
      "Epoch: 674, Loss: 16.4033552514\n",
      "Epoch: 675, Loss: 16.3940334239\n",
      "Epoch: 676, Loss: 16.3847253873\n",
      "Epoch: 677, Loss: 16.3754310866\n",
      "Epoch: 678, Loss: 16.3661504669\n",
      "Epoch: 679, Loss: 16.356883474\n",
      "Epoch: 680, Loss: 16.3476300538\n",
      "Epoch: 681, Loss: 16.3383901528\n",
      "Epoch: 682, Loss: 16.3291637177\n",
      "Epoch: 683, Loss: 16.3199506956\n",
      "Epoch: 684, Loss: 16.310751034\n",
      "Epoch: 685, Loss: 16.3015646809\n",
      "Epoch: 686, Loss: 16.2923915845\n",
      "Epoch: 687, Loss: 16.2832316935\n",
      "Epoch: 688, Loss: 16.2740849571\n",
      "Epoch: 689, Loss: 16.2649513245\n",
      "Epoch: 690, Loss: 16.2558307459\n",
      "Epoch: 691, Loss: 16.2467231713\n",
      "Epoch: 692, Loss: 16.2376285514\n",
      "Epoch: 693, Loss: 16.2285468374\n",
      "Epoch: 694, Loss: 16.2194779807\n",
      "Epoch: 695, Loss: 16.210421933\n",
      "Epoch: 696, Loss: 16.2013786468\n",
      "Epoch: 697, Loss: 16.1923480746\n",
      "Epoch: 698, Loss: 16.1833301695\n",
      "Epoch: 699, Loss: 16.1743248849\n",
      "Epoch: 700, Loss: 16.1653321748\n",
      "Epoch: 701, Loss: 16.1563519933\n",
      "Epoch: 702, Loss: 16.1473842952\n",
      "Epoch: 703, Loss: 16.1384290354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 704, Loss: 16.1294861695\n",
      "Epoch: 705, Loss: 16.1205556532\n",
      "Epoch: 706, Loss: 16.1116374427\n",
      "Epoch: 707, Loss: 16.1027314948\n",
      "Epoch: 708, Loss: 16.0938377663\n",
      "Epoch: 709, Loss: 16.0849562147\n",
      "Epoch: 710, Loss: 16.0760867977\n",
      "Epoch: 711, Loss: 16.0672294735\n",
      "Epoch: 712, Loss: 16.0583842006\n",
      "Epoch: 713, Loss: 16.0495509379\n",
      "Epoch: 714, Loss: 16.0407296447\n",
      "Epoch: 715, Loss: 16.0319202806\n",
      "Epoch: 716, Loss: 16.0231228057\n",
      "Epoch: 717, Loss: 16.0143371802\n",
      "Epoch: 718, Loss: 16.005563365\n",
      "Epoch: 719, Loss: 15.9968013211\n",
      "Epoch: 720, Loss: 15.9880510099\n",
      "Epoch: 721, Loss: 15.9793123934\n",
      "Epoch: 722, Loss: 15.9705854335\n",
      "Epoch: 723, Loss: 15.9618700928\n",
      "Epoch: 724, Loss: 15.9531663342\n",
      "Epoch: 725, Loss: 15.9444741208\n",
      "Epoch: 726, Loss: 15.9357934161\n",
      "Epoch: 727, Loss: 15.9271241839\n",
      "Epoch: 728, Loss: 15.9184663885\n",
      "Epoch: 729, Loss: 15.9098199942\n",
      "Epoch: 730, Loss: 15.901184966\n",
      "Epoch: 731, Loss: 15.8925612689\n",
      "Epoch: 732, Loss: 15.8839488683\n",
      "Epoch: 733, Loss: 15.8753477302\n",
      "Epoch: 734, Loss: 15.8667578204\n",
      "Epoch: 735, Loss: 15.8581791054\n",
      "Epoch: 736, Loss: 15.8496115519\n",
      "Epoch: 737, Loss: 15.8410551268\n",
      "Epoch: 738, Loss: 15.8325097974\n",
      "Epoch: 739, Loss: 15.8239755312\n",
      "Epoch: 740, Loss: 15.8154522962\n",
      "Epoch: 741, Loss: 15.8069400604\n",
      "Epoch: 742, Loss: 15.7984387923\n",
      "Epoch: 743, Loss: 15.7899484605\n",
      "Epoch: 744, Loss: 15.7814690341\n",
      "Epoch: 745, Loss: 15.7730004823\n",
      "Epoch: 746, Loss: 15.7645427746\n",
      "Epoch: 747, Loss: 15.7560958808\n",
      "Epoch: 748, Loss: 15.7476597711\n",
      "Epoch: 749, Loss: 15.7392344156\n",
      "Epoch: 750, Loss: 15.7308197851\n",
      "Epoch: 751, Loss: 15.7224158503\n",
      "Epoch: 752, Loss: 15.7140225824\n",
      "Epoch: 753, Loss: 15.7056399527\n",
      "Epoch: 754, Loss: 15.6972679329\n",
      "Epoch: 755, Loss: 15.6889064948\n",
      "Epoch: 756, Loss: 15.6805556105\n",
      "Epoch: 757, Loss: 15.6722152524\n",
      "Epoch: 758, Loss: 15.6638853931\n",
      "Epoch: 759, Loss: 15.6555660053\n",
      "Epoch: 760, Loss: 15.6472570623\n",
      "Epoch: 761, Loss: 15.6389585374\n",
      "Epoch: 762, Loss: 15.630670404\n",
      "Epoch: 763, Loss: 15.622392636\n",
      "Epoch: 764, Loss: 15.6141252074\n",
      "Epoch: 765, Loss: 15.6058680924\n",
      "Epoch: 766, Loss: 15.5976212656\n",
      "Epoch: 767, Loss: 15.5893847017\n",
      "Epoch: 768, Loss: 15.5811583756\n",
      "Epoch: 769, Loss: 15.5729422624\n",
      "Epoch: 770, Loss: 15.5647363375\n",
      "Epoch: 771, Loss: 15.5565405766\n",
      "Epoch: 772, Loss: 15.5483549555\n",
      "Epoch: 773, Loss: 15.5401794502\n",
      "Epoch: 774, Loss: 15.5320140369\n",
      "Epoch: 775, Loss: 15.5238586923\n",
      "Epoch: 776, Loss: 15.5157133929\n",
      "Epoch: 777, Loss: 15.5075781156\n",
      "Epoch: 778, Loss: 15.4994528377\n",
      "Epoch: 779, Loss: 15.4913375363\n",
      "Epoch: 780, Loss: 15.4832321892\n",
      "Epoch: 781, Loss: 15.4751367739\n",
      "Epoch: 782, Loss: 15.4670512685\n",
      "Epoch: 783, Loss: 15.4589756511\n",
      "Epoch: 784, Loss: 15.4509099002\n",
      "Epoch: 785, Loss: 15.4428539942\n",
      "Epoch: 786, Loss: 15.434807912\n",
      "Epoch: 787, Loss: 15.4267716325\n",
      "Epoch: 788, Loss: 15.4187451349\n",
      "Epoch: 789, Loss: 15.4107283987\n",
      "Epoch: 790, Loss: 15.4027214033\n",
      "Epoch: 791, Loss: 15.3947241286\n",
      "Epoch: 792, Loss: 15.3867365545\n",
      "Epoch: 793, Loss: 15.3787586612\n",
      "Epoch: 794, Loss: 15.3707904292\n",
      "Epoch: 795, Loss: 15.3628318388\n",
      "Epoch: 796, Loss: 15.354882871\n",
      "Epoch: 797, Loss: 15.3469435066\n",
      "Epoch: 798, Loss: 15.3390137267\n",
      "Epoch: 799, Loss: 15.3310935128\n",
      "Epoch: 800, Loss: 15.3231828463\n",
      "Epoch: 801, Loss: 15.3152817089\n",
      "Epoch: 802, Loss: 15.3073900825\n",
      "Epoch: 803, Loss: 15.2995079491\n",
      "Epoch: 804, Loss: 15.2916352911\n",
      "Epoch: 805, Loss: 15.2837720909\n",
      "Epoch: 806, Loss: 15.275918331\n",
      "Epoch: 807, Loss: 15.2680739943\n",
      "Epoch: 808, Loss: 15.2602390637\n",
      "Epoch: 809, Loss: 15.2524135225\n",
      "Epoch: 810, Loss: 15.2445973539\n",
      "Epoch: 811, Loss: 15.2367905414\n",
      "Epoch: 812, Loss: 15.2289930688\n",
      "Epoch: 813, Loss: 15.2212049198\n",
      "Epoch: 814, Loss: 15.2134260785\n",
      "Epoch: 815, Loss: 15.2056565291\n",
      "Epoch: 816, Loss: 15.1978962559\n",
      "Epoch: 817, Loss: 15.1901452435\n",
      "Epoch: 818, Loss: 15.1824034765\n",
      "Epoch: 819, Loss: 15.1746709398\n",
      "Epoch: 820, Loss: 15.1669476184\n",
      "Epoch: 821, Loss: 15.1592334975\n",
      "Epoch: 822, Loss: 15.1515285624\n",
      "Epoch: 823, Loss: 15.1438327986\n",
      "Epoch: 824, Loss: 15.1361461917\n",
      "Epoch: 825, Loss: 15.1284687275\n",
      "Epoch: 826, Loss: 15.120800392\n",
      "Epoch: 827, Loss: 15.1131411713\n",
      "Epoch: 828, Loss: 15.1054910516\n",
      "Epoch: 829, Loss: 15.0978500193\n",
      "Epoch: 830, Loss: 15.0902180609\n",
      "Epoch: 831, Loss: 15.0825951631\n",
      "Epoch: 832, Loss: 15.0749813127\n",
      "Epoch: 833, Loss: 15.0673764967\n",
      "Epoch: 834, Loss: 15.0597807022\n",
      "Epoch: 835, Loss: 15.0521939163\n",
      "Epoch: 836, Loss: 15.0446161266\n",
      "Epoch: 837, Loss: 15.0370473203\n",
      "Epoch: 838, Loss: 15.0294874852\n",
      "Epoch: 839, Loss: 15.021936609\n",
      "Epoch: 840, Loss: 15.0143946796\n",
      "Epoch: 841, Loss: 15.0068616849\n",
      "Epoch: 842, Loss: 14.999337613\n",
      "Epoch: 843, Loss: 14.9918224523\n",
      "Epoch: 844, Loss: 14.984316191\n",
      "Epoch: 845, Loss: 14.9768188176\n",
      "Epoch: 846, Loss: 14.9693303207\n",
      "Epoch: 847, Loss: 14.9618506889\n",
      "Epoch: 848, Loss: 14.954379911\n",
      "Epoch: 849, Loss: 14.946917976\n",
      "Epoch: 850, Loss: 14.9394648727\n",
      "Epoch: 851, Loss: 14.9320205904\n",
      "Epoch: 852, Loss: 14.9245851181\n",
      "Epoch: 853, Loss: 14.9171584453\n",
      "Epoch: 854, Loss: 14.9097405612\n",
      "Epoch: 855, Loss: 14.9023314553\n",
      "Epoch: 856, Loss: 14.8949311172\n",
      "Epoch: 857, Loss: 14.8875395366\n",
      "Epoch: 858, Loss: 14.8801567032\n",
      "Epoch: 859, Loss: 14.8727826067\n",
      "Epoch: 860, Loss: 14.8654172371\n",
      "Epoch: 861, Loss: 14.8580605844\n",
      "Epoch: 862, Loss: 14.8507126385\n",
      "Epoch: 863, Loss: 14.8433733897\n",
      "Epoch: 864, Loss: 14.8360428281\n",
      "Epoch: 865, Loss: 14.8287209439\n",
      "Epoch: 866, Loss: 14.8214077276\n",
      "Epoch: 867, Loss: 14.8141031693\n",
      "Epoch: 868, Loss: 14.8068072597\n",
      "Epoch: 869, Loss: 14.7995199892\n",
      "Epoch: 870, Loss: 14.7922413484\n",
      "Epoch: 871, Loss: 14.7849713279\n",
      "Epoch: 872, Loss: 14.7777099183\n",
      "Epoch: 873, Loss: 14.7704571104\n",
      "Epoch: 874, Loss: 14.763212895\n",
      "Epoch: 875, Loss: 14.7559772627\n",
      "Epoch: 876, Loss: 14.7487502046\n",
      "Epoch: 877, Loss: 14.7415317115\n",
      "Epoch: 878, Loss: 14.7343217742\n",
      "Epoch: 879, Loss: 14.7271203839\n",
      "Epoch: 880, Loss: 14.7199275314\n",
      "Epoch: 881, Loss: 14.7127432078\n",
      "Epoch: 882, Loss: 14.7055674042\n",
      "Epoch: 883, Loss: 14.6984001116\n",
      "Epoch: 884, Loss: 14.6912413212\n",
      "Epoch: 885, Loss: 14.6840910241\n",
      "Epoch: 886, Loss: 14.6769492115\n",
      "Epoch: 887, Loss: 14.6698158744\n",
      "Epoch: 888, Loss: 14.6626910042\n",
      "Epoch: 889, Loss: 14.655574592\n",
      "Epoch: 890, Loss: 14.6484666291\n",
      "Epoch: 891, Loss: 14.6413671066\n",
      "Epoch: 892, Loss: 14.6342760158\n",
      "Epoch: 893, Loss: 14.627193348\n",
      "Epoch: 894, Loss: 14.6201190943\n",
      "Epoch: 895, Loss: 14.6130532462\n",
      "Epoch: 896, Loss: 14.6059957947\n",
      "Epoch: 897, Loss: 14.5989467312\n",
      "Epoch: 898, Loss: 14.5919060469\n",
      "Epoch: 899, Loss: 14.5848737331\n",
      "Epoch: 900, Loss: 14.577849781\n",
      "Epoch: 901, Loss: 14.5708341819\n",
      "Epoch: 902, Loss: 14.5638269269\n",
      "Epoch: 903, Loss: 14.5568280074\n",
      "Epoch: 904, Loss: 14.5498374144\n",
      "Epoch: 905, Loss: 14.5428551392\n",
      "Epoch: 906, Loss: 14.535881173\n",
      "Epoch: 907, Loss: 14.5289155069\n",
      "Epoch: 908, Loss: 14.5219581321\n",
      "Epoch: 909, Loss: 14.5150090397\n",
      "Epoch: 910, Loss: 14.5080682208\n",
      "Epoch: 911, Loss: 14.5011356664\n",
      "Epoch: 912, Loss: 14.4942113678\n",
      "Epoch: 913, Loss: 14.4872953158\n",
      "Epoch: 914, Loss: 14.4803875015\n",
      "Epoch: 915, Loss: 14.4734879159\n",
      "Epoch: 916, Loss: 14.46659655\n",
      "Epoch: 917, Loss: 14.4597133946\n",
      "Epoch: 918, Loss: 14.4528384408\n",
      "Epoch: 919, Loss: 14.4459716793\n",
      "Epoch: 920, Loss: 14.439113101\n",
      "Epoch: 921, Loss: 14.4322626967\n",
      "Epoch: 922, Loss: 14.4254204572\n",
      "Epoch: 923, Loss: 14.4185863732\n",
      "Epoch: 924, Loss: 14.4117604356\n",
      "Epoch: 925, Loss: 14.4049426348\n",
      "Epoch: 926, Loss: 14.3981329617\n",
      "Epoch: 927, Loss: 14.3913314068\n",
      "Epoch: 928, Loss: 14.3845379606\n",
      "Epoch: 929, Loss: 14.3777526138\n",
      "Epoch: 930, Loss: 14.3709753569\n",
      "Epoch: 931, Loss: 14.3642061802\n",
      "Epoch: 932, Loss: 14.3574450743\n",
      "Epoch: 933, Loss: 14.3506920296\n",
      "Epoch: 934, Loss: 14.3439470363\n",
      "Epoch: 935, Loss: 14.337210085\n",
      "Epoch: 936, Loss: 14.3304811657\n",
      "Epoch: 937, Loss: 14.3237602689\n",
      "Epoch: 938, Loss: 14.3170473847\n",
      "Epoch: 939, Loss: 14.3103425033\n",
      "Epoch: 940, Loss: 14.3036456149\n",
      "Epoch: 941, Loss: 14.2969567096\n",
      "Epoch: 942, Loss: 14.2902757774\n",
      "Epoch: 943, Loss: 14.2836028085\n",
      "Epoch: 944, Loss: 14.2769377929\n",
      "Epoch: 945, Loss: 14.2702807204\n",
      "Epoch: 946, Loss: 14.2636315812\n",
      "Epoch: 947, Loss: 14.2569903651\n",
      "Epoch: 948, Loss: 14.2503570621\n",
      "Epoch: 949, Loss: 14.2437316619\n",
      "Epoch: 950, Loss: 14.2371141545\n",
      "Epoch: 951, Loss: 14.2305045296\n",
      "Epoch: 952, Loss: 14.223902777\n",
      "Epoch: 953, Loss: 14.2173088866\n",
      "Epoch: 954, Loss: 14.210722848\n",
      "Epoch: 955, Loss: 14.2041446511\n",
      "Epoch: 956, Loss: 14.1975742853\n",
      "Epoch: 957, Loss: 14.1910117406\n",
      "Epoch: 958, Loss: 14.1844570065\n",
      "Epoch: 959, Loss: 14.1779100726\n",
      "Epoch: 960, Loss: 14.1713709287\n",
      "Epoch: 961, Loss: 14.1648395643\n",
      "Epoch: 962, Loss: 14.1583159691\n",
      "Epoch: 963, Loss: 14.1518001326\n",
      "Epoch: 964, Loss: 14.1452920445\n",
      "Epoch: 965, Loss: 14.1387916944\n",
      "Epoch: 966, Loss: 14.1322990718\n",
      "Epoch: 967, Loss: 14.1258141664\n",
      "Epoch: 968, Loss: 14.1193369677\n",
      "Epoch: 969, Loss: 14.1128674653\n",
      "Epoch: 970, Loss: 14.1064056489\n",
      "Epoch: 971, Loss: 14.099951508\n",
      "Epoch: 972, Loss: 14.0935050323\n",
      "Epoch: 973, Loss: 14.0870662114\n",
      "Epoch: 974, Loss: 14.080635035\n",
      "Epoch: 975, Loss: 14.0742114927\n",
      "Epoch: 976, Loss: 14.0677955743\n",
      "Epoch: 977, Loss: 14.0613872694\n",
      "Epoch: 978, Loss: 14.0549865679\n",
      "Epoch: 979, Loss: 14.0485934594\n",
      "Epoch: 980, Loss: 14.0422079337\n",
      "Epoch: 981, Loss: 14.0358299809\n",
      "Epoch: 982, Loss: 14.0294595906\n",
      "Epoch: 983, Loss: 14.0230967528\n",
      "Epoch: 984, Loss: 14.0167414576\n",
      "Epoch: 985, Loss: 14.0103936949\n",
      "Epoch: 986, Loss: 14.0040534547\n",
      "Epoch: 987, Loss: 13.9977207273\n",
      "Epoch: 988, Loss: 13.9913955027\n",
      "Epoch: 989, Loss: 13.9850777712\n",
      "Epoch: 990, Loss: 13.9787675232\n",
      "Epoch: 991, Loss: 13.9724647489\n",
      "Epoch: 992, Loss: 13.9661694389\n",
      "Epoch: 993, Loss: 13.9598815836\n",
      "Epoch: 994, Loss: 13.9536011737\n",
      "Epoch: 995, Loss: 13.9473281997\n",
      "Epoch: 996, Loss: 13.9410626525\n",
      "Epoch: 997, Loss: 13.9348045229\n",
      "Epoch: 998, Loss: 13.9285538018\n",
      "Epoch: 999, Loss: 13.9223104802\n"
     ]
    }
   ],
   "source": [
    "from stochastic_gradient_descent.nn import *\n",
    "from stochastic_gradient_descent.miniflow import *\n",
    "main8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
