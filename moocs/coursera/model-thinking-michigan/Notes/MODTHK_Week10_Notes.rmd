<center> <h2>Model Thinking - Week 10</h2> </center>
<center> <h3>---------------------------------------</h3> </center>

####*Reading 1: Markov Processes*

[Markov Processes](http://www.cscs.umich.edu/~spage/ONLINECOURSE/R10Markov.pdf)

**A. The Basics**

- think about 1-1 linkages between states of the world
- components of a Markov process:
    - finite numbero f states
    - transition probabilities for moving between those states
- Markov processes settle into long-run equilibria which are completely independent of starting conditions or intermediate states...totally determined by transition probabilities

**B. Democractic Vistas**

- lots of switching betweens states implies that at Markov provess might be more appropriate than linear extrapolation

**C. Happy or Sad: a Two State Markov Process**

- important note: Markov process imply *system* equilibria, not individual equilibria:
    - there could still be lots of churning between states
    - statistical equilbrium = expected number of people in each state remains unchanged

**D. The Ergodic Theorem**

- conditions for a Markov process:
    1. The state of the process at any point in time belongs to a finite set of possible states.
    2. The state of the process at the next point in time depends only on its current state and does so according to fixed transition probabilities.
    3. It is posisble through a series of transitions to get from any one state to any other.
    4. The system does not produce a deterministc cycle through a sequence of states.
- "Any Markov Process converges to a unique statistical equilibrium that does not depend on the initial state of the process or any one time changes to the state during the history of the process"
    - i.e. no path dependence
    - relies on the crucial condition that "it is possible through a series of transitions to get from any one state to any other"

**E. Fertility of the Markov Model**

- we think that history should matter, but this doesn't invalidate the use of Markov models
    - comparing Markov results to time-series will give you a sense of how much history REALLY matters

**F. Outside the Box**

- remember Shannon's Markov chain of letters/words?
- "...models aren't always correct. They're very often wrong. But what they do accomplish is focusing our thinking on some aspects of a situation and thinking through the logic clearly."

**G. Takeaways**

1. If we want to say that history matters, then we first have to say why history cannot be captured by a Markov model.
2. In a system with many entities moving in and out of states, beware of inferring too much from initial linear trends. Pay attention to the long run equilibrium.
3. If a system is Markov, changing the current state has no long term impact. Making someone laugh won't change their long term demeanor, though it will give a temporary burst of joy.
4. FUndamental change requires changing transition probabilities. It requires changing processes.
5. Different processes produce distinct Markov model approximations. Therefore, we can fit Markov models to identify processes and to see the effects of interventions on long run equilibria.
    
####*Video 10.1 - Markov Models*

**A. Notes**

- Markov models are characterized by states and associated transition probabilities
    - over time, this converges to a statistical equilibrium

**B. Matrices**

- Markov models rely on matrix notation and manipulation
- we're going to learn how to multiply states by a matrix of transition probabilities

**C. Exaptation**

- we'll get this powerful finding about equilibria of systems
- this is a very fertile model...almost anything can be thought of as markov-ish

####*Video 10.2 - A Simple Markov Model*

**A. How It Works**

- Markov Transition Matrix:

|       | A(t) | B(t) |
|------:|-----:|-----:|
|A(t+1) | 0.8  | 0.25 |
|B(t+1) | 0.2  | 0.75 |

- multiplying these matrices by some state gives you the new state
- these processes converge to the same equilibrium regardless of starting point:
    - history doesn't matter

**B. Finding Equilibrium**

- you want to find the point where the shares in each state don't change
- equilibrium point = nothing changes
- statistical equilibrium = probabilities are staying fixed (statistics don't change) even though individuals are consantly churning

####*Video 10.3 - Markov Model of Democratization*

**A. More Sophistication (3 states)**

- Markov models cannot take you to a "lock-in" outcome (100% in one class)
- DON'T WORRY - with computers, you can make big matrices and easily solve for equilibrium
- the takeaway:
    - if you want to change the long-run equilibrium, you need to change the transition probabilities (not just the current state)

####*Video 10.4 - Markov Convergence Theorem*

**A. Markov Convergence Theorem**

- the assumptions that need to be satisfied for a Markov process to converge to an equilibrium
- rules:
    1. Finite number of states
    2. Fixed transition probabilities
    3. Can eventually get from any one state to any other (via transitions)
    4. Not a simple, deterministic cycle
- Markov Convergence Theorem - given the four above, a Markov process converges to an equilibrium distribution which is unique
    - the key is that the equilibrium is UNIQUE
- an implication is that if this thing truly follows a Markov process, history doesn't matter
    - the equilibrium is independent of current or starting states
    - interventions to change the state don't matter

**B. Dealing with These Assumptions**

- don't read too much into these assumptions:
    - the model says nothing about how long it takes to reach equilibrium
    - real-world social systems don't follow these processes in a rigid way
    - transition probabilities probably change!
- if the system you're looking at is Markov-y, focus policy interventions on changing transition probabilities, not states
- everything isn't a Markov process, but Markov processes help to focus our thinking

####*Video 10.5 - Exapting the Markov Model*

**A. Using the Markov Model for Other Problems**

- entities move between states according to transition probabilities

**B. Voter Turnout**

- you could get a Markov transition matrix for switching between voting and non-voting behavior
- similar approach for school enrollment

**C. A More Interesting Application**

- take the transition matrix and use it to:
    1. identify writers by word choice
    2. identify the completion of a sequence by observing only initial steps of it (taxi cab Kaggle, drug users failure vs. success)
    3. identify patterns (lead up to war?)
- even if you don't care about the long-run equilibrium, that transition matrix is a great way to organize the data and organize yoru thinking
