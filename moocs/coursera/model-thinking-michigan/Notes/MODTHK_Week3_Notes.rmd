<center> <h2>Model Thinking - Week 3</h2> </center>
<center> <h3>---------------------------------------</h3> </center>

####*Reading 1: Sampling Distributions and the CLT*

**A. Sampling Distributions**

- sampling distribution = "probability distribution of a statistic"
- standard error = "standard deviation of the sampling distribution"
- if the population size is much larger than the sample, the standard error will be the same whether we sample with or without replacement
    - however, if the sample represents a significant fraction (maybe 1/20) of the population size, the standard error will be meaningfully smaller when we sample without replacement

**B. Samping Distribution of the Mean**

- the finite population correction (FPC) represents the standard deviation adjustment made for sample size:
    - sqrt[(N - n) / (N - 1)]
- when the population size is very large relative to the sample size, the FPC is approximately equal to 1 and the standard error fomula can be approximated by just dividing the sample standard deviation by sqrt(n)
- this approximation (with no FPC) is often overlook in introductory statistics texts

**C. Sampling Distribution of the Proportion**

- imagine we draw a bunch of possible samples from, run test, and measures the proportion of successes *p* and failures *q*. This is what we mean when we say a "sampling distribution of the proportion"
- mean of the sampling distribution of the proportion is equal to the probability of success in the population (P)
    - standard error of the sampling distribution is determined by the standard deviation of the population, thep opulation size, and the sample size
- this also includes a finitie population correction (FPC), usually overlooked or approximated away

**D. Central Limit Theorem**

- "the sampling distribution of any statistic will be normal or nearly normal, if the sample size is large enough"

**E. T-Distribution vs. Normal Distribution**

- if the population standard deviation is known, use the normal distribution
- if the population standard deviation is unknown, use the t-distribution
    - t dist also better for smaller samples
- "the t distribution should not be used with small samples from populations that are not approximately normal"

####*Reading 2 - Binomial Probability Distribution*

- source [HERE](http://stattrek.com/probability-distributions/binomial.aspx)

**A. Binomial Experiment**

- the binomial experiment is a statistical experiment with the following properties:
    - *n* repeated trials
    - each trial can result in just two possible outcomes (success or failure)
    - the probability of success, denoted by *P*, is the same on every trial
    - the trials are independent
- a binomial random variable is the number of successes x in n repeated trials of a binomial experiment
- the probability distribution of a binomial random variable is called a binomial distribution
- properties of this distribution: 
    1. mean = nP
    2. variance = nP(1-p)
    
**B. Binomial Formula and Binomial Probability**

- the binomial probability refers to the probability that a binomial experiment results in exactly *x* successes
- b(x: n, P) = nC~x~P^x^(1-P)^n-x^, where x is the exact number of successes, n is the number of trials, P is the probability of success, and nC~x~ is the number of combinations of n things, taken x at a time (choose x in n)
    - nC~x~ = "choose x in n" = n!/[x!(n-x)!]

**C. Cumulative Binomial Probability**

- this refers to the probability that the binomial random variables falls within a specified range (e.g. is greater than or equal to a stated lower limit and less than or equal to a state upper limit)
    - this has to be estimated by adding up the probabilities of each individual state in the range (see formula above)

####*Reading 3 - Six Sigma*

- source [HERE](https://en.wikipedia.org/wiki/Six_Sigma)

**A. Introduction**

- tools for process improvement
- control methods and people within the organization (e.g. "black belts") who manage them
- "The maturity of a manufacturing process can be described by a *sigma* rating indicating its yield or the percentage of defect-free products is creates. A six sigma process is one in which 99.99966% of all opportunities to produce some feature of a part are statistically expected to be free of defects (3.4 defective features / millions opportunities). 

**B. Guiding principles**

- clear focus on achieving measurable and quantifiable financial returns from any Six Sigma project
- an increased emphasis on strong and passionate management leadership and support
- clear commitment to making decisions on the basis of verifiable data and statistical methods, rather than assumptions and guesswork
- goal: 3.4 defects per million opportunities (DPMO)

**C. Etymology of "six sigma process"**

- capability studies measure the number of standard deviations between the process mean and the nearest specification limit in sigma units
- "as process standard deviation goes up, or the mean of the process moves away from the center of the tolerance, fewer standard deviations will fit between the mean and the nearest specification limit, decreasing the sigma number and increasing the likelihood of items outside specification"

**D. Criticism**

- Six Sigma is "narrowly designed to fix exisiting processes"
- however, it cannot help in "coming up with new products or disruptive technologies"
- there is rampant misuse of P-values, multiple regression, other stats
- "one of the most serious but all-too-common misuses of inferential statistics is to take a model that was developed through exploratory model building and subject it to the same sorts of statistical tests that are used to validate a model that was specified in advance"
- Wharton School research: "Six Sigma leads to incremental innovation at the expense of blue skies research"
    - Six Sigma is probably not appropriate in a research environment

####*Reading 4 - Cellular Automata 1*

"Chapter 8 - Complex Adaptive Social Systems in One Dimension"

**A. Notes**

- exploring some models hoping to "...provide some useful glimpses into the behavior of complex adaptive social systems"
- models of complex social systems as "devilishly simple"

**B. Simple Model**

- agents make decisions based on the decisions of their neighbors
- difference between social and physical systems:
> A curical difference between models of complex social and physical systems is in our assumptions about appropriate behavioral rules. ...one hydrogen atom acts just like another hydrogen atom relying on a set of fixed, external physical properties and forces. SOcial agents, on the other hand, often alter their behavior in response to, and in anticipation of, the actions of others.

- micro agents interact in ways that produce macro phenomena which in turn feed back into the micro-level interactions in various ways
- the most important idea here is that of *expectations*...social agents (people?) change their behavior based on expectations of other agents' behaviors

**C. Cellular Automata**

- rule table = mapping for each possible input state to an output state
- in this simple rule example;
    - "...even though individual behavior is based on tha actions observed at three sites, coherent triangular structures emerge that encompass far more sites
- all finite, deterministic systems are guaranteed to cycle, but the lengths of these cycles can be relatively long
- "simple local interactions among agents can result in interesting aggregate behavior"
- the behavior of a single rule can fall into different broad classifications depending on the initial conditions

####*Reading 5 - Cellular Automata 2*

**A. Is There an Edge?**

- in models of the edge of chaos there is an attempt to detect whether a given rule will tend to imply a system that is either chaotic, stable, or poised somehwere in between
- the "edge" is not in phase space, but in the space of rules
    - "if we slightly perturb a rule that generates complexity we will get a rule that either generates chaos or stasis"
- Wolfram's classification of automata:
    - fixed
    - periodic
    - chaotic
    - complex
- Langton's lambda
    - an attempt to model the edge of chaos as some location on a semi-continuous spectrum
- from Langton: "complexity occurs at the edge of chaos"

**B. Multiple Edges**

- there is not "an" edge of chaos
- "Rule 126 tends to create 1's then destroy them"
- "Everyone one of the rules classified as complex in this space has at least one chaotic neighbor with a lower lambda value and one with a higher value"
- maybe a different classification:
    - identity rules
    - blinker rules
    - 0-attractor rules
    - 1-attractor rules
- attractor rules produce chaos because they have, at their  core, a propensity for creative destruction
- "stability does not lie at the edge of periodicity"
- the author's deconstruction, which notes that the majority of chaotic rules and all complex rules belong to "Attractor" sets, lends support to the notion of an edge of chaos
    - however, not a single edge as usually supposed, but rather a multitude of edges contained within the set of attractor rules
- Langton's lambda is a sufficient but not necessary condition for interesting behavior
- consider reading "Computation at the Edge of Chaos" by Christopher Langton

####*Video 3.1 - Aggregation*

**A. "More is Different"**

- You can't always understand the whole by looking at the parts
- one water molecule can't be wet, can't have viscosity, can't ripple
    - "wetness is a property of a bunch of water molecules, but not one molecule"
- "personality, consciousness, cognition" as emergent properties of the "system" of neurons in your brain

**B. A Plan**

- start with aggregations of actions (CLT)
- "Game of Life" = Single Rule
    - what if we had a family of rules?
    - really simple models can produce a wide range of behaviors
- how do you aggregate preferences (ordered groups)?

**C. Central Limit Theorem**

- if things follow a known distribution, we can make good predictions just on the basis of a small sample

**D. Understand Patterns**

- we can see patterns in the macro structure emerging from micro-level rules and behaviors

**E. Cellular Automata**

- look at different behaviors of cells following very simple rules
    - any configuration is possible even with just simple rules
    - we can get patterns, equilibrium, randomness, complexity

**F. Big Picture**

- most interesting stuff in social science is "groups of people"
    - understand how the parts work
    - understand how to add them up

####*Video 3.2 - Central Limit Theorem*

**A. Introduction**

- probability distribution = all possible states (number of outcomes) and associated probabilities
    - probabilities add up to 1
- CLT = "If I add up a bunch of independent events, you get a normal probability distribution of outcome frequencies"

**B. Flip Coin 2 Times**

- what is the probability?
    - TT = 1/4
    - 1H = 1/2
    - HH = 1/4
- 4 flips?
    - TTTT = 1/16
    - 1H = 4/16
    - 2H = 6/16
    - 3H = 4/16
    - HHHH = 1/16

**C. Binomial Distribution**

- $\mu$ = Mean = pN, where p is the probability of success and N is the number of trials
- $\sigma$ = std deviation = $\frac{\sqrt{N}}{2}$

<img src="http://blog.kanbanize.com/wp-content/uploads/2014/07/Standard_deviation_diagram.png" width=75% height=50%x>

- CLT gives us not just the average, but the "spread"

**D. What about the more general case?**

- for binomial dis with arbitrary p:
    - $\mu$ = Mean = pN, where p is the probability of success and N is the number of trials
    - $\sigma$ = std deviation = $\sqrt{p(1-p)N}$

**E. An Example**

- Boeing 747: 380 seats
- 90% show up rate
- sell 400 tickets
- what is the likelihood of having more than 380 show up?
    - mean = 360 tickets
    - sd = SQRT(.9*.1*400) = 6
- 380 = 20/6 s.d. = 3.33 s.d

**F. Formal CLT Definition**

- add random variables:
    - independent
    - finite variance
- sum to a normal distribution
- a lot of the predictability of the world comes from the facts that you should get things averaging out to normal-ish behavior

**G. Is Everything Normally Distributed?**

- definitely not
- stock prices are not:
    - fat tails as a result of non-independence
    - gives you more big events or small events than might be expected for a normally-distributed variable

####*Video 3.3 - Six Sigma*

**A. What is It?**

- Using stats to make production processes more predictable
- outside of 6 sigma?
    - 3.4 in a million
- Six Sigma might give you a "sigma target"
    - your goal is to reduce the standard deviation
    - if you do this, you can be very very confident in the quality of your work
    - driving "continuous improvement"

####*Video 3.4 - Game of Life*

**A. Why Learn This?**

- GOL is a toy model, just meant to show off some key concepts
- [Conway's game of life](https://en.wikipedia.org/wiki/Conway's_Game_of_Life)
     - a particular type of cellular automata model

**B. Running the game**

- "seeding" the model = setting initial conditions
- use netlogo to set up
- simple things following simple rules can aggregate into complicated and fascinating patterns
- The "F-pimento" seed results in stunning behavior.
    - the blocks seem to grow, multiple, live and explore the entire space
- four classes of outcomes from this model:
    - fixed equilibrium
    - simple alternation
    - randomness
    - complex patterns
- anything a computer can do can be created by the game of life
- what do we get?
    - organization
    - emergence (functionalities appear: gliders, glider guns, counters, computers)
    
####*Video 3.5 - Cellular Automata*

**A. Some History**

- jon von neumann invented cellular automata
    - von neumann was also one of the founders of game theory and economic growth theory
- Stephen Wolfram - *A New Kind of Science*
    - cellular automata as "computation, inductive" approach to learning about the world
    - logic (simple rules produce incredible phenomena

**B. Making it Work**

- we can study every single rule, see what they produce

**C. It From Bit**

- maybe all complexity comes from simple binary interactions
- John Wheeler: "It from bit. Every particle, every structure, derives it's structure from the answers to yes/no questions. All things physical are information-theoeretic in origin, and this is a participatory universe."
    - the bottom of reality is binary switches
- the cellular automata model can produce pretty much anything

**E. Langton's lambda**

- Langton's lambda tells us the percentage of bits that are on
- 0: die off
- 1/8: blink
- 4/8: chaos
- 5/8: complex
- middle range of lambda's is where you see random or complex behavior
    - chaos and complexity comes from "intermediate levels of interdependence"
- complexity comes from a lot of interdependence

**F. What Do We Learn**

- simple rules combine to form anything
- "it from bit"
- complexity and randomness require interdependency

####*Video 3.6 - Preference Aggregation*

**A. This is Difffernet**

- not just "adding things up"
- we have a bunch of orderings (e.g. A > B > C, C > A > B, etc.)
- we are looking for "revealed preferences"
    - you get an ordering within a certain class or limited set of options
- preferences must satistfy transitivity
    - e.g. if A > B and B > C, A > C
- transitive preference set = n!
    - so for n = 3 possibilities, you get 3 x 2 x 1 = 6 possible transitive preferences
- you can get irrational, non-transitive communal preferences from aggregation of rationa, transitive preferences
- Condorecet's Paradox:
    - each person has rational preferences
    - when we vote, non-rational outcomes prevail
- crowds are NOT guaranteed to be rational (not even a little bit, oh lord)
- main takeaway: observed macro phenomena are not always representative of any particular individual micro units
    - micro units with one nature can create (maybe inadvertently) very different and interesting macro behavior through their interactions
>>>>>>> 3420dfebdf0240b9e98eb24ee1319873cc42b92b
