For a prediction approach using boosting, I employed the [GBM method](http://www.saedsayad.com/docs/gbm2.pdf) (boosting with regression trees).

In the lectures and quizzes, we mostly assumed that the ```caret``` defaults were good starting points. For this analysis, I opted to pass my own tuning parameters to gbm in an attempt to improve the analysis. Per [Max Kun](http://www.jstatsoft.org/v28/i05/paper), this can be done by defining a grid of potential tuning parameters.

For this application, I chose to have ```gbm``` choose from 10-100 boosting iterations, 1-6 tree splits ("interaction depth", in gbm), and a more deliberate learning rate (shrinkage = 0.1).

```{r sec3gbm, echo = TRUE, eval= TRUE, messages = FALSE, warning = FALSE}
## gbm with caret defaults
mod1 <- train(classe ~., method = "gbm", data = traindata1, verbose = FALSE)

## evaluate the accuracy
pred1 <- predict(mod1, traindata1, n.trees
confusionMatrix(pred1, traindata1$classe)
```

create gbmGrid

```{r sec3gbm, echo = TRUE, eval= TRUE, messages = FALSE, warning = FALSE}
gbmGrid <- expand.grid(.interaction.depth=(1:3)*2, .n.trees=100, .shrinkage=.1)
mod2 <- train(classe ~., data=traindata1, method="gbm", verbose = FALSE, tuneGrid = gbmGrid)

## evaluate the accuracy
pred2 <- predict.gbm(mod2, traindata1, n.trees=100)
confusionMatrix(pred2, traindata1$classe)
```



Delete near-zero-variance features
```{r sec1nearzero, echo = TRUE, eval= TRUE, messages = FALSE, warning = FALSE}
library(caret) ## load the caret package

sm_var <- nearZeroVar(traindata[,-53], saveMetrics=TRUE) ## loop through the training variables, identify the ones with near-zero variance (exclude classe)
dels <- sm_var[sm_var$nzv==TRUE,] ## get the subset of sm_var for which variance was near zero
delete_num <- nrow(dels) ## count how many features will be deleted

## remove near-zero-variance features from the training data
delete_indx_trn <- NULL
for(i in 1:nrow(dels)){delete_indx_trn <- c(delete_indx_trn,grep(paste("^", row.names(dels)[i],"$",sep=""), names(traindata1)))} ## get the column index to remove from training
    traindata1 <- traindata1[,-delete_indx_trn] ## remove the near-zero-variance features

## remove near-zero-variance features from the testing data
delete_indx_tst <- NULL
for(i in 1:nrow(dels)){delete_indx_tst <- c(delete_indx_tst,grep(paste("^", row.names(dels)[i],"$",sep=""), names(testdata1)))} ## get the column index to remove from training
    testdata1 <- testdata1[,-delete_indx_tst] ## remove the near-zero-variance features
```

