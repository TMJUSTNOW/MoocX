<center> <h2>A System View of Communications: From Signals to Packets (pt 1) - Week 4</h2> </center>
<center> <h3>---------------------------------------</h3> </center>

####*Video 4.1.1 - Why We Need the Equalizer*

**A. Notes**

- the idea is to undo the effect of the channel

**B. Modeling the Channel - recursive channel**

- in order to reverse the effect of the channel, we start with a model of the effect of a channel on the input
- we have seen that the reponse of the channel to an input x(n) can be described (modeled) in two equivalent ways:
    - 1. LTI with step response $s(n) = k(1-a^{n+1})u(n)$
    - 2. recursively as $y(n) = a*y(n-1) + (1-a)*k*x(n)$
- for the purposes of deriving the equalizer, the recursive model is better

**C. Intuition for Equalizer**

- due to intersignal interference, the output does nto always move far enough to cross the threshold in response to a change in the bit
- thus, looking at the value (or level) of output is not a reliable way to determine the input bit
- we might make better guesses by looking at how the channel *changes*
    - it will start to move towards 1 when it wants to be one
- the equalizer will allow us to combine the level and change


####*Video 4.1.2 - Derivation of the Equalizer*

**A. Notes**

- expression for input to the channel as function of the output
    - $x(n) = \frac{1}{(1-a)*k}[y(n) - a*y(n-1)]$
- the equation is only exact if the equation is exact...
    - noise will confound this
    - biased estimates of the parameters will hurt performance
- this equation captures the effect of both the current level and how it's changing
- reformulation:
    - $x(n) = \frac{1}{k}[y(n) + \frac{a}{(1-a)}(y(n)-y(n-1))]$
- higher a means a slower channel
    - more weighting on the change (level no longer as reliable)

**B. Going from LTI to the recursive Model**

- some step response:
    - $s(n) = \frac{1}{2}(1-\frac{2}{3}^n+1)u(n)$
- remember that this shares the params a and k with the recursive model:
    - $s(n) = k(1-a^{n+1})u(n)
- just plug in and solve for x(n)
- 

####*Video 4.1.3 - Effect of Equalization on the Eye Diagram*

**A. Notes**

- the channel equalizer opens the eye diagram
    - makes it easier to distinguish between 1s and 0s

**B. Comparing Eye Diagrams**

- what if our assumption about the channel is not correct?
- the key difference is bit error rate...did we guess correctly?

**C. Case 1: Perfect estimate of a**

- for difference SPB, we still get relatively open eye after equalization
    - greatly improved performance, allows you to lower the bit time and transmit more data

**D. Case 2: Channel a < equalizer a**

- the channel is actually moving more more quickly than we think
    - we'll tend to overemphasize changes
    
**E. Case 3: Channel a > equalizer a**

- the channel is moving more slowly than we think
    - underweight changes, overweight level
    - not applying enough dependency on changes, we lose some degree of "eye opening"
    - especially at smallish bit times, error rates go up a lot

**F. Case 4: Perfect a, but some noise**

- bad situation
- height of the eye is smaller (more close in some sense)
- decoding with equalizer worse than without equalizer

**G. Summary**

- using a model of the relationship between the channel input and the channel output, we have developed an equalizer that "undoes" the effect of the channel
- "opens" the eye, which "closes" due to intersymbol interference (ISI)
-the equzlier is robust (i.e. still works) even if the parameters of the channel are not correctly estimated
- however, it might magnify the effect of noise
    - it is built on an assumption of no noise
    - it indulges every movement of the channel response as important for estimation

####*Video 4.1.4 - Lab 6 - Equalization*

**A. Notes**

- we can use protocols to handle issues caused by the channel
- exponential step response = ISI
- in this lab, we will use equalizers
- if equalization works, it handles the ISI situation perfectly

####*Video 4.2.1 - Noise*

**A. The Received Signal**

- the signal at the receiver is the sum of two parts:
    - noise is really the limiting factor in our LTI communication system
    - the actual transmitted sample
    
**B. Noise**

- noise is one of the most critical and fundamental concepts in communications systems
- without noise this course would not exist!
- noise occurs everywhere and a typical noise signal is something normally varying around 1
- it is essentially a "random signal"

**C. Where Does Noise Come From?**

- individual electronic components that measure and manipulate signals
- extrenal influences (e.g. other users of the system)
- resisters, devices and the atmosphere are all sources of thermal noise
- \textbf{thermal noise} is simply due to the ambient heat causing electrons to move and vibrate and create random voltages and emissions
- noise arises internally in systems as well as externally from such things as the atmosphere

**D. Why is nosie so critical?**

- without noise, you should be able to talk very very very quietly and you would still be heard and understood
- the amount of noise determines the minimum signal of what you can understand
- noise determines the minimum signal that can be decoded by radioes and receivers
- we like to use small signals to save energy
- if the desired signal falls below the noise level, bit errors increase significantly

####*Video 4.2.2 - Additive Noise and Its Effects*

**A. Notes**

- additive noise:
    - $y(n) = r(n) + v(n)$
    - with r(n) = channel output without noise
    - v(n) = noise
    - y(n) = received output

**B. Simplifying Assumptions for BER Analysis**

- perfect synchronization:
    - we know exactly where to sample the output to decode each bit
- single sample decoding:
    - make bit decisions on a single point in the sample
- no ISI:
    - the channel response depends only on the current bit, and not on past bits
- additive "white" Gaussian Noise (AWGN):
    - White: the noise varies fast enough that its value at different samples are unrelated to each other
    - Gaussian: to be defined later
- under these simplified assumptions, we only need to consider on sample per bit and can analyze each bit in isolation (independently) of other bits

**C. Computing/Predicting BER in this environment**

- no noise = no bit errors

####*Video 4.2.3 - The Binary Channel and Calculating BER*

**A. Notes**

- simplify additive noise into a "binary channel model"
- we ignore details about the noise and received signal levels
- we look only at the input and output bits
- binary channel: both input and output have two possible values (0 or 1)
- we have errors (0-->1 and 1-->0) that can be probabilistically model
- this seems like it could be modeled with decision trees!

**B. Probabilistic Inquiry**

- $BER = P_{e} = P_{e0}*P(IN=0) + P_{e1}*P(IN=1)$
- these error rates will be useful in informing/bounding the additive noise model

####*Video 4.2.4 - BER Examples*

**A. Notes**

- the BER formula we got is just a weighted average
- a formula as "a statement in the language of mathematics"
    - it should be telling us something
- you can make some good predictions without even using your calculator..if you understand the formula and how the weighting comes into play
- the P(IN) probabilities are set by the transmitter, not really the channel
- usually, 50/50 for the P(IN) probabilities is actually not a bad assumption