<center> <h2>A System View of Communications: From Signals to Packets (pt 1) - Week 5</h2> </center>
<center> <h3>---------------------------------------</h3> </center>

####*Video 5.1.1 - Average Power in Signals*

**A. Notes**

- the binary channel model lets us identify what impacts the bit error rate
- what in the channel ipacts the value of those parameters?
- the actual value of the BER relies on probabilities of paths in the Binary Channel Model
- Probs rely on:
    - the transmit levels
    - the power of the noise
    
**B. Inside the binary Channel**

- output is obtained by thresholding the channel output
    - which, remember, includes noise!
- if noise pushes in the wrong direction strongly enough, we get a bit error
- power is "energy used per unit time"
    - 1 Watt = unit of power
    - lifting an apple (_100g) up by 1m in 1s requires ~1W
- batteries contain a fixed amount of energy
    - the higher the power consumption of the device they are powering, the faster this energy is used up:
    - usable time = $\frac{energy}{power consumption}$
    
**C. Power Consumption**

- calculating the amount of enegy in a battery:;
    - batteries are typically rated at fixed coltage in volts (V) and a charge capacity in milliamp-hours (mAh)
    - multiplying these together gives the total energy stored in the battery in milliwatt-hours (mWh)
- typical power consumption:
    - microwave oven 1000 W
    - desktop computer 120W
    - notebook computer 40W
    - human brain 10W
    - mobile phone 1W
    
**D. Average Power in Signals**

- for communication, we usually have signals that vary around an average value
- for communication, we are interested in how much the signals differ from their average
- since the change can be positive and negative, the average over many samples is zero
- to solve this, we look at average squared deviations
- Power = average squared deviations
- average power (when 0s and 1s equally likely) $P = \frac{(r_{max}-r_{min})^{2}}{4}$
    - power is higher when the max and min are further from each other
    - to save energy, you want to make smaller changes and be able to sense them

####*Video 5.1.2 - Gaussian Noise Model*

**A. Notes**

- the signal compared to the threshold is transmitted wave + noise
- individually, each noise observation is random
- over many samples, the random variations follow a distribution with a regular structure

**B. Probability Density Function**

- the histogram is not totally smooth since we count the samples in bins of finite width
- as the bins get smaller and smaller, the curve gets smoother
- it approaches a function known as the probability density function (pdf)

**C. Gaussian Density Function**

- the probability density function of many naturally occurring random quantities, such as noise, tends to have a bell-like shape known as a Gaussian distribution
- this very important result is called the CLT
- the Gaussian distribution is the "normal" distribution
- PDF:
    - $f_{v}(V) = \frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(v-m)^{2}}{2\sigma^{2}}$
    
**D. Parameters Controlling the Shape**

- the mean (m) is:
    - the average value over many samples
    - the center of the PDF
- the standard deviation (sigma):
    - the spread of the data
- the variance:
    - the square of the standard deviation
    - the average power computed over many many samples
- changing the mean:
    - just shifts the PDf
- changin the variance:
    - smaller variance = small power
    - large variance = high power

**E. Why We Care**

- the probability that the noise v is bewteen two candidates or how likely is it that it's greater than a certain value
- this is hypothesis testing
- features of valid PDF:
    - all values non-negative
    - total AUC = 1

####*Video 5.1.3 - Lab 7: Additive Noise*

**A. Notes**

- additive noise is not a deterministic signal, but a random process
- how can we estimate the distribution of the additive noise?
    - transmit the all-0 sequence (so that everything left over is pure noise)
- by measuring noise in the all-zeros and all-one we can:
    - estimate power of signal and power of noise
    - estimate threshold
    - estimate received signal power as diff between means of two distributions
- increasing the transmission distance will give us a lower received signal power (higher BER)

####*Video 5.1.4 - Calculating the BER*

**A. Notes**

- Pe0, Pe1 depend on:
    - transmit levels
    - power of the noise
    - the threshold
- output for IN=0 will be centered around r_min with variance defined by the noise
    - similar for IN=1, but centered on r_max
    - same variance (coming from additive noise model)
- received signal will comprise two PDFs (one for IN=0, one for IN=1)
- Probability of Error if N=0:
    - when noise is big enough to push the signal above T
    - this is just a hypothesis test on prob(T)
- the probability of error decreases as T increases
    - i.e. as you move from left to right on the r_min PDF
- the r_max analysis is just switch (eval from right to left)
    - value below T is an error
    - AUC on IN=1 PDF gives error rate for that threshold

**B. Putting it Together**

- increasing the threshold:
    - decreases error rate in predicting 0s
    - increases error rate in predicting 1s
- the overall BER is the overlap between the IN=0 PDF and IN=1 PDF
- BER = 0.5 x (Pe0 + pe1)

**C. What is the Effect of Changing the Threshold (T)?**

- BER as function of T is like a "canyon"
- plateau, dip, plateau
- might make sense to choose a threshold halfway between r_min and r_max
    - if probability of ones and zeros are the same

####*Video 5.1.5 - The Effect of Signal to Noise Ratio*

**A. Notes**

- Pe1 and Pe0 rely on signal power
- signal to noise ratio:
    - SNR usually mesaures in decibels (dB)
    - $SNR = \frac{P_{signal}}{P_{noise}} = \frac{(r_{max}-r_{min})^{2}}{4\sigma ^{2}}$
- SNR increases as difference between r_min and r_max gets bigger
- 10db = 10 times noise power
- 30db = 1000 times noise power

**B. Noise Levels in Mobile Phones**

- determines the minimum signal that can be received by radioes and receivers
- typical noise power is extremely small
- you want much higher signal power than noise power

**C. Factors affecting SNR**

- received power decreases as the receiver moves away
- decrease in received signal power leads to decreased SNR
- Once SNR falls below around 10dB, the receiver will stop functioning
- BER falls as SNR increases
- decreasing signal power = moving the IN=0 and IN=1 PDFs closer to each other
    - higher BER in this case
    - lower signal power relative to noise power
- similar ideas with raising noise power
    - when you increase noise power, you're increasing the variance in the PDF of received waves
- "using our graphical analysis, we can understand the effect of changes in the signal power or the noise power in terms of changesi n the probability distribution of the received signal"
- SNR in decibels:
    - $SNR(db) = 10 log_{10}\frac{P_{signal}}{P_{noise}}$

####*Video 5.1.6 - An Expression for BER with Gaussian Noise*

**A. Notes**

- there is not closed form solution for integrating under the Gaussian curve
- as an approximation, we use "the Q-function"
    - design for Gaussian with 0 mean and standard deviation of 1
- $Q(T) = P[v > T]$
    - found numerically from tables
    - or the ```qfunc(t)``` function in Matlab
- other side? 1 - Q(T)
- Q(T) evaluates to 1/2 at 0

**B. Extending to Other Gaussian distributions**

- when we change the mean, we aren't really changing the probability function
    - just shifting it over
- changes in the mean are just going to cause changes in the Q-funciton that shift it to the left and the right by the mean
- most general format:
    - $P[y > T] = Q(\frac{T-m}{\sigma}$
- this can be used to help predict the bit error rate


####*Video 5.2.1 - Channel Coding (6:23)*

**A. Notes**

- noise is a fundamental limiter in any comm system
    - noise causes bit errors
- how can we reduce bit errors?
    - redundancy
- example: professor
    - spoken information
    - slides
    - review lecture

**B. Adding redundancy in communication**

- Claude Shannon = modern information theory
- created "the mathematical theory of communication"
- Shannon's noisy channel coding theorem: "Any noisy channel has a channel capacity C, the maximum rate at which useful information can be transmitted. For any data rate R < C, there exists a way to encode the data such that the probability of error is aribitrarily small.
- key idea: introduce redundancy into the data stream
- error correcting coding/error correction
    - add new bits that don't carry new information
    - we just repeat the information
    - 

####*Video 5.2.2 - Block Codes (9:25)*

**A. Notes**

- (n,k,d) codes:
    - split the message into k-bit blocks
    - create a codeword by adding (n-k) extra bits to each block
    - the extra bits are computed based on the message bits
    - thus, they contain no new information
- d = minimum Hamming Distance between codewords
- sometimes we drop the d and indicate only (n,k) code

**B. Code Rate**

- code rate = fraction of the total number of bits that are sent that actually correspond to useful bits (the message)
- code rate = k/n
- gross bit rate = rate at which we're sending bits (useful or otherwise)
- net bit rate = rate that we send useful bits at--> code rate*gross bit rate

**C. Hamming Distance**

- the number of bit positions where the corresponding bits are different
- the hamming distance measures the number of bit errors it takes to transform one codeword to another
    - for example, if we use no coding, each bit is represented by a 0 or 1 and they are just 1 HD appart
    - we want to "separate the codewords you expect to see as much as possible"
- the higher Hamming distance, the more resilient the system is to errors

**D. Error Correction vs. Detection**

- Error detection:
    - we can detect errors
    - but we don't know how to fix them
- error correction:
    - we can detect and fix them
- for a given code, the receiver can choose whether to use the code to detect errors or to correct them

**E. The Error Detection/Correction Capability**

- the minimum Hamming distance determines the maximum number of bit errors the receiver can detect or correct
- if the minimum Hamming distance is d, the receiver can either:
    - detect but not correct errors in at most d-1 bits of each codeword
    - detect and correct errors in at most (d-1)/2 bits of each codeword

####*Video 5.2.3 - Repetition Codes (10:11)*

**A. Notes**

- a type of block code
- the (2,1,2) repetition code:
    - each bit is repeated twice
    - each code word has an even number of 1 bits, we refer to this as "even parity"
    - the Hamming distance between codewords is d=2
- the (3,1,3) code
    - each bit is repeated 3 times
    - there are three position, 8 possibile combinations
    - this means there are 6 combos of others
    - the Hamming distance is d=3
    - you need to take three steps to go from 111 to 0000
    - we can detect 2 errors of correct 1 error
    
**B. Correcting Errors**

- if we assume that at most 1-bit error can occur, we can do error correction
- if we receive 100, since at most 1-bit error occurred, 000 must have been sent
- we can correct errors by seeing whether 0 or 1 has the most "'votes"

**C. More**

- we can further increase Hamming distance by goging to (4,1,4) repetition code (etc.)
- (4,1,4) - detect and correct 1 bit error and detect 2 bit errors
    - can detect 3 total errors
