<center> <h2>Predictive Machine Learning - Week 3</h2> </center>
<center> <h3>---------------------------------------</h3> </center>

####*Video 3.1 Predicting with trees*

**A. Key ideas**

- iteratively split variables into groups
- evaluate homogeneity within each group
- split again if necessary
- pros:
    - easy to interpret
    - split again if necessary
- cons:
    - without pruning/cross-validation can lead to overfitting
    - harder to estimate uncertainty
    - results may be variable
    
**B. Decision Trees: Basic Algorithm**

1. start with all variables in one group
2. find the variable/split that best separates the outcomes
3. divide the data into two groups ("leaves") on that split ("node")
4. Within each split, find the best variable/split that separates the outcomes
5. Continue until the groups are too small or sufficiently "pure"

**C. Measures of impurity**

- misclassification error: 1 - probability that you're in the most common class
    - 0 = perfect purity
    - 0.5 = no purity (perfectly balanced between outcomes)
- GINI index (different from gini coefficient in econ)
    - 0 = perfect purity
    - 0.5 = no purity

**D. Measures of Impurity**

- deviance/information gain
    - comes from information theory
    - 0 = perfect purity
    - 1 = no purity

**E. An Example**

```{r sec1trees, eval=FALSE}
data(iris); library(ggplot2)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]

## exploratory plot
qplot(Petal.Width, Sepal.Width, color = Species, data=training)

##let's fit the tree model
library(caret)
modFit <- train(Species ~., method = "rpart", data=training) ## rpart is the r thing for using trees
print(modFit$finalModel)

## plot the tree
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n=TRUE, all = TRUE, cex=0.8)

## Prettier Tree Plot with rattle package
library(rattle)
fancyRpartPlot(modFit$finalModel)

## predictions
predict(modFit, newdata=testing) ## this will write out the class predicted
```

**F. Notes and Further Resources**

- classification trees are non-linear models
    - they use interactiosn between variables
    - data transformations may be less improtant (monotone transformations)
    - trees can also be used for regression problems (continuous outcome)
        - use RMSE as a measure of impurity
- note that there are multiple tree building options in R both in the carety package
    - party
    - rpart
    - tree
- see "Classification and regression trees" book linked to

####*Video 3.2 Bagging*

**A. What is Bagging?**

- bagging = "bootstrap aggregating"
- this is a model averaging approach

**B. Basic Idea**

1. resample cases and recalculate predictions
2. Average or majority vote
-Notes:
    - similar bias
    - reduced variance (relative to single model)
    - more useful for non-linear functions)
    
**C. Example with Ozone data**

```{r sec2ozone, eval=FALSE}
library(ElemStatLearn); data(ozone, package = "ElemStatLearn")
ozone <- ozone[order(ozone$ozone,]

## bagged loess
ll <- matrix(NA, nrow=10, ncol=155) ## create a matrix with 10 rows, 155 cols

## loop through, sample with replacement, pick 10 datasets
## reorder the dataset every time by ozone
## fit a loess curve each time (a smoother, similar to a spline modelfit)
## span is a measure of smoothness
## predict for each loess curve, outcome for new dataset
## predict from loess0 curve from the i-th resample
## fill in the i-th row of ll with predictions for i-th run
for(i in 1:10){
    ss <- sample(1:dim(ozone)[1], replace=T)
    ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone,]
    loess0 <- loess(temperature ~ ozone, data=ozone0, span=0.2)
    ll[i,] <- predict(loess, newdata=data.frame(ozone=1:155))
    }

## plotting the averages

plot(ozone$ozone, ozone$temperature, pch=19, cex=0.5)
for(i in 1:10){lines(1:155, ll[i,], col="grey", lwd=2)} ## plot the individual forecasts
lines(1:155, apply(ll,2,mean),col="red", lwd=2) ## take column means, plot
```

**C. Some Notes on Bagging**

- the bagged estimate will have lower variability
- some stuff in train:
    - method = bagEarth
    - method = treebag
    - method = bagFDA

**D. More bagging in caret**

- this is a very advanced use

```{r sec3bags, eval=FALSE}
## put predictors in their own data frame
predictors = data.frame(ozone=ozone$ozone)

## same with variable of interest
temperature = ozone$temperature

## set up the bagging
treebag <- bag(predictors, temperature, B = 10,
               bagControl = bagControl(fit = ctreeBag$fit,
                                    predict = ctreeBag$pred,
                                    aggregate = ctreeBag$aggregate))


```


**E. parts of Bagging**

- ```ctreebag$fit```
    - takes the dataframe(x) and outcome(y)
    - uses the ctree() function to run a regression tree on the data
- ```ctreeBag$pred```
    - takes an object from the fit, and a new dataset x
    - calculates the tree response from the object and new data
    - calculates the probability matrix
    - either returns observed level or predicted response
- ```ctreeBag$aggregate```
    - gets all the predictions from the model fits
    - binds them into one data matrix (each row is one model prediction)
    - takes median prediction from all model fits across bootstrapped samples
    
**F. Notes and Further Resources**

- Notes:
    - bagging is most useful for nonlinear models
    - often used with trees - an extension is random forests
    - several models use bagging in caret's train function
- Further resources:
    - bagging (link)
    - bagging and boosting (link)
    - Elements of Statistical Learning (link)
####*Video 3.3 Random Forests*

**A. Basic Ideas**

- similar to bagging
    1. bootstrap samples (then run regression trees)
    2. at each split, bootstrap variables
    3. Grow lots of trees and vote
- Pros:
    - very accurate
- cons:
    - speed
    - interpretability
    - overfitting (very important to use cross-validation)
    
**B. How it works in practice**

- build trees on random subsamples (bootstrapped samples)

```{r sec4rando, eval=FALSE}
data(iris); library(ggplot2)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]

## Let's set up the random forest prediction

library(caret)

## use method=rf for random forest to predict the outcome variable, Species. Prox=true produces some extra info
modFit <- train(Species ~., data=training, method="rf", prox=TRUE)

## LOok at a specific tree in the final model
getTree(modFit$finalModel, k=2)

## use the "centers" info to get class centers
irisP <- classCenter(training[,c(3,4)], training$Species, modFit$finalModel$prox) ## get center value for each class
irisP <- as.data.frame(irisP); irisP$Species <- rownames(irisP)
p <- qplot(Petal.Width, Petal.Length, col=Species, data=training) ## plot the observed data, coloring by species
p + geom_point(aes(x=Petal.Width, y = Petal.Length, col=Species), size = 5, shape=4, data=irisP) ## plot the data centers for each class, as X's

##get some predictions
pred <- predict(modFit, testing)
testing$predRight <- pred==testing$Species ## subset out all the ones you correctly predicted
table(pred,testing$Species) ## table of predicted vs. actual. (predictions down column)

## plot the predictions (color by whether or not you predicted accurately, see where mistakes lie)
qplot(Petal.Width, Petal.Length, colour=predRight, data=testing, main="newdata Predictions")
```

**C. Notes and further resources**

- Notes:
    - random forests are usually one of the two top performing algorithms along with boosting in prediction constests
    - random forests are difficult to interpret but often very accuract
    - care should be taken to avoid overfitting (see rfcv function) -- train() in caret will also handle cross-validation

####*Video 3.4 Boosting*

**A. Basic Idea**

- take lots of possibly weak predictors
- weight them and add them up
- get a stronger predictor

**B. Basic Idea (cont.)**

1. Start with a set of of classifiers
    - example: all possible trees, all possible regression models, all possible cutoffs
2. Create a classifier that combines classification functions
    - goal is to minimize error (on training set)
    - iterative, select one h at each step
    - calculate weights based on errors
    - upweight missed classifications and select next h
- see links to other resources in the slides
- most famous boosting algorithm is "Adaboost"

**C. Simple Example**

- [Boosting Tutorial](http://webee.technion.ac.il/people/remeir/BoostingTutorial.pdf)
- you upweight misclassified points so that the next iteration builds a model that attracts them
- completed classifier...works much better than any individual one, gets a strong weighted classifier

**D. Boosting in R**

- boosting can be used with any subset of classifiers
- one large subclass is [gradient boosting](http://en.wikipedia.org/wiki/Gradient_boosting)
    - gradient boosting is a machine learning technique for regression problems, which produces a prediction model in the form of an ensemble of weak prediction models (typically decision trees)
    - It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.
    - the grdietn boosting method can also be used for classification problems by reducing them to regression with a suitable loss function
    - gradient boosting is a special case of the functional gradient descent view of boosting
- R has multiple boosting libraries. Differences include the choice of basic classification functions and combination rules:
    - gbm - boosting with trees
    - mboost - model based boosting
    - ada - statistical boosting based on additive logistic regression
    - gamBoost - for boosting generalized additive models
- most of the above libraries are available in the caret package
    - you can use them directly with the ```train()``` function in caret
    
**E. Example in R**

```{r sec5boosting, eval=FALSE}
## Set up the problem, split the data
library(ISLR); data(Wage); library(ggplot2); library(caret);
Wage <- subset(wage, select=-c(logwage)) ## everything except what we care about (wage)
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]

## lets use GBM (boosting with regression trees), set verbose=FALSE to suppress all the output
modFit <- train(wage ~., method="gbm", data=training, verbose=FALSE)  ## wage as a function of everything else
print(modFit)

## Plot the results
qplot(predict(modFit,testing), wage,data=testing)
```

- again, the basic idea:
    - take these weak classifiers, average them together with weights to get a better classifier
    
**F. Notes and further reading**

- A couple of nice tutorials for boosting:
    - [Freund and Shapire](http://www.cc.gatech.edu/~thad/6601-gradAI-fall2013/boosting.pdf)
    - [Ron Meir](http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf)
- Boosting, random forests, and model ensembling are the most common tools that win Kaggle and other prediction contests:
    - [NetFlix Prize](http://www.netflixprize.com/assets/GrandPrize2009_BPC_BigChaos.pdf)
    - [Kaggle Info](https://kaggle2.blob.core.windows.net/wiki-files/327/09ccf652-8c1c-4a3d-b979-ce2369c985e4/Willem%20Mestrom%20-%20Milestone%201%20Description%20V2%202.pdf)

####*Video 3.5 Model Based Prediction*

**A. Basic Idea**

1. assume the data follow a probabilistic model
2. Use Bayes' theorem to identify optimal classifiers

Pros:
    - can take advantage of structure of the data
    - may be computationally convenient
    - are resonably accurate on real problems
Cons:
    - make additional assumptions about the data
    - reduced accuracy if model is incorrect
    
**B. Model based Approach**

1. Our goal is to build parametric model for condition distribution
2. A typical approach is to apply [Bayes theorem](http://en.wikipedia.org/wiki/Bayes%27_theorem)
3. Typically priors are set in advance
4. A common choice for f(x) is a Gaussian distribution
5. Estimate parameters from the data
6. Classify to the class with the highest value of P(y=k|x=x)

**C. Classifying using the model**

- a range of models use this approach:
    - [Linear discriminant analysis](http://en.wikipedia.org/wiki/Linear_discriminant_analysis) assumes f~k~(x) is a multivariate Gaussian with same covariances
        - find a linear combination of features which characterizes or separates two or more classes of objects or evenets. The resultign combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification
        - draw lines through the covariance space
    - Quadratic discriminant analysis assumes f~k~(x) is multivariate Gaussian with different covariances
        - different within each class (quadratic curves through data)
    - Model based prediction assumes more complicated versions for the covariance matrix
    - Naive Bayes assumes independence between features for model building
        - assume predictors are independent
- [good resource from Stanford](http://statweb.stanford.edu/~tibs/ElemStatLearn/)

**D. Why linear discriminant analysis?**

- key term:
    - log(Pr(Y = k|X = x)/Pr(Y = j|X = x))

**E. Decision Boundaries**

- [LDA Decision Boundar Example](http://i.stack.imgur.com/YijL7.png)
- fit multiple Gaussian distribution (one for each class)
    - draw lines where probability switches from being higher for one class than another
- fit Guassian distributions
    - use those to draw lines assigning points to higher posterior probabilities

**F. Discriminant function**

- decide class based on:
    - pick value of k (class) that gives larges value of the discriminant function
    - estimated with maximum likelihood
    
**G. Naive Bayes**

- Suppose we have many predictors, we would want to model P(Y = k|X~1~...X~m~)
- We could use Bayes Theorem to get:
    - Probability that Y is in class K, given observed predictors, is the prior probability Y is in class k times probability of observing all the features (X's) given Y in class k, divided by some scaling constant
    - this probability is proportional to the prior probability times the probability of the features, given that we're in class k
- classify by picking largest value of:
    - prior prob of being in k * prob of predictors given class k
    - if you assume the predictors are independent, they drop out of the conditioning argument, so you can multiple your prior probability of being in the class by the product of probabilities of observing each feature (given Y in class k)
    
**H. R Examples**

```{r sec6LDA, eval=FALSE}
## load the data
data(iris); library(ggplot2)

## set up the training and test
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]

## build the LDA model
modLDA <- train(Species ~., data=training, method="lda")
modNB <- train(Species ~., data=training, method="nb")

## predictions
pLDA <- predict(modLDA, testing); pNB <- prediction(modNB, testing)

## table of results:
table(pLDA, pNB)

## Comparison of Results
equalPredictions = (pLDA==pNB)
qplot(Petal.Width, Sepal.Width, colour=equalPredictions,data=testing)  ## plot the predictions, coloring by whether or not the methods agreed
```