Congratulations, combining LIDAR and RADAR measurements with the help of extended and unsended comment filters is incredibly impressive. These are core skills that we use on the Sensor Fusion team at Mercedes-Benz. &gt;&gt; Let's think about where Sensor Fusion fits into the entire autonomous vehicle technology stack. First, sensors record the data, then there are pre-processing steps such as clustering algorithms, or more complex solutions to detect object hypothesis. And finally, we can use these hypothesis as input for object level fusion. &gt;&gt; Inside the sensor fusion block, we combined that data together like you did in this lesson. And this is not the only possible flow, we can apply sensor fusion at each of these processing steps. &gt;&gt; Once we have the more consistent representation of the surrounding world, we pass the results to other part of the autonomous vehicle system Further down the line, different teams will use this data to localize the vehicle, control the vehicle, and plan the vehicle's path. &gt;&gt; It's also worth thinking about ways in which the world is more complicated that what we've discussed so far. For example, in this lesson, we have assumed there is only one object to track. But what if there are many cars on the street? Then you have to find out which measurement belongs to which vehicle, that makes sensor fusion even harder. &gt;&gt; But the core techniques you have learned in this lesson are the key to extended the sensor fusion block to handle these more complicated scenarios. &gt;&gt; Sensor fusion is a critical part of an autonomous driving system and the other parts of the system rely on this data. &gt;&gt; In future modules, you will learn about how the components of this system used fused sensor data to complete their functions. Good luck on the final sensor fusion project. 