As important of an idea as intelligence is, it's hard to both define, and
measure. But it has been measured, and tested for over 100 years. The first
person to do this was Alfred Binet in 1903. He was looking for a measure that
he could give to all students in the parish school district to assess who was
struggling. Binet did some things that are still used today. He started with
simple problems. But it got more difficult as the test went on. Now, he gave
this test to thousands of children to see what was the normal or average score
for the different age groups. Now, years later, in the 1920s, other
psychologists built and developed the first Intelligence Quotient, the IQ. This
is when the IQ actually got its name. They started with the mental age of each
child. They were looking at what age most children stopped solving problems of
a certain difficulty. So if you solved problems that most seven-year olds
solve, but not most six year olds, then you have a mental age of seven. This is
regardless of your chronological age but they did also look at your
chronological age or how you are in years. Taking these two things together,
your mental age and chronological age, made up the fist IQ ratios. So your IQ
is defined as your mental age over your chronological age times 100 which was
the average. And that gave you your IQ score. So for example, if a child had a
mental age of ten and a chronological age of ten then their IQ would be 100.
And this is the average for her age. In fact, 100 is always the average of IQ.
But, if a ten year old was solving problems at a 12 year old level, then his or
her IQ would be 120, whihc is above the normal IQ score. Likewise if a ten year
old child can only solve problems at an eight year old's level. Your IQ would
be 80, just below the norm. People who are scoring above 100 are scoring at a
higher intellectual level than most of the population. People that are scoring
less than 100 are scoring at a lower intellectual level than most of the
population.
